<!DOCTYPE html><html><div class="item-title">
        Item 230
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                 Producer will assign
              </div></li><li><div>
                 replace "deleteme" with a tombstone
              </div></li><li><div>
                 the buffer should serialize the buffer time and the value as byte[],
 which we can't compare for equality using ProducerRecord.
 As a workaround, I'm deserializing them and shoving them in a KeyValue, just for ease of testing.
              </div></li><li><div>
                 flush the buffer into a list in buffer order so we can make assertions about the contents.
              </div></li><li><div>
                 flush everything to the changelog
              </div></li><li><div>
                
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License. You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                 Several things to note:
 * The buffered records are ordered according to their buffer time (serialized in the value of the changelog)
 * The record timestamps are properly restored, and not conflated with the record's buffer time.
 * The keys and values are properly restored
 * The record topic is set to the changelog topic. This was an oversight in the original implementation,
   which is fixed in changelog format v1. But upgraded applications still need to be able to handle the
   original format.
              </div></li><li><div>
                 Several things to note:
 * The buffered records are ordered according to their buffer time (serialized in the value of the changelog)
 * The record timestamps are properly restored, and not conflated with the record's buffer time.
 * The keys and values are properly restored
 * The record topic is set to the original input topic, *not* the changelog topic
 * The record offset preserves the origininal input record's offset, *not* the offset of the changelog record
              </div></li><li><div>
                 nothing to do.
              </div></li><li><div>
                 As we add more buffer implementations/configurations, we can add them here
              </div></li><li><div>
                 expected
              </div></li><li><div>
                 sizes of key and value
              </div></li><li><div>
                 value.context.timestamp
 value.context.offset
 size of topic
              </div></li><li><div>
                 partition
 number of headers
              </div></li><li><div>
                 not handling the null topic condition, because we believe the topic will never be null when we serialize
              </div></li><li><div>
                 not handling the null condition because we believe topic will never be null in cases where we serialize
              </div></li><li><div>
                 create multiple partitions as a trap, in case the buffer doesn't properly set the
 partition on the records, but instead relies on the default key partitioner
              </div></li><li><div>
                 expect all post-suppress records to keep the right input topic
              </div></li><li><div>
                 note, we send all input records to partition 0
 to make sure that supppress doesn't erroneously send records to other partitions.
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> KAFKA-7895: fix Suppress changelog restore (#6536)
                </div><div><b>message:</b> KAFKA-7895: fix Suppress changelog restore (#6536)

Several issues have come to light since the 2.2.0 release:
upon restore, suppress incorrectly set the record metadata using the changelog record, instead of preserving the original metadata
restoring a tombstone incorrectly didn't update the buffer size and min-timestamp

Reviewers: Guozhang Wang &lt;wangguoz@gmail.com&gt;, Matthias J. Sax &lt;mjsax@apache.org&gt;,  Bruno Cadonna &lt;bruno@confluent.io&gt;,  Bill Bejeck &lt;bbejeck@gmail.com&gt;

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> KAFKA-7895: fix Suppress changelog restore
                </div><div><b>body:</b> Several issues have come to light since the 2.2.0 release:

* upon restore, `suppress` incorrectly set the record metadata using the changelog record, instead of preserving the original metadata
* restoring a tombstone incorrectly didn't update the buffer size and min-timestamp

### Committer Checklist (excluded from commit message)
- [ ] Verify design and implementation 
- [ ] Verify test coverage and CI build status
- [ ] Verify documentation (including upgrade notes)

                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div>
                Hi @ableegoldman @cadonna @guozhangwang ,

Please review these bugfixes for Suppression when you get the chance.

The reporter has confirmed it fixes the duplicates in his repro as well: https://issues.apache.org/jira/browse/KAFKA-7895?focusedCommentId=16810770&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16810770

I'm still chasing down a different bug to do with the interaction between Suppression and EOS, but I don't think it's related and should be a separate PR.

Thanks,
-John
              </div></li><li><div>
                Thanks @vvcephei for the PR. Do you want me to review it now or wait for some further investigations?
              </div></li><li><div>
                Sorry for the delay in responding, @guozhangwang . I think we should review this now.

Right now, it seems like the EOS thing is a different mechanism, so I plan to do a separate PR once I figure it out.
              </div></li><li><div>
                Thanks for the review, @bbejeck . I've addressed your comments and added an integration test that checks that the topic name is right, at least. It seems a little tricky to deterministically verify the incoming event's offset or partition number.
              </div></li><li><div>
                To actually generate duplicates under the condition I fixed, we'd have to run the application for a pretty long time.
              </div></li><li><div>
                I've only made a brief pass on the bullet points that's listed on the description and these fixes lgtm.

cc @mjsax @ableegoldman to make another thorough pass over it.
              </div></li><li><div>
                Thanks for the review, @cadonna ! I'm currently seeing if I can get the integration test to spit out duplicates. I'll address your comments in the next commit.
              </div></li><li><div>
                Ok @bbejeck and @cadonna ,

I managed to update the integration test to re-create the duplication bug on trunk (you can see this in action if you check out https://github.com/vvcephei/kafka/tree/testing-test , which has this test cherry-picked onto trunk). I've verified that it fails on trunk both because the topic is wrong on suppressed records *and* because it produces duplicates.

Thanks for inducing me to go the extra mile, @bbejeck .

I think this is ready for final reviews.

-John
              </div></li><li><div>
                Flaky test failure: https://issues.apache.org/jira/browse/KAFKA-7965?focusedCommentId=16815432&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16815432

Retest this, please.
              </div></li><li><div>
                SUCCESS 
 10841 tests run, 69 skipped, 0 failed.
--none--
              </div></li><li><div>
                Thanks for the reviews, @bbejeck, @cadonna, and @mjsax . I've addressed all comments. Do you mind taking another look?
              </div></li><li><div>
                Ok, in adopting the PR comments regarding assuming the topic will never be null, I broke a whole bunch of tests. I've fixed it with the following strategy:

* the topic may be null during size estimation
* the topic will never be null when serializing the record context
              </div></li><li><div><div><b>body:</b> Java 8 failed with flaky ConsumerBounceTest. This test was purportedly fixed after my build started, so I'm giving it the benefit of the doubt and not reporting it (https://issues.apache.org/jira/browse/KAFKA-7965).

Retest this, please.
                </div><div><b>label:</b> test
                </div></div></li><li><div>
                Hey, the tests passed! Do you think we're ready to merge this, @mjsax ?
              </div></li><li><div>
                @vvcephei I've made another pass LGTM.  Just ping us when the last bits are addressed and we can merge
              </div></li><li><div>
                Thanks, all. @guozhangwang and @mjsax , I believe the only remaining unresolved thread is regarding the header check above. Thanks!
              </div></li><li><div>
                Ok, @guozhangwang and @mjsax , I think I've addressed your feedbacks. I also did a sweep for any more missed feedback (sorry about that).

Please take another look when you get the chance (also @bbejeck and @cadonna ). Thanks for the reviews!
              </div></li><li><div><div><b>body:</b> Java11 failed. Flaky tests tracked in Jira. Java8 passed.

Retest this please.
                </div><div><b>label:</b> test
                </div></div></li><li><div>
                Merged #6536 into trunk.
              </div></li><li><div>
                Thanks for the fix @vvcephei!
              </div></li></ol></div><div><b>github_pulls_reviews:</b> <ol><li><div>
                Why this change from `long` to `int`?
              </div></li><li><div>
                Is there a chance here that the cumulative size of bytes for all headers and values exceeds max int?  I realize the probability is very low, but should we have a check here and throw an exception in case of an overflow (maybe I'm overthinking this?)
              </div></li><li><div>
                why not use `Objects.requireNonNull` here and in the next branch?
              </div></li><li><div><div><b>body:</b> nit: args on separate line
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> nit: args on separate line
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                If we are serializing the `ContextualRecord` could we ever hit this case? 
              </div></li><li><div>
                Unless I'm missing something aren't we restoring the record with the changelog `offset`, `topic` etc?
              </div></li><li><div><div><b>body:</b> Could you please assign `Integer.BYTES` to a variable with a meaningful name and use that variable here? It would make the code bit clearer.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> `long` is an unnecessarily large data type for `size`, since arrays can only be indexed with integers (thus, a byte array cannot hold more than a "max int" number of bytes).
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> That's a good idea. I agree it's unlikely, and if it does happen, we'd wind up with an exception while serializing later, but we could detect it sooner here and throw a nicer error.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Only because `requireNonNull` throws a `NullPointerException`, which I felt was less appropriate than an `IllegalArgumentException` in this case.
              </div></li><li><div>
                oof! Sorry, I missed these long lines.
              </div></li><li><div>
                Yep, the currently released versions of Suppress are already serializing changelog messages without this header. The purpose of this check is to be able to transition to new Streams versions in a backward-compatible way.
              </div></li><li><div>
                Yeah, this is the (incorrect) behavior of the current code. Luckily, this will only happen for the very small window of emitting records that are currently buffered when you upgrade Streams to "this" version. Once those are flushed out, the buffer will always be sending/receiving "v1" changelog messages that correctly encode the upstream topic/offset.

I figured the best way to handle this transition is to just preserve the existing behavior for "old format" messages, rather than try to do something clever.
              </div></li><li><div><div><b>body:</b> Also here, could you please give `Long.BYTES` a meaningful name?
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                I doubt that we could exceed it. Note, that `max.message.byte` is an integer broker side config, and I believe it applies to the serialized version of a message. \cc @guozhangwang @hachikuji Is this correct?

Of course, a user could emit a new key-value pairs with huge key/value/header arrays, however, this message could not be stored in Kafka itself either. If we want a guard, we should use `long size` internally here, and check if it exceed `INT_MAXVALUE` and throw of course.
              </div></li><li><div>
                This does not guard against "double overflow". I think, if we apply a check, we should check for all corner cases or not check at all.
              </div></li><li><div><div><b>body:</b> nit: `headerKey` -&gt; `headerKeyBytes` (same for `headerValues` below)
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                How could `topic` be `null`?
              </div></li><li><div>
                `valueLength` could be `-1`
              </div></li><li><div>
                Shouldn't headers be `null` or contain exactly one header with `key=="v"` and non-null value? If, the second part of "or" should never be true?
              </div></li><li><div><div><b>body:</b> nit: move `key` to new line
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                I cannot follow. What is this discussion about?
              </div></li><li><div>
                Why do we need this check?
              </div></li><li><div><div><b>body:</b> The argument that `NullPointerException` is not appropriate holds for almost all cases when `requireNonNull` in use. Thus, I would prefer code consistency and use `requireNonNull` here, too.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Why this change?
              </div></li><li><div>
                Why change from `v` to `k` ?
              </div></li><li><div><div><b>body:</b> nit: `produceSynchronouslyToPartition0` -&gt; `produceSynchronouslyToPartitionZero`
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> Is there a way to check against wrapping all the way around to positive again? Even a long could wrap around in principle... This is starting to get a little complicated. Maybe we should just fall back on the jvm to catch us when we try to make and populate the byte array after all.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> I could see making an assumption that a specific ConsumerRecord is always has a topic (because that's where they come from), but this is the ProcessorRecordContext. The class has no null checks on the topic field, so rather than tracing all the usages to find out whether it can ever be null, in main code or test code, it's simpler and bulletproof just to check for it.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                good call!
              </div></li><li><div>
                If it's an "old format" changelog record, the headers would have been copied from the record context, in which case they could be null or non-null, but still not contain a "v". Of course, it could contain a "v" which has nothing to do with our algorithm here, but I think that situation is undetectable. And unlikely, as well.
              </div></li><li><div><div><b>body:</b> &gt; Is there a way to check against wrapping all the way around to positive again?

I think you would need to check after each modification... This would of course become costly.

&gt; Even a long could wrap around in principle... 

True. But I think the risk would be minimal in practice for this case.

&gt; Maybe we should just fall back on the jvm to catch us when we try to make and populate the byte array after all.

Not sure what you mean by this?
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                `ProcessorRecordContext` would only set the topic to `null` for calls into `init()`, `close()`, and punctuation callbacks. Thus, it should never be `null` here IMHO and I believe it would indicate a bug, if it's `null` here. Hence, this check might masked a bug and we should rather fail with a NPE for this case. \cc @guozhangwang to confirm.
              </div></li><li><div>
                I don't think so. We never write headers in the changelogger. Note, that the changelog topic is used to recover the store content. However, rows in a store only have a key and a value. There is no header that we could write, because the on put, the current record header does not related to the store content.

Similarly, `suppress()` serializes the whole record context and store it in the value IIRC.
              </div></li><li><div><div><b>body:</b> I meant that maybe we should just remove the check.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                We're setting the record metadata incorrectly here, which is actually the bug I'm fixing with the new format. It's just that there's apparently nothing better to do that continue handling the old changelog format the same way, since the format itself is missing the information we need.
              </div></li><li><div>
                good call. So, we're covered. Thanks for this analysis.
              </div></li><li><div><div><b>body:</b> Thanks for the information. I think this is getting a little complicated, and we'll just drop the check. If the value is too big, we won't be able to serialize it later anyway.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                This is one of the bugs that this PR fixes. The correct operation of this class depends on maintaining `minTimestamp` on every mutation. If a future code change breaks that contract, it would re-introduce a variant of our bug, and it would be just as subtle as it was this time. This check safeguards against that eventuality, by checking on our invariant at the spot we depend on it.
              </div></li><li><div><div><b>body:</b> When I wrote the tests the first time around, I overlooked a simpler application to test suppression. Switching to this simpler application made it possible to directly test for the bug (duplicated suppress output).
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Instead of grouping on the values anymore, now I'm just counting events. I just didn't realize that simpler aggregation was available when I wrote the tests the first time.
              </div></li><li><div>
                &gt; or not check at all.

Fine with we to remove the check.
              </div></li><li><div>
                Just reviewing this PR, it reminds me on this JIRA: https://issues.apache.org/jira/browse/KAFKA-8235

Seems `firstKey` would throw if `sortedMap` is empty.
              </div></li><li><div><div><b>body:</b> Well, my point is, that the check can be simplified. I don't think that `record.headers() == null` can be true; it's guaranteed that there is a headers object.

Not sure if we can simplify the second check. It iterators over the headers map and does String comparison to find a header with key `v` -- seems to be rather heavy.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Ack. So we don't need it, but we want it :) 
              </div></li><li><div>
                Oof! You're absolutely right. What a strange api asymmetry...
              </div></li><li><div>
                Ok, so I have a clarification: this method is used to estimate the size of the record context in memory, for a variety of usages not restricted to processing. Including some of the cases where you point out it may be null, so we do need the null-check in this method.

In the de//serialize methods below, I've removed the null-check, following the spirit of your feedback.
              </div></li><li><div>
                This seems not fixed yet?
              </div></li><li><div>
                If I understand correct, a record read from the changelog topic should only be:

1) having no headers at all (old version)
2) having a singleton header with `v --&gt; byte`.

All other cases should indicate a bug in the code.

So it seems we can just check if `record.headers() == null`, and inside the if condition though, we should also check the `v` and assume it's always there (if not then throw illegal state), and switching on the byte value:

1) byte == 1: do as below.
2) otherwise: to not support forward compatibility, we can just throw unsupported.
              </div></li><li><div><div><b>body:</b> We were previously setting the changelog record headers from the record headers. I really don't think it's safe to just assume that record headers are always empty for records inside of Streams.

Also, I'd like to point out that @guozhangwang thinks that old-format headers are _always_ null, and @mjsax think that they are _never_ null, but _always_ empty...

The point is, header handling is partially undefined inside Streams. I'd be very much in favor of being defensive in the face of such ambiguity, rather than making potentially faulty assumptions, since the risk is a runtime exception or even data corruption (if we mis-interpret the binary format, but it happens to de-serialize without exceptions into gibberish).

Further, none of this feedback is pointing out a problem with the code, just that there might be one unnecessary clause in the boolean expression (which, I still am not convinced is unnecessary after all).
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Gah. Thanks for the catch!
              </div></li><li><div>
                Thanks for the explanation @vvcephei , makes sense.

One thing I'd still like to ask though: from the current code it seems we do not check the byte of `v` if it indeed exist, while I think to be safer we should still check it, and throw exception indicating that we do not support forward compatibility when the format has evolved. WDYT?
              </div></li><li><div>
                This comment is acked but not addressed. Did it slip? Or did you decide to ignore it?
              </div></li><li><div><div><b>body:</b> I understand your argument. However, I am not 100% sure if this does not introduce a weird asymmetry? Also, we only estimate the size _if_ we serialize the context. We don't estimate in-memory/object space usage, do we?

                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Why this block?
              </div></li><li><div>
                - It's a contract that `KafkaConsumers` _guarantees_ that `header != null`. \cc @hachikuji to confirm.
- And we know that KafkaStreams never writes headers into changelog topics.

Thus, I don't see any reason to check for something that we know is the case, ie, we know that `header.size() == 0` in old format. For everything else, we could throw `IllegalStateException`. Of course the header API is limiting and we cannot get `size()` and thus `record.headers().lastHeader("v") == null)` is what we need to do... :( -- but we can safely remove the first `null` check -- it could even mask a bug and we should rather fail for this case.

We can also do a version check as suggested by Guozhang.
              </div></li><li><div>
                &gt; we know that KafkaStreams never writes headers into changelog topics.

We're restoring records that we wrote like this: https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java#L240

We write headers to the suppression changelog topic. Sorry if that wasn't clear, that's why I'm insisting we have to check it.

If there's a contract that headers are never null, then I agree we can remove that check. In that case, we should add null-checks to the ConsumerRecord constructor.
              </div></li><li><div>
                Yes, we do. This is also used for computing the size of the record cache to determine cached state store flushing, etc. (I was a little surprised when a bunch of "unrelated" tests failed as a result of the change I tried here).

I shared your concern about the asymmetry, but I think it's actually ok, even if you only consider the suppression buffer. The `sizeBytes` method is an estimation of the resident size of this object in memory, which is only loosely related to the size of the serialized form of the data in this object.
              </div></li><li><div>
                Oy, it slipped. Fixing it now.
              </div></li><li><div>
                I shouldn't have acked it until I fixed it; that's why I overlooked it on later passes.
              </div></li><li><div><div><b>body:</b> Actually, the latter conversation with @mjsax about the relationship (or lack of one) between this and the serialized form makes me think maybe this response was incorrect. I'll move it back to long.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> Ok, I've had a chat with @hachikuji , and he confirmed that the headers _should_ never be null. I did a quick audit of code base as well, and agree that this should be the case.

I added a null-check to the header and docs to this effect in f5bd0999c (in this PR), and then removed the null-check.

I also added an exception if the record has a "v" header, but it isn't `1`. I'm wondering, @guozhangwang , if we should just assume in this case that the record is old-format, and happened to already have a "v" header. The alternative thing is you could have run Streams with a _newer_ version, and then downgraded. In that case, it would be wrong to try restoring as old-format, and an exception would be preferable. I'm leaning toward the exception, since people haven't been using Suppress that long, so the risk of a spurious "v" header is low. WDYT? 
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> It just prevents polluting the method scope with the temp variable `topicBytes`
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                &gt; We're restoring records that we wrote like this: https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java#L240
&gt;
&gt; We write headers to the suppression changelog topic. Sorry if that wasn't clear, that's why I'm insisting we have to check it.

Arg. We should not have done this... But serialize the header in KS and add them to the value. Well, too late now I guess. Or could we switch the format? Note sure what the impact on the upgrade path would be? Thought?

&gt; I added a null-check to the header and docs to this effect in f5bd099 (in this PR), and then removed the null-check.

+100
              </div></li><li><div>
                Ack. Thanks for being patient with me :)
              </div></li></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> Ktable supress operator emitting more than one record for the same key per window
                </div><div><b>description:</b> Hi, We are using kstreams to get the aggregated counts per vendor(key) within a specified window.

Here's how we configured the suppress&nbsp;operator to emit one final record per key/window.
{code:java}
KTable&lt;Windowed&lt;Integer&gt;, Long&gt; windowedCount = groupedStream
     .windowedBy(TimeWindows.of(Duration.ofMinutes(1)).grace(ofMillis(5L)))
     .count(Materialized.with(Serdes.Integer(),Serdes.Long()))
     .suppress(Suppressed.untilWindowCloses(unbounded()));
{code}
But we are getting more than one record for the same key/window as shown below.
{code:java}
[KTABLE-TOSTREAM-0000000010]: [131@1549067040000/1549067100000], 1039
[KTABLE-TOSTREAM-0000000010]: [131@1549067040000/1549067100000], 1162
[KTABLE-TOSTREAM-0000000010]: [9@1549067040000/1549067100000], 6584
[KTABLE-TOSTREAM-0000000010]: [88@1549067040000/1549067100000], 107
[KTABLE-TOSTREAM-0000000010]: [108@1549067040000/1549067100000], 315
[KTABLE-TOSTREAM-0000000010]: [119@1549067040000/1549067100000], 119
[KTABLE-TOSTREAM-0000000010]: [154@1549067040000/1549067100000], 746
[KTABLE-TOSTREAM-0000000010]: [154@1549067040000/1549067100000], 809{code}
Could you please take a look?

Thanks

&nbsp;

&nbsp;

Added&nbsp;by John:

Acceptance Criteria:
 * add suppress to system tests, such that it's exercised with crash/shutdown recovery, rebalance, etc.
 ** [https://github.com/apache/kafka/pull/6278]
 * make sure that there's some system test coverage with caching disabled.
 ** Follow-on ticket: https://issues.apache.org/jira/browse/KAFKA-7943
 * test with tighter time bounds with windows of say 30 seconds and use system time without adding any extra time for verification
 ** Follow-on ticket:&nbsp;https://issues.apache.org/jira/browse/KAFKA-7944
                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol><li><div>
                [~vvcephei] Not sure how quickly we can figure this out. However, 2.1.1 is not released yet and there might be another RC... For this case, if we are quickly enough, we could get it into 2.1.1. Don't think it's a blocker for 2.1.1 though. \cc [~cmccabe]
              </div></li><li><div>
                I am experiencing what appears similar/related.&nbsp;I’m seeing&nbsp;quite a large number of duplicates being created when my topology comes back up after being restarted. (Caching is disabled and Im using a short commit internal). I’m using a groupBy key -&gt; windowBy (time) -&gt; aggregate -&gt; suppress. The suppress is configured until window closes, unbounded, window-size 1 hour, grace 1 min. Its there are a smaller number of duplicates created whilst the topology is running, but its much smaller compared to when its restarted.
              </div></li><li><div>
                Hi [~mjsax],

Thanks for thinking about that. I've been hacking on this issue for a little over a day now, with no luck so far.

Given that we don't know how long it will take to repro the issue or figure it out and fix it, I'd hold off on blocking the release.

If I have a breakthrough, I'll figure out what stage the release is in.

It is a serious report, though, so in either case, once we have a fix, I would request an immediate bugfix release.

Thanks,

-John

(cc [~cmccabe])
              </div></li><li><div>
                Quick update:

I have added a suppression operator to Streams's system tests, and I believe that I have a local repro of the bug. I'll dig in more tomorrow and update everyone with status as I learn more.

Thanks,

-John
              </div></li><li><div>
                Great! Thank you John for the update.
              </div></li><li><div>
                I think I figured it out.

&nbsp;

The suppression operator depends on a guarantee from upstream windowed aggregations that they will not send any further events for a window once its grace period expires. The windowed aggregations themselves correctly implement this guarantee, but record caches can violate it, since they'll hold onto events until the next commit/flush.

This suggests that a workaround would be to disable caching in the windowed aggregation prior to suppression, but I didn't try it out. Depending on the specific topology, this might not be sufficient.

It just so happens that in all my tests, I'd disabled caching, probably with the idea of making the output deterministic. I didn't predict that it would affect correctness.

I have a fix for it, by changing the way that stream-time is accounted for. I'm running the system tests now to be sure the fix is ok. Once a preliminary PR ([https://github.com/apache/kafka/pull/6231)]&nbsp;is merged and the system tests pass, I'll clean up my branch and send a PR.

Once I submit the PR, maybe some intrepid folks out there can pull my branch and try it out.

&nbsp;

Does this explanation make sense?

Thanks,

-John
              </div></li><li><div>
                Hi John, thanks for looking into it. One thing though is that [~mbragg] reported he sees similar issues in his environment while he's already disabled caching, does that mean there are other issues not related to caching layer as well?
              </div></li><li><div>
                Thanks for the reminder, [~guozhang].

I think that the same mechanism could cause duplicates as well during recovery, since stream time is currently determined by the input partitions, but the recovery is from the changelog topic, so recovered events may all appear to be arriving at the same stream time.

If I'm right about this, then the fix I have in mind should address that as well.

However, it is troubling that Michael also said that he observed duplicates during steady-state processing, post recovery, with caching disabled. There may be something else going on as well.

&nbsp;

The system tests already exercise crash and shutdown recovery, so to complete the picture, I'll make sure I add a system test with caching disabled (if there's not already one).
              </div></li><li><div>
                vvcephei commented on pull request #6278: KAFKA-7895: Fix stream-time reckoning for suppress
URL: https://github.com/apache/kafka/pull/6278
 
 
   * Add suppress to system tests
   * Move stream-time reckoning from Task into Processor
   
   Even within a Task, different Processors have different perceptions
   of time, due to record caching on stores and in suppression itself,
   and in general, due to any processor logic that may hold onto
   records arbitrarily and emit them later. Thanks to this, we can't rely
   on the whole task existing in the same "instant" of stream-time. The
   solution is for each processor node that cares about stream-time to
   track it independently.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li><li><div>
                Hi all,

&nbsp;

I'm proposing [https://github.com/apache/kafka/pull/6278]&nbsp;as a fix to this issue.

If anyone is willing to build my branch and verify if it fixes the issue for them, I would be grateful.

Your code reviews are also appreciated.

&nbsp;

Thanks,

-John
              </div></li><li><div>
                https://github.com/apache/kafka/pull/6278
              </div></li><li><div>
                Hi [~vvcephei]

Thanks for your proposed fix, sounds promising! To clarify my initial comment, I meant to say that its _possible_ there's duplicates during steady processing... but if so the number would be much less then during restore of the state stores; where there is a massive spike. Sorry for the confusion.&nbsp;
              </div></li><li><div>
                mjsax commented on pull request #6278: KAFKA-7895: Fix stream-time reckoning for suppress
URL: https://github.com/apache/kafka/pull/6278
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li><li><div>
                vvcephei commented on pull request #6286: KAFKA-7895: fix stream-time reckoning for Suppress
URL: https://github.com/apache/kafka/pull/6286
 
 
   * Add suppress to system tests
   * Move stream-time reckoning from Task into Processor
   
   Even within a Task, different Processors have different perceptions
   of time, due to record caching on stores and in suppression itself,
   and in general, due to any processor logic that may hold onto
   records arbitrarily and emit them later. Thanks to this, we can't rely
   on the whole task existing in the same "instant" of stream-time. The
   solution is for each processor node that cares about stream-time to
   track it independently.
   
   See also #6278
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li><li><div>
                Hey [~vvcephei]&nbsp;

What would be a typical ETA of this fix being available in a release? Are there any pre-releases published anywhere?

Sorry to be a pain!&nbsp;

Thanks
              </div></li><li><div>
                bbejeck commented on pull request #6286: KAFKA-7895: fix stream-time reckoning for Suppress (2.2)
URL: https://github.com/apache/kafka/pull/6286
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li><li><div>
                vvcephei commented on pull request #6325: KAFKA-7895: fix stream-time reckoning for Suppress (2.2) (#6286)
URL: https://github.com/apache/kafka/pull/6325
 
 
   Even within a Task, different Processors have different perceptions
   of time, due to record caching on stores and in suppression itself,
   and in general, due to any processor logic that may hold onto
   records arbitrarily and emit them later. Thanks to this, we can't rely
   on the whole task existing in the same "instant" of stream-time. The
   solution is for each processor node that cares about stream-time to
   track it independently.
   
   On the side, backporting some internally-facing code maintainability updates
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li><li><div>
                Hi, [~mbragg],

Sorry for the delay.

Thanks for the clarification. I'm very curious if the patch resolves the problem for you.

The 2.2 release should be very soon. They are currently voting on the release candidate in the dev mailing list.

Actually, the release candidate message contains information about testing the release candidate: [https://lists.apache.org/list.html?dev@kafka.apache.org:lte=1M:%5BVOTE%5D%202.2.0%20RC1]

Please let me know if you have any trouble with it.

Thanks,

-John
              </div></li><li><div>
                bbejeck commented on pull request #6325: KAFKA-7895: fix stream-time reckoning for Suppress (2.1) (#6286)
URL: https://github.com/apache/kafka/pull/6325
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li><li><div>
                Hi

I am still seeing this behaviour using the 2.2.0 Maven artifacts from [https://repository.apache.org/content/groups/staging]

With the topology below, I invariably get emissions of previously emitted windows for the same key on restart of my streams application, and sometimes the re-emitted windows have earlier timestamps and aggregated data that is consistent with the latter part of the window being lost (i.e., state seems to be resetting to an earlier version):
{code:java}
return sourceStream
    .groupByKey()
    .windowedBy(TimeWindows.of(rollupInterval).grace(config.getGracePeriodDuration()))
    .aggregate(
        () -&gt; rollupFactory.createRollup(windowDuration),
        aggregator,
        Materialized.&lt;String, R, WindowStore&lt;Bytes, byte[]&gt;&gt;as(outputTopic + "_state")
            .withValueSerde(rollupSerde)
    )
    .suppress(Suppressed.untilWindowCloses(BufferConfig.unbounded())
        .withName(outputTopic + "_suppress_state"))
    .toStream((stringWindowed, rollup) -&gt; stringWindowed.key());
{code}
&nbsp;I have tried disabling caching using:
{code:java}
streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);
{code}
&nbsp;but this does not appear to make a difference.

There also seems to be no difference in behaviour between 2.2.0 and 2.1.1.

Regards

Andrew
              </div></li><li><div>
                Hi [~AndrewRK],

Sorry to hear that!

Looking at your topology, I'd expect it to work as advertised (obviously), and it also looks very similar to what I've tested heavily, so I might need to get some more help from you to investigate the cause.

I have a couple of follow-up questions...
 # Is this restarting after a clean shutdown or a crash? (i.e., how do you stop and restart the app?)
 # Do you have EOS enabled? (If you don't, can you try to repro with EOS on?)

Thanks,

-John
              </div></li><li><div><div><b>body:</b> Hi John Roesler

To answer your first question, I have a shutdown hook in place that seems to be working correctly according to the slf4j logs (the application changes state from RUNNING -&gt; PENDING_SHUTDOWN -&gt; NOT_RUNNING).
{code:java}
Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
{code}
I do have exactly-once configured. My settings are as follows (commit interval of 10s):
{code:java}
        streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, appConfig.getApplicationID());
        streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG,
            appConfig.getBootstrapServerList().stream().map(HostPort::toString).collect(Collectors.joining(",")));
        streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, appConfig.getStateDirectory());
        streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG,
            appConfig.getCommitIntervalDuration().toMillis());
        streamsConfiguration.put(StreamsConfig.DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG,
            com.booking.infra.rollup.kafka.TimestampedValueTimestampExtractor.class);
        streamsConfiguration.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG,
            StreamsConfig.EXACTLY_ONCE);
{code}
When I test I start with a new docker-based Kafka cluster and delete the application state directory. I feed the app with enough input to generate one or two commits and then hit Ctrl-C, which appears to shut everything down correctly. I then restart the app leaving the state directory untouched. My code does not call streams.cleanup(). Almost without fail this will produce repeated entries for the same key/window. Sometimes the repeated entries will be different, in which case the repeat matches an older entry in the source KTable.

I do not have to stress the app in any way to cause this, which makes me wonder if I am doing something fundamentally wrong.

Here is an example of the repeated output:
{code}
0.0026: 1553591980510: monitors.group1:stat.stat1: time=1553591980510 wstart=1553591980000 wsize=1000 count=12 sum=1044 sumSq=91400 min=76 max=98 last=98
0.0041: 1553591980057: monitors.group1:stat.stat1: time=1553591980057 wstart=1553591980000 wsize=1000 count=2 sum=154 sumSq=11860 min=76 max=78 last=78
{code}
And here is are the corresponding entries in the KTable changelog:
{code}
0.0026: 1553591980057: [monitors.group1:stat.stat1@1553591980000/1553591981000]: time=1553591980057 wstart=1553591980000 wsize=1000 count=2 sum=154 sumSq=11860 min=76 max=78 last=78
0.0028: 1553591987079: [monitors.group1:stat.stat1@1553591980000/1553591981000]: time=1553591980510 wstart=1553591980000 wsize=1000 count=12 sum=1044 sumSq=91400 min=76 max=98 last=98
{code}
Please let me know if you need any more information, or if you need me to run any more tests.

Regards
 Andrew
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Thanks for the extra details, [~AndrewRK],

I'll try setting up a scenario similar to what you've described and see if I get the same behavior.

I doubt this is the problem, but just to check all the boxes, when you read the output topic, have you set the consumer to "read_committed"?

-John
              </div></li><li><div>
                Hi John Roesler

Yes, my consumer's isolation level is set to read_committed.

Regards
Andrew
              </div></li><li><div>
                EDIT: actually, nevermind. After replicating your application, I was able to reproduce what you're seeing. I'll let you know when I figure out what's going on.

&nbsp;

Thanks!

Also, can you elaborate on how `TimestampedValueTimestampExtractor` behaves?

Thanks,

-John
              </div></li><li><div>
                Here's the repro I put together. It'll take me a little while to debug it, but I wanted to share my approach.

[https://github.com/vvcephei/suppress-demo]

-John
              </div></li><li><div><div><b>body:</b> Quick update: The main source of the duplicates seems to be this odd situation where the suppression buffer is sometimes sending its records to the wrong changelog partition. This results in duplicates because those changelog partitions are handled independently, so the message sent to the wrong partition will be emitted in addition to the ones sent to the right partition.&nbsp;It's still not clear why some records are being logged to the wrong partition.

When I fully understand why this is happening, I should also be able to explain why none of the existing tests have caught this condition, when it's so easy to reproduce with your application.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Status update: I found a couple of subtle bugs in the logic for restoring tombstones from the changelog for the suppression buffer. Running with my patched version of Streams, I was able to restart the application in my suppress-demo six times in a row without seeing any duplicates. (Previously, I'd see it on the first restart). So, I feel pretty good about this fix.

I'm cleaning up my branch and will have a PR for review by tomorrow morning.

I think the reason that none of the existing integration/system tests caught it is just that they don't run over a long enough time scale. I'm working on regression tests for these specific bugs, which I'll include as a part of my PR. I'll create a ticket to add a more realistic system test to exercise Streams better and expose bugs like this.
              </div></li><li><div>
                vvcephei commented on pull request #6536: KAFKA-7895: fix Suppress changelog restore
URL: https://github.com/apache/kafka/pull/6536
 
 
   Several issues have come to light since the 2.2.0 release:
   
   * upon restore, `suppress` incorrectly set the record metadata using the changelog record, instead of preserving the original metadata
   * restoring a tombstone incorrectly didn't update the buffer size and min-timestamp
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li><li><div>
                Hey [~AndrewRK],

I've created a PR to fix the issues I've found from your report. Do you mind testing my patch and confirming that it fixes your issues?

[https://github.com/apache/kafka/pull/6536]

&nbsp;

Note: I'm currently seeing no duplicates if I close Streams gracefully (using SIGTERM + shutdown hook), but I am still seeing duplicates if I terminate Streams abruptly (using SIGKILL). I think this has a separate root cause, and it wasn't part of your report, so I'm submitting this PR now, while I continue to debug the further issues.

&nbsp;

Thanks,

-John
              </div></li><li><div>
                Hi John Roesler

I can confirm that I am no longer seeing duplicates. Thanks a lot for your help.

Regards
Andrew
              </div></li><li><div>
                No problem. Thanks for _your_&nbsp;help!

The PR is being reviewed now. Once it's merged, I'll request it to be cherry-picked to all versions that have Suppress, and then request bugfix releases for them.

&nbsp;

As an update, I've tracked down the problem where I'm still seeing duplicates when I crash the application, even with EOS enabled. The problem is that Streams flushes the state stores in no particular order.&nbsp;If there is a cached state store upstream of the suppression, and Streams flushes the suppression buffer first, and then the cached state store second, the cached data will get flushed and processed by Suppress, and there may be processing results that Suppress emits, but the changelog for the buffer won't be updated again. The solution I have in mind is to flush the stores in topological order. In the mean time, disabling caching should eliminate this particular source of duplicates, if someone is looking for a workaround.
              </div></li><li><div>
                I'm tracking the incorrect flushing behavior in a separate ticket (KAFKA-8204). This is the root cause of the continued duplicate results, even when EOS is enabled, when the Streams process undergoes an abrubt stop.
              </div></li><li><div>
                bbejeck commented on pull request #6536: KAFKA-7895: fix Suppress changelog restore
URL: https://github.com/apache/kafka/pull/6536
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li><li><div>
                Hi [~vvcephei], when do we have a new release for this?
              </div></li><li><div>
                vvcephei commented on pull request #6615: KAFKA-7895: fix Suppress changelog restore (#6536)
URL: https://github.com/apache/kafka/pull/6615
 
 
   Several issues have come to light since the 2.2.0 release:
   upon restore, suppress incorrectly set the record metadata using the changelog record, instead of preserving the original metadata
   restoring a tombstone incorrectly didn't update the buffer size and min-timestamp
   
   Cherry-picked from #6536 / 6538e9e4d6c1f64fe3045a5c3fbfe306277a1bee
   
   Reviewers: Guozhang Wang &lt;wangguoz@gmail.com&gt;, Matthias J. Sax &lt;mjsax@apache.org&gt;,  Bruno Cadonna &lt;bruno@confluent.io&gt;,  Bill Bejeck &lt;bbejeck@gmail.com&gt;
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li><li><div>
                vvcephei commented on pull request #6616: KAFKA-7895: fix Suppress changelog restore (#6536)
URL: https://github.com/apache/kafka/pull/6616
 
 
   Several issues have come to light since the 2.2.0 release:
   upon restore, suppress incorrectly set the record metadata using the changelog record, instead of preserving the original metadata
   restoring a tombstone incorrectly didn't update the buffer size and min-timestamp
   
   Cherry-picked from #6536 / 6538e9e
   
   Reviewers: Guozhang Wang &lt;wangguoz@gmail.com&gt;, Matthias J. Sax &lt;mjsax@apache.org&gt;,  Bruno Cadonna &lt;bruno@confluent.io&gt;,  Bill Bejeck &lt;bbejeck@gmail.com&gt;
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li><li><div>
                Hi [~songkun],

No, this fix is just now merged, and we're working on merging the 2.2 and 2.1 backports. We also need to merge https://issues.apache.org/jira/browse/KAFKA-8204 before we do buxfix releases. I'll send a note to this thread when we do the releases, so all the subscribers can verify they are fixed.
              </div></li><li><div>
                [~songkun]: planned release dates are published in the wiki:&nbsp;[https://cwiki.apache.org/confluence/display/KAFKA/Future+release+plan]
              </div></li><li><div>
                bbejeck commented on pull request #6615: KAFKA-7895: fix Suppress changelog restore (#6536)
URL: https://github.com/apache/kafka/pull/6615
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li><li><div>
                bbejeck commented on pull request #6616: KAFKA-7895: fix Suppress changelog restore (#6536)
URL: https://github.com/apache/kafka/pull/6616
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li><li><div>
                Update: this bugfix has been merged to trunk (2.3), 2.2, and 2.1 branches. I'm waiting on other bugfixes before requesting patch releases.
              </div></li><li><div>
                Thank you [~mjsax] for your kind suggesstion, and&nbsp;[~vvcephei]&nbsp;, thank your timely notification, looking forward the patch release!
              </div></li><li><div>
                2.2.1 release was proposed today. Cf:&nbsp;[https://cwiki.apache.org/confluence/display/KAFKA/Release+Plan+2.2.1]
              </div></li><li><div>
                Hi [~vvcephei], [~mjsax], any progress on that?
              </div></li><li><div>
                Hi [~songkun],

The release process takes a little while, as they have to wait on critical bugfixes to be merged, then build release candidates, and then have a vote for approval.

I'd just keep an eye on the release plan page for current status, and if you want to watch it play out "in real time", you can subscribe to the Kafka dev mailing list. There's a thread for the release.

Thanks! And I look forward to getting this fix out for you. Thanks again for the report.
-John
              </div></li><li><div>
                FYI, 2.2.1 RC0 vote is in progress. (https://lists.apache.org/thread.html/3486798e63ae666fc336ce9009f07c7fdf66a96badc1fed63bcbd2ed@%3Cdev.kafka.apache.org%3E)

Please feel free to test it out, and reply on the vote thread if you have some trouble with it.
              </div></li><li><div>
                Hi [~vvcephei],

Great work!

I want to test it in my env, but I can't figure how to find this version, since it's still not released, could you please give me some tips where I can download it, thanks!

Edit: I just find it now&nbsp;:)
              </div></li><li><div>
                As pointed out in the email John linked to, the RC artifact can be downloaded:&nbsp;[https://home.apache.org/~vahid/kafka-2.2.1-rc0/]
              </div></li><li><div>
                vvcephei commented on pull request #7373: KAFKA-7895: Revert suppress changelog bugfix for 2.1
URL: https://github.com/apache/kafka/pull/7373
 
 
   The bugfix from (#6536) (#6616) breaks compatibility with brokers using log format less than 0.11. (Because record headers are not supported in 0.10 brokers)
   
   Even though this fix is useful, we should not break broker compatibility in a bugfix release, so I'm reverting just the changelog format change. Note: this re-introduces the suppress bug related to changelog restoration, so folks using suppress are recommended to upgrade to 2.2.1 at least.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li><li><div>
                This won't fix for 2.1.2&nbsp;due to backward compatibility issues – reverting the back port.
              </div></li><li><div>
                mjsax commented on pull request #7373: KAFKA-7895: Revert suppress changelog bugfix for 2.1
URL: https://github.com/apache/kafka/pull/7373
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li></ol></div></div></html>