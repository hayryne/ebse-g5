<!DOCTYPE html><html><div class="item-title">
        Item 352
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
     * hadoop custom param for sqoop job
     
              </div></li><li><div>
                *
     * sqoop job name - map-reduce job name
     
              </div></li><li><div>
                *
     * sqoop advanced param
     
              </div></li><li><div>
                *
     * sqoop job type:
     * CUSTOM - custom sqoop job
     * TEMPLATE - sqoop template job
     
              </div></li><li><div>
                *
     * customJob eq 1, use customShell
     
              </div></li><li><div>
                 sqoop job type is template set task relation
              </div></li><li><div>
                set sqoop advanced custom param
              </div></li><li><div>
                set sqoop job name
              </div></li><li><div>
                set hadoop custom param
              </div></li><li><div>
                export hive to mysql
              </div></li><li><div>
                export hdfs to mysql using update mode
              </div></li><li><div>
                sqoop TEMPLATE job
import mysql to HDFS with hadoo
              </div></li><li><div>
                *
     * test SqoopJobGenerator
     
              </div></li><li><div>
                import mysql to hive
              </div></li><li><div>
                *
     * get taskExecutionContext include mysql
     * @return TaskExecutionContext
     
              </div></li><li><div>
                test sqoop tash init method
              </div></li><li><div>
                sqoop CUSTOM job
              </div></li><li><div>
                192.168.0.111:3306\",\"database\":\"test\",\"jdbcUrl\":\"jdbc:mysql://192.168.0.111:3306/test\",\"user\":\"kylo\",\"password\":\"123456\"}";
192.168.0.111:3306\",\"database\":\"test\",\"jdbcUrl\":\"jdbc:mysql://192.168.0.111:3306/test\",\"user\":\"kylo\",\"password\":\"123456\"}";
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> Merge pull request #2943 from Eights-Li/dev-sqoop-optimization
                </div><div><b>message:</b> Merge pull request #2943 from Eights-Li/dev-sqoop-optimization

Sqoop task optimization
                </div></div></li></ol></div><div><b>github_issues:</b> <ol><li><div><div><b>title:</b> [Feature] Sqoop component optimization
                </div><div><b>body:</b> **Is your feature request related to a problem? Please describe.**
dev branch sqoop task need to enhancment.
optimization points:
Sqoop's data access and data export do not support Hadoop-level custom parameters, that is, -D level parameters
        – MR task name
        – MR map and reduce memory and quantity, etc.
    • Split-by field is not supported. If -m is greater than 1, if the primary key of the relational database table is not self-increasing, Sqoop It may cause duplicate data imported into Hadoop. The general solution is to specify a split-by field. therefore, split-by needs support
    • Cannot customize parameters, such as import mysql, some tables can add –direct to speed up the import speed

**Describe the solution you'd like**
ideas:
 • The task name of Sqoop is universal, and it must be changed to the required parameter on the Sqoop page
    • Add Hadoop custom parameter input box for setting MR parameter memory, etc.
    • Add Sqoop task-level custom parameters, like –driect, –fetch-size and other parameters used in specific situations
    • Add option button to choose, custom script or use template script, refer to the design of DataX node



                </div></div></li><li><div><div><b>title:</b> [Feature] Sqoop component optimization
                </div><div><b>body:</b> **Is your feature request related to a problem? Please describe.**
dev branch sqoop task need to enhancment.
optimization points:
Sqoop's data access and data export do not support Hadoop-level custom parameters, that is, -D level parameters
        – MR task name
        – MR map and reduce memory and quantity, etc.
    • Split-by field is not supported. If -m is greater than 1, if the primary key of the relational database table is not self-increasing, Sqoop It may cause duplicate data imported into Hadoop. The general solution is to specify a split-by field. therefore, split-by needs support
    • Cannot customize parameters, such as import mysql, some tables can add –direct to speed up the import speed

**Describe the solution you'd like**
ideas:
 • The task name of Sqoop is universal, and it must be changed to the required parameter on the Sqoop page
    • Add Hadoop custom parameter input box for setting MR parameter memory, etc.
    • Add Sqoop task-level custom parameters, like –driect, –fetch-size and other parameters used in specific situations
    • Add option button to choose, custom script or use template script, refer to the design of DataX node



                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>title:</b> [Feature] Sqoop component optimization
                </div><div><b>body:</b> **Is your feature request related to a problem? Please describe.**
dev branch sqoop task need to enhancment.
optimization points:
Sqoop's data access and data export do not support Hadoop-level custom parameters, that is, -D level parameters
        – MR task name
        – MR map and reduce memory and quantity, etc.
    • Split-by field is not supported. If -m is greater than 1, if the primary key of the relational database table is not self-increasing, Sqoop It may cause duplicate data imported into Hadoop. The general solution is to specify a split-by field. therefore, split-by needs support
    • Cannot customize parameters, such as import mysql, some tables can add –direct to speed up the import speed

**Describe the solution you'd like**
ideas:
 • The task name of Sqoop is universal, and it must be changed to the required parameter on the Sqoop page
    • Add Hadoop custom parameter input box for setting MR parameter memory, etc.
    • Add Sqoop task-level custom parameters, like –driect, –fetch-size and other parameters used in specific situations
    • Add option button to choose, custom script or use template script, refer to the design of DataX node



                </div></div></li><li><div><div><b>title:</b> [Feature] Sqoop component optimization
                </div><div><b>body:</b> **Is your feature request related to a problem? Please describe.**
dev branch sqoop task need to enhancment.
optimization points:
Sqoop's data access and data export do not support Hadoop-level custom parameters, that is, -D level parameters
        – MR task name
        – MR map and reduce memory and quantity, etc.
    • Split-by field is not supported. If -m is greater than 1, if the primary key of the relational database table is not self-increasing, Sqoop It may cause duplicate data imported into Hadoop. The general solution is to specify a split-by field. therefore, split-by needs support
    • Cannot customize parameters, such as import mysql, some tables can add –direct to speed up the import speed

**Describe the solution you'd like**
ideas:
 • The task name of Sqoop is universal, and it must be changed to the required parameter on the Sqoop page
    • Add Hadoop custom parameter input box for setting MR parameter memory, etc.
    • Add Sqoop task-level custom parameters, like –driect, –fetch-size and other parameters used in specific situations
    • Add option button to choose, custom script or use template script, refer to the design of DataX node



                </div></div></li><li><div><div><b>title:</b> [Feature] Sqoop component optimization
                </div><div><b>body:</b> **Is your feature request related to a problem? Please describe.**
dev branch sqoop task need to enhancment.
optimization points:
Sqoop's data access and data export do not support Hadoop-level custom parameters, that is, -D level parameters
        – MR task name
        – MR map and reduce memory and quantity, etc.
    • Split-by field is not supported. If -m is greater than 1, if the primary key of the relational database table is not self-increasing, Sqoop It may cause duplicate data imported into Hadoop. The general solution is to specify a split-by field. therefore, split-by needs support
    • Cannot customize parameters, such as import mysql, some tables can add –direct to speed up the import speed

**Describe the solution you'd like**
ideas:
 • The task name of Sqoop is universal, and it must be changed to the required parameter on the Sqoop page
    • Add Hadoop custom parameter input box for setting MR parameter memory, etc.
    • Add Sqoop task-level custom parameters, like –driect, –fetch-size and other parameters used in specific situations
    • Add option button to choose, custom script or use template script, refer to the design of DataX node



                </div></div></li></ol></div><div><b>github_issues_comments:</b> <ol><li><div>
                The suggestion is very good. At present, the Sqoop node type does only support data import and export, and other Sqoop commands do not support it. But according to my understanding, the person in charge of the Sqoop class node may not want to open the function of custom parameters, because this is more difficult to verify.
The above is just my understanding. I hope @zixi0825 can give us your opinions. Thank you.
              </div></li><li><div><div><b>body:</b> The solution is good. When I designed, I thought that the graphical interface can meet the needs of simple and fast use. It is better to use custom scripts to support custom parameters. Custom scripts I think can be implemented with shell scripts, so they are not added to the sqoop task type.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                merge into dev, close
              </div></li></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> Sqoop task optimization
                </div><div><b>body:</b> ## What is the purpose of the pull request
#2917 #2763 
*sqoop task optimization*

## Brief change log
  - *Add CUSTOM and TAMELATE job for sqoop task*
  - *support hadoop params like mr job name, etc*
  - *suooprt sqoop advanced params like --direct &amp; --fetch-size, etc*
  - *Add sqoop task front-end selection box cascade display*
  - *fix bug : java map fields and hive map fields generate error*

## Verify this pull request
  - */server/worker/task/sqoop/SqoopTaskTest.java*


                </div></div></li><li><div><div><b>title:</b> Sqoop task optimization
                </div><div><b>body:</b> ## What is the purpose of the pull request
#2917 #2763 
*sqoop task optimization*

## Brief change log
  - *Add CUSTOM and TAMELATE job for sqoop task*
  - *support hadoop params like mr job name, etc*
  - *suooprt sqoop advanced params like --direct &amp; --fetch-size, etc*
  - *Add sqoop task front-end selection box cascade display*
  - *fix bug : java map fields and hive map fields generate error*

## Verify this pull request
  - */server/worker/task/sqoop/SqoopTaskTest.java*


                </div></div></li><li><div><div><b>title:</b> Sqoop task optimization
                </div><div><b>body:</b> ## What is the purpose of the pull request
#2917 #2763 
*sqoop task optimization*

## Brief change log
  - *Add CUSTOM and TAMELATE job for sqoop task*
  - *support hadoop params like mr job name, etc*
  - *suooprt sqoop advanced params like --direct &amp; --fetch-size, etc*
  - *Add sqoop task front-end selection box cascade display*
  - *fix bug : java map fields and hive map fields generate error*

## Verify this pull request
  - */server/worker/task/sqoop/SqoopTaskTest.java*


                </div></div></li><li><div><div><b>title:</b> Sqoop task optimization
                </div><div><b>body:</b> ## What is the purpose of the pull request
#2917 #2763 
*sqoop task optimization*

## Brief change log
  - *Add CUSTOM and TAMELATE job for sqoop task*
  - *support hadoop params like mr job name, etc*
  - *suooprt sqoop advanced params like --direct &amp; --fetch-size, etc*
  - *Add sqoop task front-end selection box cascade display*
  - *fix bug : java map fields and hive map fields generate error*

## Verify this pull request
  - */server/worker/task/sqoop/SqoopTaskTest.java*


                </div></div></li><li><div><div><b>title:</b> Sqoop task optimization
                </div><div><b>body:</b> ## What is the purpose of the pull request
#2917 #2763 
*sqoop task optimization*

## Brief change log
  - *Add CUSTOM and TAMELATE job for sqoop task*
  - *support hadoop params like mr job name, etc*
  - *suooprt sqoop advanced params like --direct &amp; --fetch-size, etc*
  - *Add sqoop task front-end selection box cascade display*
  - *fix bug : java map fields and hive map fields generate error*

## Verify this pull request
  - */server/worker/task/sqoop/SqoopTaskTest.java*


                </div></div></li><li><div><div><b>title:</b> Sqoop task optimization
                </div><div><b>body:</b> ## What is the purpose of the pull request
#2917 #2763 
*sqoop task optimization*

## Brief change log
  - *Add CUSTOM and TAMELATE job for sqoop task*
  - *support hadoop params like mr job name, etc*
  - *suooprt sqoop advanced params like --direct &amp; --fetch-size, etc*
  - *Add sqoop task front-end selection box cascade display*
  - *fix bug : java map fields and hive map fields generate error*

## Verify this pull request
  - */server/worker/task/sqoop/SqoopTaskTest.java*


                </div></div></li><li><div><div><b>title:</b> Sqoop task optimization
                </div><div><b>body:</b> ## What is the purpose of the pull request
#2917 #2763 
*sqoop task optimization*

## Brief change log
  - *Add CUSTOM and TAMELATE job for sqoop task*
  - *support hadoop params like mr job name, etc*
  - *suooprt sqoop advanced params like --direct &amp; --fetch-size, etc*
  - *Add sqoop task front-end selection box cascade display*
  - *fix bug : java map fields and hive map fields generate error*

## Verify this pull request
  - */server/worker/task/sqoop/SqoopTaskTest.java*


                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div>
                # [Codecov](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943?src=pr&amp;el=h1) Report
&gt; Merging [#2943](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943?src=pr&amp;el=desc) into [dev](https://codecov.io/gh/apache/incubator-dolphinscheduler/commit/2749c7e5369459c1a410508cc4769db37f4a0043&amp;el=desc) will **increase** coverage by `0.94%`.
&gt; The diff coverage is `60.86%`.

[![Impacted file tree graph](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943/graphs/tree.svg?width=650&amp;height=150&amp;src=pr&amp;token=bv9iXXRLi9)](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943?src=pr&amp;el=tree)

```diff
@@             Coverage Diff              @@
##                dev    #2943      +/-   ##
============================================
+ Coverage     37.20%   38.14%   +0.94%     
- Complexity     2559     2621      +62     
============================================
  Files           435      436       +1     
  Lines         20020    20079      +59     
  Branches       2426     2443      +17     
============================================
+ Hits           7449     7660     +211     
+ Misses        11904    11711     -193     
- Partials        667      708      +41     
```


| [Impacted Files](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943?src=pr&amp;el=tree) | Coverage Δ | Complexity Δ | |
|---|---|---|---|
| [...ommon/task/sqoop/targets/TargetMysqlParameter.java](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943/diff?src=pr&amp;el=tree#diff-ZG9scGhpbnNjaGVkdWxlci1jb21tb24vc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2RvbHBoaW5zY2hlZHVsZXIvY29tbW9uL3Rhc2svc3Fvb3AvdGFyZ2V0cy9UYXJnZXRNeXNxbFBhcmFtZXRlci5qYXZh) | `100.00% &lt;ø&gt; (ø)` | `19.00 &lt;0.00&gt; (ø)` | |
| [...ver/master/consumer/TaskPriorityQueueConsumer.java](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943/diff?src=pr&amp;el=tree#diff-ZG9scGhpbnNjaGVkdWxlci1zZXJ2ZXIvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2RvbHBoaW5zY2hlZHVsZXIvc2VydmVyL21hc3Rlci9jb25zdW1lci9UYXNrUHJpb3JpdHlRdWV1ZUNvbnN1bWVyLmphdmE=) | `51.82% &lt;0.00%&gt; (-7.00%)` | `16.00 &lt;0.00&gt; (ø)` | |
| [.../sqoop/generator/sources/MysqlSourceGenerator.java](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943/diff?src=pr&amp;el=tree#diff-ZG9scGhpbnNjaGVkdWxlci1zZXJ2ZXIvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2RvbHBoaW5zY2hlZHVsZXIvc2VydmVyL3dvcmtlci90YXNrL3Nxb29wL2dlbmVyYXRvci9zb3VyY2VzL015c3FsU291cmNlR2VuZXJhdG9yLmphdmE=) | `58.00% &lt;42.85%&gt; (+58.00%)` | `5.00 &lt;0.00&gt; (+5.00)` | |
| [...inscheduler/common/task/sqoop/SqoopParameters.java](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943/diff?src=pr&amp;el=tree#diff-ZG9scGhpbnNjaGVkdWxlci1jb21tb24vc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2RvbHBoaW5zY2hlZHVsZXIvY29tbW9uL3Rhc2svc3Fvb3AvU3Fvb3BQYXJhbWV0ZXJzLmphdmE=) | `74.00% &lt;58.06%&gt; (-6.00%)` | `25.00 &lt;12.00&gt; (+10.00)` | :arrow_down: |
| [...worker/task/sqoop/generator/SqoopJobGenerator.java](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943/diff?src=pr&amp;el=tree#diff-ZG9scGhpbnNjaGVkdWxlci1zZXJ2ZXIvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2RvbHBoaW5zY2hlZHVsZXIvc2VydmVyL3dvcmtlci90YXNrL3Nxb29wL2dlbmVyYXRvci9TcW9vcEpvYkdlbmVyYXRvci5qYXZh) | `74.07% &lt;72.72%&gt; (+74.07%)` | `10.00 &lt;2.00&gt; (+10.00)` | |
| [...he/dolphinscheduler/common/enums/SqoopJobType.java](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943/diff?src=pr&amp;el=tree#diff-ZG9scGhpbnNjaGVkdWxlci1jb21tb24vc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2RvbHBoaW5zY2hlZHVsZXIvY29tbW9uL2VudW1zL1Nxb29wSm9iVHlwZS5qYXZh) | `88.88% &lt;88.88%&gt; (ø)` | `3.00 &lt;3.00&gt; (?)` | |
| [...r/worker/task/sqoop/generator/CommonGenerator.java](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943/diff?src=pr&amp;el=tree#diff-ZG9scGhpbnNjaGVkdWxlci1zZXJ2ZXIvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2RvbHBoaW5zY2hlZHVsZXIvc2VydmVyL3dvcmtlci90YXNrL3Nxb29wL2dlbmVyYXRvci9Db21tb25HZW5lcmF0b3IuamF2YQ==) | `83.87% &lt;89.47%&gt; (+83.87%)` | `6.00 &lt;0.00&gt; (+6.00)` | |
| [.../org/apache/dolphinscheduler/api/enums/Status.java](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943/diff?src=pr&amp;el=tree#diff-ZG9scGhpbnNjaGVkdWxlci1hcGkvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2RvbHBoaW5zY2hlZHVsZXIvYXBpL2VudW1zL1N0YXR1cy5qYXZh) | `100.00% &lt;100.00%&gt; (ø)` | `5.00 &lt;0.00&gt; (ø)` | |
| [.../sqoop/generator/targets/MysqlTargetGenerator.java](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943/diff?src=pr&amp;el=tree#diff-ZG9scGhpbnNjaGVkdWxlci1zZXJ2ZXIvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2RvbHBoaW5zY2hlZHVsZXIvc2VydmVyL3dvcmtlci90YXNrL3Nxb29wL2dlbmVyYXRvci90YXJnZXRzL015c3FsVGFyZ2V0R2VuZXJhdG9yLmphdmE=) | `75.75% &lt;100.00%&gt; (+75.75%)` | `4.00 &lt;0.00&gt; (+4.00)` | |
| ... and [17 more](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943/diff?src=pr&amp;el=tree-more) | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943?src=pr&amp;el=continue).
&gt; **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
&gt; `Δ = absolute &lt;relative&gt; (impact)`, `ø = not affected`, `? = missing data`
&gt; Powered by [Codecov](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943?src=pr&amp;el=footer). Last update [2749c7e...a420644](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/2943?src=pr&amp;el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).

              </div></li><li><div>
                @break60 please front-end. thx
@zixi0825 hi, this pr for sqoop task
@743294668 bro, Can this pr meet your requirements?


              </div></li><li><div>
                front-end +1
              </div></li><li><div>
                Kudos, SonarCloud Quality Gate passed!

[&lt;img src='https://sonarsource.github.io/sonarcloud-github-static-resources/v2/common/bug.png' alt='Bug' width='16' height='16' /&gt;](https://sonarcloud.io/project/issues?id=apache-dolphinscheduler&amp;pullRequest=2943&amp;resolved=false&amp;types=BUG) [&lt;img src='https://sonarsource.github.io/sonarcloud-github-static-resources/v2/checks/RatingBadge/A.png' alt='A' width='16' height='16' /&gt;](https://sonarcloud.io/project/issues?id=apache-dolphinscheduler&amp;pullRequest=2943&amp;resolved=false&amp;types=BUG) [0 Bugs](https://sonarcloud.io/project/issues?id=apache-dolphinscheduler&amp;pullRequest=2943&amp;resolved=false&amp;types=BUG)  
[&lt;img src='https://sonarsource.github.io/sonarcloud-github-static-resources/v2/common/vulnerability.png' alt='Vulnerability' width='16' height='16' /&gt;](https://sonarcloud.io/project/issues?id=apache-dolphinscheduler&amp;pullRequest=2943&amp;resolved=false&amp;types=VULNERABILITY) [&lt;img src='https://sonarsource.github.io/sonarcloud-github-static-resources/v2/checks/RatingBadge/A.png' alt='A' width='16' height='16' /&gt;](https://sonarcloud.io/project/issues?id=apache-dolphinscheduler&amp;pullRequest=2943&amp;resolved=false&amp;types=VULNERABILITY) [0 Vulnerabilities](https://sonarcloud.io/project/issues?id=apache-dolphinscheduler&amp;pullRequest=2943&amp;resolved=false&amp;types=VULNERABILITY) (and [&lt;img src='https://sonarsource.github.io/sonarcloud-github-static-resources/v2/common/security_hotspot.png' alt='Security Hotspot' width='16' height='16' /&gt;](https://sonarcloud.io/project/issues?id=apache-dolphinscheduler&amp;pullRequest=2943&amp;resolved=false&amp;types=SECURITY_HOTSPOT) [0 Security Hotspots](https://sonarcloud.io/project/issues?id=apache-dolphinscheduler&amp;pullRequest=2943&amp;resolved=false&amp;types=SECURITY_HOTSPOT) to review)  
[&lt;img src='https://sonarsource.github.io/sonarcloud-github-static-resources/v2/common/code_smell.png' alt='Code Smell' width='16' height='16' /&gt;](https://sonarcloud.io/project/issues?id=apache-dolphinscheduler&amp;pullRequest=2943&amp;resolved=false&amp;types=CODE_SMELL) [&lt;img src='https://sonarsource.github.io/sonarcloud-github-static-resources/v2/checks/RatingBadge/A.png' alt='A' width='16' height='16' /&gt;](https://sonarcloud.io/project/issues?id=apache-dolphinscheduler&amp;pullRequest=2943&amp;resolved=false&amp;types=CODE_SMELL) [1 Code Smell](https://sonarcloud.io/project/issues?id=apache-dolphinscheduler&amp;pullRequest=2943&amp;resolved=false&amp;types=CODE_SMELL)

[&lt;img src='https://sonarsource.github.io/sonarcloud-github-static-resources/v2/checks/CoverageChart/60.png' alt='68.4%' width='16' height='16' /&gt;](https://sonarcloud.io/component_measures?id=apache-dolphinscheduler&amp;pullRequest=2943&amp;metric=new_coverage&amp;view=list) [68.4% Coverage](https://sonarcloud.io/component_measures?id=apache-dolphinscheduler&amp;pullRequest=2943&amp;metric=new_coverage&amp;view=list)  
[&lt;img src='https://sonarsource.github.io/sonarcloud-github-static-resources/v2/checks/Duplications/3.png' alt='0.0%' width='16' height='16' /&gt;](https://sonarcloud.io/component_measures?id=apache-dolphinscheduler&amp;pullRequest=2943&amp;metric=new_duplicated_lines_density&amp;view=list) [0.0% Duplication](https://sonarcloud.io/component_measures?id=apache-dolphinscheduler&amp;pullRequest=2943&amp;metric=new_duplicated_lines_density&amp;view=list)

&lt;img src='https://sonarsource.github.io/sonarcloud-github-static-resources/v2/common/message_warning.png' alt='warning' width='16' height='16' /&gt; The version of Java (1.8.0_252) you have used to run this analysis is deprecated and we will stop accepting it from October 2020. Please update to at least Java 11.
Read more [here](https://sonarcloud.io/documentation/upcoming/)



              </div></li></ol></div><div><b>github_pulls_reviews:</b> <ol><li><div>
                If return null, I see the follow-up code, the task will eventually be judged as successful.
              </div></li><li><div>
                yes, if null should throw a runtimeException
              </div></li></ol></div><div><b>jira_issues:</b> <ol></ol></div><div><b>jira_issues_comments:</b> <ol></ol></div></div></html>