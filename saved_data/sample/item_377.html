<!DOCTYPE html><html><div class="item-title">
        Item 377
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                *
   * This method will initialize the thread pool size to be used for creating the
   * max number of threads for a job
   
              </div></li><li><div>
                *
   * this variable is to enable/disable prefetch of data during merge sort while
   * reading data from sort temp files
   
              </div></li><li><div>
                *
   * This method will delete the local data load folder location after data load is complete
   *
   * @param loadModel
   * @param segmentName
   
              </div></li><li><div>
                *
   * This method will get the store location for the given path, segment id and partition id
   *
   * @param carbonStorePath
   * @param dbName
   * @param tableName
   * @param partitionCount
   * @param segmentId
   
              </div></li><li><div>
                 form local store location
              </div></li><li><div>
                *
   * This method will get the store location for the given path, segment id and partition id
   *
   * @return data directory path
   
              </div></li><li><div>
                *
   * This method will read the records from sort temp file and keep it in a buffer
   *
   * @param numberOfRecords
   * @return
   * @throws CarbonSortKeyAndGroupByException
   
              </div></li><li><div>
                 notify the consumer thread when index at which object is to be inserted
 becomes equal to current index from where data has to be picked for writing
              </div></li><li><div>
                 insert the object in array according to sequence number
              </div></li><li><div>
                *
     * array of blocklet data holder objects
     
              </div></li><li><div>
                *
   * a private class that will hold the data for blocklets
   
              </div></li><li><div>
                *
   * This method will close writer execution service and get the node holders and
   * add them to node holder list
   *
   * @param service the service to shutdown
   * @throws CarbonDataWriterException
   
              </div></li><li><div>
                *
     * Computes a result, or throws an exception if unable to do so.
     *
     * @return computed result
     * @throws Exception if unable to compute a result
     
              </div></li><li><div>
                *
   * a private class which will take each blocklet in order and write to a file
   
              </div></li><li><div>
                *
   * This method will iterate through future task list and check if any exception
   * occurred during the thread execution
   *
   * @param taskList
   * @throws CarbonDataWriterException
   
              </div></li><li><div>
                *
   * This method will reset the block processing count
   
              </div></li><li><div>
                *
   * Producer which will process data equivalent to 1 blocklet size
   
              </div></li><li><div>
                *
   * integer that will be incremented for every new blocklet submitted to producer for processing
   * the data and decremented every time consumer fetches the blocklet for writing
   
              </div></li><li><div>
                *
   * Consumer class will get one blocklet data at a time and submit for writing
   
              </div></li><li><div>
                *
     * index from which data node holder object needs to be picked for writing
     
              </div></li><li><div>
                *
     * flag to check whether the producer has completed processing for holder
     * object which is required to be picked form an index
     
              </div></li><li><div>
                 if entry count reaches to leaf node size then we are ready to
 write
 this to leaf node file and update the intermediate files
              </div></li><li><div>
                *
   * number of cores configured
   
              </div></li><li><div>
                *
   * semaphore which will used for managing node holder objects
   
              </div></li><li><div>
                 wait until all blocklets have been finished writing
              </div></li><li><div>
                *
   * data directory location in carbon store path
   
              </div></li><li><div>
                *
   * This class will hold the holder objects and manage producer and consumer for reading
   * and writing the blocklet data
   
              </div></li><li><div>
                 reset current index when it reaches length of node holder array
              </div></li><li><div>
                *
   * counter that incremented for every job submitted to data writer thread
   
              </div></li><li><div>
                *
   * flag to check whether all blocklets have been finished writing
   
              </div></li><li><div>
                *
     * @param nodeHolder
     * @param index
     
              </div></li><li><div>
                *
     * @return a node holder object
     * @throws InterruptedException if consumer thread is interrupted
     
              </div></li><li><div>
                 still some data is present in stores if entryCount is more
              </div></li><li><div>
                 if node holder is null means producer thread processing the data which has to
 be inserted at this current index has not completed yet
              </div></li><li><div>
                *
   * carbon data directory path
   
              </div></li><li><div>
                *
   * executorService
   
              </div></li><li><div>
                *
     * complete path along with file name which needs to be copied to
     * carbon store path
     
              </div></li><li><div>
                *
   * This method will copy the given file to carbon store location
   *
   * @param localFileName local file name with full path
   * @throws CarbonDataWriterException
   
              </div></li><li><div>
                *
   * This method will rename carbon data file from in progress status to normal
   *
   * @throws CarbonDataWriterException
   
              </div></li><li><div>
                 open channel for new data file
              </div></li><li><div>
                *
   * file size at any given point
   
              </div></li><li><div>
                *
   * data directory location in carbon store path
   
              </div></li><li><div>
                *
   * This method will read the local carbon data file and write to carbon data file in HDFS
   *
   * @param carbonStoreFilePath
   * @param localFilePath
   * @param bufferSize
   * @param blockSize
   * @throws IOException
   
              </div></li><li><div>
                *
   * This method will return max of block size and file size
   *
   * @param blockSize
   * @param fileSize
   * @return
   
              </div></li><li><div>
                *
     * Computes a result, or throws an exception if unable to do so.
     *
     * @return computed result
     * @throws Exception if unable to compute a result
     
              </div></li><li><div>
                 block size should be exactly divisible by 512 which is  maintained by HDFS as bytes
 per checksum, dfs.bytes-per-checksum=512 must divide block size
              </div></li><li><div>
                 rename carbon data file from in progress status to actual
              </div></li><li><div>
                *
   * This method will close the executor service which is used for copying carbon
   * data files to carbon store path
   *
   * @throws CarbonDataWriterException
   
              </div></li><li><div>
                *
   * If node holder flag is enabled the object will be added to list
   * and all the blocklets will be return together. If disabled then this
   * method will itself will call for writing the fact data
   *
   * @param holder
   
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> [Issue-324] Data loading performance optimization (#444)
                </div><div><b>message:</b> [Issue-324] Data loading performance optimization (#444)

1. Enabled prefetch - code modifications done to make prefetch work according to new code
2. Moved mdkey processing code to a different thread
3. Moved copying of file from local to hdfs as soon as file is completed writing
                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> [CARBONDATA -541] Tescases for dictionary subpackage in processing added
                </div><div><b>body:</b> 
                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div>
                Build Failed  with Spark 1.5.2, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder/228/

              </div></li></ol></div><div><b>github_pulls_reviews:</b> <ol></ol></div><div><b>jira_issues:</b> <ol></ol></div><div><b>jira_issues_comments:</b> <ol></ol></div></div></html>