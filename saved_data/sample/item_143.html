<!DOCTYPE html><html><div class="item-title">
        Item 143
      </div> <div class="item-details"><div><b>git_comments:</b> <ol></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> HADOOP-2040 Hudson hangs AFTER test has finished
                </div><div><b>message:</b> HADOOP-2040 Hudson hangs AFTER test has finished
Second commit against this jira


git-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@595406 13f79535-47bb-0310-9956-ffa450edef68

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol></ol></div><div><b>github_pulls_comments:</b> <ol></ol></div><div><b>github_pulls_reviews:</b> <ol></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> [hbase] Hudson hangs AFTER test has finished
                </div><div><b>description:</b> Weird.  Last night TestBloomFilter was hung after junit had printed test had completed without error.  Just now, I noticed a hung TestHStore -- again after junit had printed out test had succeeded (Nigel Daley has reported he's seen at least two hangs in TestHStoreFile, perhaps in same location).

Last night and just now I was unable to get a thread dump.

Here is log from around this evenings hang:

{code}
...
    [junit] 2007-10-12 04:19:28,477 INFO  [main] org.apache.hadoop.hbase.TestHStoreFile.testOutOfRangeMidkeyHalfMapFile(TestHStoreFile.java:366): Last bottom when key &gt; top: zz/zz/1192162768317
    [junit] 2007-10-12 04:19:28,493 WARN  [IPC Server handler 0 on 36620] org.apache.hadoop.dfs.FSDirectory.unprotectedDelete(FSDirectory.java:400): DIR* FSDirectory.unprotectedDelete: failed to remove /testOutOfRangeMidkeyHalfMapFile because it does not exist
    [junit] Shutting down the Mini HDFS Cluster
    [junit] Shutting down DataNode 1
    [junit] Shutting down DataNode 0
    [junit] 2007-10-12 04:19:29,316 WARN  [org.apache.hadoop.dfs.PendingReplicationBlocks$PendingReplicationMonitor@ed9f47] org.apache.hadoop.dfs.PendingReplicationBlocks$PendingReplicationMonitor.run(PendingReplicationBlocks.java:186): PendingReplicationMonitor thread received exception. java.lang.InterruptedException: sleep interrupted
    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 16.274 sec
    [junit] Running org.apache.hadoop.hbase.TestHTable
    [junit] Starting DataNode 0 with dfs.data.dir: /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data1,/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data2
    [junit] Starting DataNode 1 with dfs.data.dir: /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data3,/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data4
    [junit] 2007-10-12 05:21:48,332 INFO  [main] org.apache.hadoop.hbase.HMaster.&lt;init&gt;(HMaster.java:862): Root region dir: /hbase/hregion_-ROOT-,,0
...
{code}

Notice the hour of elapsed (hung) time in above.
                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol><li><div>
                Looks like it hung again in same build -- #931 -- but this time in a test that hasn't been prone to hanging, TestListTables.  Again I can't get a thread dump but log is interesting on the way out:

{code}
    [junit] Shutting down the Mini HDFS Cluster
    [junit] Shutting down DataNode 1
    [junit] 2007-10-12 05:23:16,082 WARN  [DataNode: [/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data3,/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data4]] org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:596): java.io.IOException: java.lang.InterruptedException
    [junit] Shutting down DataNode 0
    [junit] 	at org.apache.hadoop.fs.ShellCommand.runCommand(ShellCommand.java:59)
    [junit] 	at org.apache.hadoop.fs.ShellCommand.run(ShellCommand.java:42)
    [junit] 	at org.apache.hadoop.fs.DU.getUsed(DU.java:52)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.getDfsUsed(FSDataset.java:299)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.getDfsUsed(FSDataset.java:396)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.getDfsUsed(FSDataset.java:495)
    [junit] 	at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:520)
    [junit] 	at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1494)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

    [junit] 2007-10-12 05:23:16,349 WARN  [DataNode: [/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data1,/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data2]] org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:596): java.io.InterruptedIOException
    [junit] 	at java.net.SocketOutputStream.socketWrite0(Native Method)
    [junit] 	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
    [junit] 	at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
    [junit] 	at org.apache.hadoop.ipc.Client$Connection$2.write(Client.java:192)
    [junit] 	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
    [junit] 	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
    [junit] 	at java.io.DataOutputStream.flush(DataOutputStream.java:106)
    [junit] 	at org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:327)
    [junit] 	at org.apache.hadoop.ipc.Client.call(Client.java:474)
    [junit] 	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:184)
    [junit] 	at org.apache.hadoop.dfs.$Proxy1.sendHeartbeat(Unknown Source)
    [junit] 	at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:520)
    [junit] 	at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1494)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

    [junit] 2007-10-12 05:23:16,351 WARN  [org.apache.hadoop.dfs.PendingReplicationBlocks$PendingReplicationMonitor@157c2bd] org.apache.hadoop.dfs.PendingReplicationBlocks$PendingReplicationMonitor.run(PendingReplicationBlocks.java:186): PendingReplicationMonitor thread received exception. java.lang.InterruptedException: sleep interrupted
    [junit] 2007-10-12 05:23:16,610 INFO  [main] org.apache.hadoop.hbase.MiniHBaseCluster.shutdown(MiniHBaseCluster.java:424): Shutting down FileSystem
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 36.108 sec
{code}

It reports tests succeeded but just before hand its reporting and interrupted flush.  I wonder if interrupt broke the flush.  It would be interesting to know (for HADOOP-1924).
              </div></li><li><div>
                Patch to end thread dumping to hbase unit test teardown so can learn more about whats going on in these end-of-test hangs.  Intend to let it run on hudson for a day or two.
              </div></li><li><div>
                Passing to hudson.
              </div></li><li><div>
                The hudson patch build #940 is hung in TestHBaseCluster.  Here is the end-of-test thread dump (just before it prints test completed w/o error):
{code}
    [junit] Process Thread Dump: Temporary end-of-test thread dump debugging HADOOP-2040: testHBaseCluster
    [junit] 6 active threads
    [junit] Thread 53 (process reaper):
    [junit]   State: RUNNABLE
    [junit]   Blocked count: 0
    [junit]   Waited count: 0
    [junit]   Stack:
    [junit]     java.lang.UNIXProcess.waitForProcessExit(Native Method)
    [junit]     java.lang.UNIXProcess.access$900(UNIXProcess.java:17)
    [junit]     java.lang.UNIXProcess$2$1.run(UNIXProcess.java:86)
    [junit] Thread 30 (org.apache.hadoop.io.ObjectWritable Connection Culler):
    [junit]   State: TIMED_WAITING
    [junit]   Blocked count: 0
    [junit]   Waited count: 0
    [junit]   Stack:
    [junit]     java.lang.Thread.sleep(Native Method)
    [junit]     org.apache.hadoop.ipc.Client$ConnectionCuller.run(Client.java:404)
    [junit] Thread 4 (Signal Dispatcher):
    [junit]   State: RUNNABLE
    [junit]   Blocked count: 0
    [junit]   Waited count: 0
    [junit]   Stack:
    [junit] Thread 3 (Finalizer):
    [junit]   State: WAITING
    [junit]   Blocked count: 137
    [junit]   Waited count: 21
    [junit]   Waiting on java.lang.ref.ReferenceQueue$Lock@1931579
    [junit]   Stack:
    [junit]     java.lang.Object.wait(Native Method)
    [junit]     java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:116)
    [junit]     java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:132)
    [junit]     java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)
    [junit] Thread 2 (Reference Handler):
    [junit]   State: WAITING
    [junit]   Blocked count: 209
    [junit]   Waited count: 17
    [junit]   Waiting on java.lang.ref.Reference$Lock@166bfd8
    [junit]   Stack:
    [junit]     java.lang.Object.wait(Native Method)
    [junit]     java.lang.Object.wait(Object.java:474)
    [junit]     java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
    [junit] Thread 1 (main):
    [junit]   State: RUNNABLE
    [junit]   Blocked count: 44
    [junit]   Waited count: 4095
    [junit]   Stack:
    [junit]     sun.management.ThreadImpl.getThreadInfo0(Native Method)
    [junit]     sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:144)
    [junit]     sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:120)
    [junit]     org.apache.hadoop.util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:114)
    [junit]     org.apache.hadoop.hbase.HBaseClusterTestCase.tearDown(HBaseClusterTestCase.java:94)
    [junit]     junit.framework.TestCase.runBare(TestCase.java:130)
    [junit]     junit.framework.TestResult$1.protect(TestResult.java:106)
    [junit]     junit.framework.TestResult.runProtected(TestResult.java:124)
    [junit]     junit.framework.TestResult.run(TestResult.java:109)
    [junit]     junit.framework.TestCase.run(TestCase.java:118)
    [junit]     junit.framework.TestSuite.runTest(TestSuite.java:208)
    [junit]     junit.framework.TestSuite.run(TestSuite.java:203)
    [junit]     org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)
    [junit]     org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)
    [junit]     org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 64.096 sec
{code}
Looking at previous unit tests, the odd-man-out is the unix process waiting on process end.  Whats that from?

I can't get a thread dump at [hudson] dateSat Oct 13 04:17:30 GMT 2007.  Killing current test... so build can move on.
              </div></li><li><div><div><b>body:</b> +1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12367672/endoftesttd.patch
against trunk revision r584336.

    @author +1.  The patch does not contain any @author tags.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new compiler warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/940/testReport/
Findbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/940/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/940/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/940/console

This message is automatically generated.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Thread dump looks like http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4811767.  1.5JVMs are not supposed to have the issue.


              </div></li><li><div>
                Hudson is hung.  Here is tail of log.

{code}
    [junit] 2007-11-09 08:20:49,385 DEBUG [main] org.apache.hadoop.hbase.TestLogRolling.countLogFiles(TestLogRolling.java:174): number of log files: 1
    [junit] 2007-11-09 08:20:49,386 INFO  [main] org.apache.hadoop.hbase.TestLogRolling.testLogRolling(TestLogRolling.java:191): Finished writing. There are 1 log files. Sleeping to let cache flusher and log roller run
    [junit] 2007-11-09 08:20:49,386 DEBUG [main] org.apache.hadoop.hbase.LocalHBaseCluster.shutdown(LocalHBaseCluster.java:202): Shutting down HBase Cluster
    [junit] 2007-11-09 08:20:49,488 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.run(HRegionServer.java:502): Got regionserver stop message
    [junit] 2007-11-09 08:20:49,488 INFO  [RegionServer:0] org.apache.hadoop.hbase.Leases.close(Leases.java:109): RegionServer:0 closing leases
    [junit] 2007-11-09 08:20:49,489 INFO  [RegionServer:0.leaseChecker] org.apache.hadoop.hbase.Chore.run(Chore.java:62): RegionServer:0.leaseChecker exiting
    [junit] 2007-11-09 08:20:49,489 INFO  [RegionServer:0] org.apache.hadoop.hbase.Leases.close(Leases.java:123): RegionServer:0 closed leases
    [junit] 2007-11-09 08:20:49,490 INFO  [RegionServer:0.logRoller] org.apache.hadoop.hbase.Chore.run(Chore.java:62): RegionServer:0.logRoller exiting
    [junit] 2007-11-09 08:20:49,607 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,608 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,608 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,608 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,608 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,608 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,609 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,609 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,609 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,609 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,609 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,610 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,610 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,610 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,610 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,610 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,611 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,611 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,611 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,611 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,612 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,612 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,612 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,612 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,612 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,613 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,613 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,613 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,613 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,613 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,614 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,614 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,614 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,614 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,615 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,615 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,615 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,615 WARN  [org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1835282] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,814 WARN  [IPC Server handler 5 on 58346] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,830 DEBUG [RegionServer:0.cacheFlusher] org.apache.hadoop.hbase.HStore.flushCacheHelper(HStore.java:504): Added -1547818355/info/8261001142386214874 with sequence id 2208 and size 16.8k
    [junit] 2007-11-09 08:20:49,830 DEBUG [RegionServer:0.cacheFlusher] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:919): Finished memcache flush for region testLogRolling,row1025,1194596368242 in 523ms
    [junit] 2007-11-09 08:20:49,831 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.closeAllRegions(HRegionServer.java:971): closing region -ROOT-,,0
    [junit] 2007-11-09 08:20:49,831 INFO  [RegionServer:0.splitOrCompactChecker] org.apache.hadoop.hbase.Chore.run(Chore.java:62): RegionServer:0.splitOrCompactChecker exiting
    [junit] 2007-11-09 08:20:49,832 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:847): Started memcache flush for region -ROOT-,,0. Size 0.0
    [junit] 2007-11-09 08:20:49,832 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:865): Finished memcache flush; empty snapshot
    [junit] 2007-11-09 08:20:49,833 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HStore.close(HStore.java:419): closed -70236052/info
    [junit] 2007-11-09 08:20:49,833 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegion.close(HRegion.java:402): closed -ROOT-,,0
    [junit] 2007-11-09 08:20:49,833 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.closeAllRegions(HRegionServer.java:971): closing region .META.,,1
    [junit] 2007-11-09 08:20:49,833 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:847): Started memcache flush for region .META.,,1. Size 0.0
    [junit] 2007-11-09 08:20:49,833 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:865): Finished memcache flush; empty snapshot
    [junit] 2007-11-09 08:20:49,833 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HStore.close(HStore.java:419): closed 1028785192/info
    [junit] 2007-11-09 08:20:49,834 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegion.close(HRegion.java:402): closed .META.,,1
    [junit] 2007-11-09 08:20:49,834 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.closeAllRegions(HRegionServer.java:971): closing region testLogRolling,,1194596277787
    [junit] 2007-11-09 08:20:49,834 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:847): Started memcache flush for region testLogRolling,,1194596277787. Size 0.0
    [junit] 2007-11-09 08:20:49,834 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:865): Finished memcache flush; empty snapshot
    [junit] 2007-11-09 08:20:49,835 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HStore.close(HStore.java:419): closed 216611736/info
    [junit] 2007-11-09 08:20:49,835 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegion.close(HRegion.java:402): closed testLogRolling,,1194596277787
    [junit] 2007-11-09 08:20:49,835 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.closeAllRegions(HRegionServer.java:971): closing region testLogRolling,row0513,1194596368241
    [junit] 2007-11-09 08:20:49,835 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:847): Started memcache flush for region testLogRolling,row0513,1194596368241. Size 0.0
    [junit] 2007-11-09 08:20:49,835 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:865): Finished memcache flush; empty snapshot
    [junit] 2007-11-09 08:20:49,836 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HStore.close(HStore.java:419): closed 1463872906/info
    [junit] 2007-11-09 08:20:49,836 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegion.close(HRegion.java:402): closed testLogRolling,row0513,1194596368241
    [junit] 2007-11-09 08:20:49,836 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.closeAllRegions(HRegionServer.java:971): closing region testLogRolling,row1025,1194596368242
    [junit] 2007-11-09 08:20:49,836 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:847): Started memcache flush for region testLogRolling,row1025,1194596368242. Size 0.0
    [junit] 2007-11-09 08:20:49,836 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:865): Finished memcache flush; empty snapshot
    [junit] 2007-11-09 08:20:49,837 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HStore.close(HStore.java:419): closed -1547818355/info
    [junit] 2007-11-09 08:20:49,837 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegion.close(HRegion.java:402): closed testLogRolling,row1025,1194596368242
    [junit] 2007-11-09 08:20:49,837 DEBUG [RegionServer:0] org.apache.hadoop.hbase.HLog.close(HLog.java:382): closing log writer in /hbase/log_140.211.11.75_-2039724685788569167_58358
    [junit] 2007-11-09 08:20:49,838 WARN  [IPC Server handler 3 on 58346] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:20:49,848 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.run(HRegionServer.java:603): telling master that region server is shutting down at: 140.211.11.75:58358
    [junit] 2007-11-09 08:20:49,849 DEBUG [IPC Server handler 4 on 60000] org.apache.hadoop.hbase.HMaster.regionServerReport(HMaster.java:1316): Region server 140.211.11.75:58358: MSG_REPORT_EXITING -- cancelling lease
    [junit] 2007-11-09 08:20:49,849 INFO  [IPC Server handler 4 on 60000] org.apache.hadoop.hbase.HMaster.cancelLease(HMaster.java:1438): Cancelling lease for 140.211.11.75:58358
    [junit] 2007-11-09 08:20:49,849 INFO  [IPC Server handler 4 on 60000] org.apache.hadoop.hbase.HMaster.regionServerReport(HMaster.java:1323): Region server 140.211.11.75:58358: MSG_REPORT_EXITING -- lease cancelled
    [junit] 2007-11-09 08:20:49,850 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.run(HRegionServer.java:610): stopping server at: 140.211.11.75:58358
    [junit] 2007-11-09 08:20:49,977 INFO  [RegionServer:0.worker] org.apache.hadoop.hbase.HRegionServer$Worker.run(HRegionServer.java:920): worker thread exiting
    [junit] 2007-11-09 08:20:49,977 INFO  [RegionServer:0] org.apache.hadoop.hbase.HRegionServer.run(HRegionServer.java:615): RegionServer:0 exiting
    [junit] 2007-11-09 08:20:50,947 INFO  [HMaster.metaScanner] org.apache.hadoop.hbase.Chore.run(Chore.java:62): HMaster.metaScanner exiting
    [junit] 2007-11-09 08:20:50,948 INFO  [HMaster] org.apache.hadoop.hbase.Leases.close(Leases.java:109): HMaster closing leases
    [junit] 2007-11-09 08:20:50,947 INFO  [HMaster.rootScanner] org.apache.hadoop.hbase.Chore.run(Chore.java:62): HMaster.rootScanner exiting
    [junit] 2007-11-09 08:20:50,949 INFO  [HMaster.leaseChecker] org.apache.hadoop.hbase.Chore.run(Chore.java:62): HMaster.leaseChecker exiting
    [junit] 2007-11-09 08:20:50,949 INFO  [HMaster] org.apache.hadoop.hbase.Leases.close(Leases.java:123): HMaster closed leases
    [junit] 2007-11-09 08:20:50,949 INFO  [HMaster] org.apache.hadoop.hbase.HMaster.run(HMaster.java:1163): HMaster main thread exiting
    [junit] 2007-11-09 08:20:50,949 INFO  [main] org.apache.hadoop.hbase.LocalHBaseCluster.shutdown(LocalHBaseCluster.java:226): Shutdown HMaster 1 region server(s)
    [junit] Shutting down the Mini HDFS Cluster
    [junit] Shutting down DataNode 1
    [junit] 2007-11-09 08:20:51,709 WARN  [DataNode: [/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data3,/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data4]] org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:617): java.io.IOException: java.lang.InterruptedException
    [junit] 	at org.apache.hadoop.fs.ShellCommand.runCommand(ShellCommand.java:59)
    [junit] 	at org.apache.hadoop.fs.ShellCommand.run(ShellCommand.java:42)
    [junit] 	at org.apache.hadoop.fs.DU.getUsed(DU.java:52)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.getDfsUsed(FSDataset.java:299)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.getDfsUsed(FSDataset.java:396)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.getDfsUsed(FSDataset.java:495)
    [junit] 	at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:532)
    [junit] 	at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1695)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

    [junit] Shutting down DataNode 0
    [junit] 2007-11-09 08:20:52,252 WARN  [org.apache.hadoop.dfs.PendingReplicationBlocks$PendingReplicationMonitor@105b99f] org.apache.hadoop.dfs.PendingReplicationBlocks$PendingReplicationMonitor.run(PendingReplicationBlocks.java:186): PendingReplicationMonitor thread received exception. java.lang.InterruptedException: sleep interrupted
    [junit] 2007-11-09 08:20:52,570 ERROR [org.apache.hadoop.dfs.DataNode$DataXceiver@166c114] org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:839): DataXceiver: java.io.IOException: df: (/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data3) not a block device, directory or mounted resource
    [junit] 	at org.apache.hadoop.fs.ShellCommand.runCommand(ShellCommand.java:52)
    [junit] 	at org.apache.hadoop.fs.ShellCommand.run(ShellCommand.java:42)
    [junit] 	at org.apache.hadoop.fs.DF.getAvailable(DF.java:72)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.getAvailable(FSDataset.java:308)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.getNextVolume(FSDataset.java:386)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:580)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockReceiver.&lt;init&gt;(DataNode.java:1458)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:929)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:824)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

    [junit] Exception! java.io.IOException: No such file or directory
    [junit] 2007-11-09 08:20:52,864 ERROR [org.apache.hadoop.dfs.DataNode$DataXceiver@1c0b8a0] org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:839): DataXceiver: java.io.IOException: df: (/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data4) not a block device, directory or mounted resource
    [junit] 	at org.apache.hadoop.fs.ShellCommand.runCommand(ShellCommand.java:52)
    [junit] 	at org.apache.hadoop.fs.ShellCommand.run(ShellCommand.java:42)
    [junit] 	at org.apache.hadoop.fs.DF.getCapacity(DF.java:62)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.getCapacity(FSDataset.java:303)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.getAvailable(FSDataset.java:307)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.getNextVolume(FSDataset.java:386)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:580)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockReceiver.&lt;init&gt;(DataNode.java:1458)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:929)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:824)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

    [junit] 2007-11-09 08:20:52,864 WARN  [org.apache.hadoop.dfs.DataNode$DataTransfer@6e3e5e] org.apache.hadoop.dfs.DataNode$DataTransfer.run(DataNode.java:1668): Failed to transfer blk_-379738272651084333 to 127.0.0.1:50011 got java.io.IOException: operation failed at /127.0.0.1
    [junit] 	at org.apache.hadoop.dfs.DataNode.receiveResponse(DataNode.java:725)
    [junit] 	at org.apache.hadoop.dfs.DataNode.access$200(DataNode.java:80)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataTransfer.run(DataNode.java:1664)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 344.356 sec
    [junit] 2007-11-09 08:20:52,864 WARN  [org.apache.hadoop.dfs.DataNode$DataXceiver@b34646] org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:995): Error writing reply back to /127.0.0.1 for writing block blk_-3764842785131980349
    [junit] java.net.SocketException: Broken pipe
    [junit] 	at java.net.SocketOutputStream.socketWrite0(Native Method)
    [junit] 	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
    [junit] 	at java.net.SocketOutputStream.write(SocketOutputStream.java:115)
    [junit] 	at java.io.DataOutputStream.writeShort(DataOutputStream.java:151)
    [junit] 	at org.apache.hadoop.dfs.DataNode.sendResponse(DataNode.java:737)
    [junit] 	at org.apache.hadoop.dfs.DataNode.access$300(DataNode.java:80)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:993)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:824)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

    [junit] Exception! java.io.IOException: No such file or directory
    [junit] 2007-11-09 08:20:53,748 ERROR [org.apache.hadoop.dfs.DataNode$DataXceiver@b34646] org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:839): DataXceiver: java.io.IOException: No such file or directory
    [junit] 	at java.io.UnixFileSystem.createFileExclusively(Native Method)
    [junit] 	at java.io.File.createNewFile(File.java:850)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.createTmpFile(FSDataset.java:329)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.createTmpFile(FSDataset.java:606)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:582)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockReceiver.&lt;init&gt;(DataNode.java:1458)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:929)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:824)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

    [junit] 2007-11-09 08:20:53,748 ERROR [org.apache.hadoop.dfs.DataNode$DataXceiver@1de4376] org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:839): DataXceiver: java.io.IOException: du: /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data: No such file or directory
    [junit] 	at org.apache.hadoop.fs.ShellCommand.runCommand(ShellCommand.java:52)
    [junit] 	at org.apache.hadoop.fs.ShellCommand.run(ShellCommand.java:42)
    [junit] 	at org.apache.hadoop.fs.DU.getUsed(DU.java:52)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.getDfsUsed(FSDataset.java:299)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.getAvailable(FSDataset.java:307)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.getNextVolume(FSDataset.java:386)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:580)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockReceiver.&lt;init&gt;(DataNode.java:1458)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:929)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:824)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

    [junit] 2007-11-09 08:20:53,749 ERROR [org.apache.hadoop.dfs.DataNode$DataXceiver@71949b] org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:839): DataXceiver: java.io.IOException: No such file or directory
    [junit] 	at java.io.UnixFileSystem.createFileExclusively(Native Method)
    [junit] 	at java.io.File.createNewFile(File.java:850)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.createTmpFile(FSDataset.java:329)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.createTmpFile(FSDataset.java:606)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:582)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockReceiver.&lt;init&gt;(DataNode.java:1458)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:929)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:824)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

    [junit] Exception! java.io.IOException: No such file or directory
    [junit] 2007-11-09 08:20:53,800 ERROR [org.apache.hadoop.dfs.DataNode$DataXceiver@8f3d27] org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:839): DataXceiver: java.io.IOException: No such file or directory
    [junit] 	at java.io.UnixFileSystem.createFileExclusively(Native Method)
    [junit] 	at java.io.File.createNewFile(File.java:850)
    [junit] 	at org.apache.hadoop.dfs.FSDataset$FSVolume.createTmpFile(FSDataset.java:329)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.createTmpFile(FSDataset.java:606)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:582)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockReceiver.&lt;init&gt;(DataNode.java:1458)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:929)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:824)
    [junit] 	at java.lang.Thread.run(Thread.java:595)

    [junit] 2007-11-09 08:20:53,801 WARN  [org.apache.hadoop.dfs.DataNode$DataTransfer@1949f78] org.apache.hadoop.dfs.DataNode$DataTransfer.run(DataNode.java:1668): Failed to transfer blk_1352514030345870875 to 127.0.0.1:50011 got java.io.IOException: operation failed at /127.0.0.1
    [junit] 	at org.apache.hadoop.dfs.DataNode.receiveResponse(DataNode.java:725)
    [junit] 	at org.apache.hadoop.dfs.DataNode.access$200(DataNode.java:80)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataTransfer.run(DataNode.java:1664)
    [junit] 	at java.lang.Thread.run(Thread.java:595)
{code}

Test had not reported itself done.  Are these du'ings and unix process invocations of interest? (Check).
              </div></li><li><div>
                Looking more at this hang from last night, fs was sick from near the get-go:
{code}
    [junit] Starting DataNode 0 with dfs.data.dir: /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data1,/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data2
    [junit] Starting DataNode 1 with dfs.data.dir: /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data3,/export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/contrib/hbase/test/data/dfs/data/data4
    [junit] 2007-11-09 08:15:10,894 INFO  [main] org.apache.hadoop.hbase.HMaster.&lt;init&gt;(HMaster.java:895): Root region dir: /hbase/hregion_-70236052
    [junit] 2007-11-09 08:15:10,982 INFO  [main] org.apache.hadoop.hbase.HMaster.&lt;init&gt;(HMaster.java:904): bootstrap: creating ROOT and first META regions
    [junit] 2007-11-09 08:15:11,255 WARN  [main] org.apache.hadoop.util.NativeCodeLoader.&lt;clinit&gt;(NativeCodeLoader.java:51): Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    [junit] 2007-11-09 08:15:11,268 INFO  [main] org.apache.hadoop.hbase.HLog.rollWriter(HLog.java:298): new log writer created at /hbase/hregion_-70236052/log/hlog.dat.000
    [junit] 2007-11-09 08:15:11,341 DEBUG [main] org.apache.hadoop.hbase.HStore.&lt;init&gt;(HStore.java:182): starting -70236052/info (no reconstruction log)
    [junit] 2007-11-09 08:15:11,346 DEBUG [main] org.apache.hadoop.hbase.HStore.&lt;init&gt;(HStore.java:218): maximum sequence id for hstore -70236052/info is -1
    [junit] 2007-11-09 08:15:11,348 DEBUG [main] org.apache.hadoop.hbase.HRegion.&lt;init&gt;(HRegion.java:289): Next sequence id for region -ROOT-,,0 is 0
    [junit] 2007-11-09 08:15:11,351 INFO  [main] org.apache.hadoop.hbase.HRegion.&lt;init&gt;(HRegion.java:315): region -ROOT-,,0 available
    [junit] 2007-11-09 08:15:11,368 INFO  [main] org.apache.hadoop.hbase.HLog.rollWriter(HLog.java:298): new log writer created at /hbase/hregion_1028785192/log/hlog.dat.000
    [junit] 2007-11-09 08:15:11,379 DEBUG [main] org.apache.hadoop.hbase.HStore.&lt;init&gt;(HStore.java:182): starting 1028785192/info (no reconstruction log)
    [junit] 2007-11-09 08:15:11,382 DEBUG [main] org.apache.hadoop.hbase.HStore.&lt;init&gt;(HStore.java:218): maximum sequence id for hstore 1028785192/info is -1
    [junit] 2007-11-09 08:15:11,384 DEBUG [main] org.apache.hadoop.hbase.HRegion.&lt;init&gt;(HRegion.java:289): Next sequence id for region .META.,,1 is 0
    [junit] 2007-11-09 08:15:11,391 INFO  [main] org.apache.hadoop.hbase.HRegion.&lt;init&gt;(HRegion.java:315): region .META.,,1 available
    [junit] 2007-11-09 08:15:11,426 DEBUG [main] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:847): Started memcache flush for region -ROOT-,,0. Size 86.0
    [junit] 2007-11-09 08:15:11,428 DEBUG [main] org.apache.hadoop.hbase.HRegion.internalFlushcache(HRegion.java:876): Snapshotted memcache for region -ROOT-,,0 with sequence id 1 and entries 1
    [junit] 2007-11-09 08:15:11,519 WARN  [IPC Server handler 2 on 58346] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:15:11,988 WARN  [IPC Server handler 5 on 58346] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:15:12,036 WARN  [IPC Server handler 0 on 58346] org.apache.hadoop.dfs.ReplicationTargetChooser.chooseTarget(ReplicationTargetChooser.java:177): Not able to place enough replicas, still in need of 1
    [junit] 2007-11-09 08:15:12,098 DEBUG [main] org.apache.hadoop.hbase.HStore.flushCacheHelper(HStore.java:504): Added -70236052/info/4372126676279784460 with sequence id 1 and size 210.0
....
{code}
Failure looks unrelated though the du's at end of test might be whats running the hung unix processes.
              </div></li><li><div>
                Don't know whether it will help.
But it looks like you are shutting down MiniDFSCluster in the middle of file creates.
Some of the data-nodes were transferring data (I see writeBlock()) during the shutdown.
And they keep doing transfers even after the test is completed.
              </div></li><li><div><div><b>body:</b> Thanks for jumping in Konstantin.

Code-wise the shutdown looks orderly... we shutdown hbase, not leaving the shutdown method till all hbase servers have exited.  We then call the shutdown on the mini DFS which waits first on datanodes to go down and then the namenode (The logging above aligns; see 'Shutting down the Mini HDFS Cluster' happening after the exit of the HMaster thread).

But something untoward is going on.  My working theory is that an interrupted invocation of a native unix command ('du' by the FileSystem?) is hanging hudson with some frequency (See 12/Oct/07 09:18 PM comment above).  If the FS were quiescent, I'd imagine the hang could be avoided.

This failure has loads of complaints of 'Not able to place enough replicas, still in need of 1'.   They startup soon after the test starts.  What you think of that?  Maybe the dfs is not quiescent because its trying to replicate.  But I see that in MinDFSCluster it sets the replication to the number of data nodes.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Patch flips closing filesystem with mindfscluster shutdown.  This is how tests for dfs do it and they don't seem to have our hanging problem (Hudson hung this morning running testinfoservers).
              </div></li><li><div>
                Get a new +1 for this latest patch.
              </div></li><li><div>
                +1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12369448/mhbc.patch
against trunk revision r594460.

    @author +1.  The patch does not contain any @author tags.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new compiler warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1094/testReport/
Findbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1094/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1094/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1094/console

This message is automatically generated.
              </div></li><li><div>
                Committed.  Let it bake a while over on hudson.  See if it reduces hangs before closing.
              </div></li><li><div>
                hudson was hung just now for 7 hours in TestHRegion.  TestHRegion is unorthodox -- intentionally -- in that it doesn't use inherit from HBaseClusterTestCase so it didn't pick up the changes made w/ mhbc.patch.  Hopefully its this anomaly that was responsible for the hang.  Attaching a patch that has TestHRegion do like the rest of the hbase (and hadoop dfs) tests shutting down the filesystem first and then the dfs cluster.
              </div></li><li><div>
                Builds locally.  Trying hudson.
              </div></li><li><div>
                Integrated in Hadoop-Nightly #303 (See [http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/303/])
              </div></li><li><div>
                +1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12369568/thr.patch
against trunk revision r595246.

    @author +1.  The patch does not contain any @author tags.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new compiler warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1101/testReport/
Findbugs warnings: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1101/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1101/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1101/console

This message is automatically generated.
              </div></li><li><div>
                Committed patch to TestHRegion, the second commit on this JIRA.
              </div></li><li><div>
                Integrated in Hadoop-Nightly #305 (See [http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/305/])
              </div></li><li><div>
                Hasn't been a recurrence in a while.  Reopen if start seeing it again.
              </div></li></ol></div></div></html>