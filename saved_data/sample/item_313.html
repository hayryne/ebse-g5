<!DOCTYPE html><html><div class="item-title">
        Item 313
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                 ordinary numpy array
              </div></li><li><div>
                 0-dim
              </div></li><li><div>
                 0-size
              </div></li><li><div>
                 Test zero_copy
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> [MXNET-1398] Enable zero-copy from numpy to MXNet NDArray (#14733)
                </div><div><b>message:</b> [MXNET-1398] Enable zero-copy from numpy to MXNet NDArray (#14733)

* Enable zero-copy from numpy to MXNet NDArray

* should work

* make lint happy

* fix stupid typos

* wip to address comments

* Address comments

* Address comments

* Remove redundant code

                </div><div><b>label:</b> documentation
                </div></div></li></ol></div><div><b>github_issues:</b> <ol><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>title:</b> Feature request: share memory between numpy.array and mxnet.ndarray
                </div><div><b>body:</b> Last time I did a benchmark and @szha pointed out [that](https://gist.github.com/SunDoge/59a8ff336703b45be30b46dc3ee8b4ab#gistcomment-2841120)

&gt; MXNet made the choice of not doing a zero-copy for numpy arrays, but instead making a copy of the numpy data. 

This is a safe choice but sometimes it can be a performance problem. I check the C API and find no function to share data from outside. Is it possible for MXNet to provide such api?

                </div></div></li></ol></div><div><b>github_issues_comments:</b> <ol><li><div>
                I will take a look this weekend.
              </div></li><li><div>
                Hey, I did some research this weekend and am convinced it might be possible. So is it acceptable if we assume we are using cython (instead of ctypes) @szha
              </div></li><li><div>
                @junrushao1994 could you elaborate on the consideration?
              </div></li><li><div>
                The point is that we need transfer/share ownership of numpy's ndarray to mxnet's C++ backend, because we cannot guarantee the frontend object to exist forever.

Therefore, we need a customized deleter of MXNet's NDArray, aka calling something `Py_DECREF` of the numpy's ndarray object from the C++ backend - It is possible to implement the deleter via ctypes or cython, and then pass it to something roughly like follows this chunk of code.

https://github.com/apache/incubator-mxnet/blob/0f88f61379bd5f59fff6b825be1507d020bf2b7e/include/mxnet/ndarray.h#L131-L148

Per private discussion with @reminisce, his concern is how this could be compatible with the executor and memory planning, in which the executor may take over the ownership. I am not sure about this.

              </div></li><li><div>
                For the sharing from numpy to NDArray, we should use ctypes or weakref module add the inference of numpy object, and decrease the inference through NDArray::deleter.

For the sharing from NDArray to numpy, I think we can add a deleter attribute for numpy object.

https://docs.scipy.org/doc/numpy/user/basics.subclassing.html?highlight=deleter
              </div></li><li><div>
                Agreed with @wkcn
              </div></li><li><div>
                I prototyped a version that supports zero copy from numpy to DLManagedTensor in dlpack 0.2, but it turns out that MXNet hasn’t support dlpack 0.2 yet...
              </div></li><li><div>
                We can update the submodule dlpack.
              </div></li><li><div>
                @wkcn I opened a PR about this to https://github.com/dmlc/dlpack/pull/38. 
              </div></li><li><div>
                @wkcn Sorry I made a mistake. @reminisce reminded me that we already got dlpack 0.2 in MXNet, so everything should be fine
              </div></li><li><div>
                In the submodule DLPack, DLPACK_VERSION is 010 rather than 020.
              </div></li><li><div>
                @wkcn this is because dlpack forgots to change its version number to 020 when releasing v0.2 lol. I check the commit hash [here](https://github.com/apache/incubator-mxnet/tree/numpy/3rdparty), and it is identical to tag [v0.2](https://github.com/dmlc/dlpack/releases/tag/v0.2).
              </div></li><li><div>
                Being lazy for a while, and now I am thinking of adding an API to mxnet. @wkcn @szha @SunDoge What do you guys think of names for this API? mx.nd.zerocopy_from() or anything else?
              </div></li><li><div>
                I think it is suitable to add a new argument.

# NDArray to NumPy
a = mx.nd.array([1,2,3])
b = a.asnumpy(shared=True)

# NumPy to NDArray
c = np.array([4,5])
d = mx.nd.array(c, shared=True)
              </div></li><li><div>
                I think adding a parameter to the existing API may introduce certain level of ambiguity which is not desirable. For example, `mx.nd.array` takes not just numpy arrays as arguments, but `shared=True` only works for numpy arrays. We can consider adding this parameter to a new API, for example: `mx.nd.from_numpy(zero_copy=True)`, and it falls back to `mx.nd.array` when `zero_copy` is `False`.
              </div></li><li><div>
                @reminisce Sounds good. Will do!
              </div></li><li><div>
                Thx.
              </div></li></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> [MXNET-1398] Enable zero-copy from numpy to MXNet NDArray
                </div><div><b>body:</b> ## Description ##
This PR allows users to convert numpy's NDArray without copying content data. This is achieved by converting numpy's NDArray to `DLManagedTensor`, which is introduced in https://github.com/dmlc/dlpack/pull/38. We expect that this would help users reduce major performance bottleneck when this conversion is conducted frequently.

See also: #14244.

## Checklist ##
### Essentials ###
Please feel free to remove inapplicable items for your PR.
- [x] The PR title starts with [MXNET-$JIRA_ID], where $JIRA_ID refers to the relevant [JIRA issue](https://issues.apache.org/jira/projects/MXNET/issues) created (except PRs with tiny changes)
- [x] Changes are complete (i.e. I finished coding on this PR)
- [x] All changes have test coverage:
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- [x] Code is well-documented: 
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
- [x] To the my best knowledge, examples are either not affected by this change, or have been fixed to be compatible with this change

### Changes ###
- [x] Rewrite `NDArray::FromDLPack` to make it resource safe.
- [x] Add DLManagedTensor and its deleter via ctypes that allows C++ side deletes it.

## Comments ##
Nothing.

                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div>
                @reminisce @wkcn Could you help review? Thanks!
              </div></li><li><div>
                @reminisce Hey I have already addressed your comments. Could you review again? Thanks!
              </div></li><li><div>
                Merged. Thank you!
              </div></li></ol></div><div><b>github_pulls_reviews:</b> <ol><li><div>
                In c_api.h, there are the three apis: 
```c++

int MXNDArrayToDLPack(NDArrayHandle handle,
                      DLManagedTensorHandle *out_dlpack);

int MXNDArrayFromDLPack(DLManagedTensorHandle dlpack,
                        NDArrayHandle *out_handle);

int MXNDArrayCallDLPackDeleter(DLManagedTensorHandle dlpack);
```

MXNDArrayFromDLPack is the same as MXNDArrayFromDLManagedTensor.
              </div></li><li><div>
                It seems that FromDLManagerTensor is the same as FromDLPack, could we only use FromDLPack function?
              </div></li><li><div>
                We can test whether np_array and mx_array share the memory. For example, modify an element of np_array or mx_array, then check the other.
              </div></li><li><div>
                Could you please add an assertion to check whether str(array.dtype) in TYPE_MAP?
              </div></li><li><div><div><b>body:</b> If this function is not user facing, better to make it private, i.e. `_make_managger_ctx`.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Should we pass `dtype=ndarray.dtype`?
              </div></li><li><div>
                Better to raise instead of assert.
              </div></li><li><div>
                Same as above.
              </div></li><li><div>
                Same as above.
              </div></li><li><div>
                Remove `print`
              </div></li><li><div><div><b>body:</b> Can you add more test cases including
1. scalar tensors.
2. zero-size tensors.
3. `zero_copy=False`.
4. Perturbing the elements of numpy ndarray will equivalently affect mxnet NDArray, and vice versa.
5. Deleting either one of them should not obliterate the content of the other one when `zero_copy=True` and `False`.
6. assertRaise for numpy ndarrays that are not c-contiguous.
                </div><div><b>label:</b> test
                </div></div></li><li><div>
                Do you need to increase the ref count of array?
              </div></li><li><div><div><b>body:</b> There are asserts everywhere in MXNet's python package...Anyway, I will change it
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> No, this is rather a temporary stack variable
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Oh, I meant where incref for numpy object happened. NVM, just saw line 4189, missed reading that before.
              </div></li><li><div>
                Good catch!
              </div></li><li><div>
                The most important reason that I didn't call the original API is its overly-strong underlying assumption made me scared. Correct me if I was wrong, but I feel that it asks the DLManagedTensor to be alive throughout the entire lifetime of the internal NDArray, i.e. the frontend should never delete the numpy ndarray before we delete the mx's one, otherwise the entire process would crash on the destructor. To be more specific, in the implementation of `NDArray::FromDLPack`, the deleter would assume the argument `tensor` always points to a valid address - this doesn't usually hold, especially for that in the frontend user may delete the numpy's ndarray. I was not aware why it is initially designed like this.
              </div></li><li><div>
                No. The `FromDLManagerTensor ` does not require the address `tensor` points to to be alive.
              </div></li><li><div>
                I wrote the function FromDLPack, and it is my mistake. Could you please replace FromDLPack with FromDLManagedTensor, but keep the function name FromDLPack? Thank you!
              </div></li><li><div>
                @wkcn No worries! I will try. Thanks!
              </div></li><li><div>
                I tried several hours unifying these two implementations, but not able to resolve some core dumps in the unittest `test_ndarray.test_dlpack` because `manager_ctx` points to an invalid address (something like 0x6). Maybe we can do it later.
              </div></li><li><div>
                Done
              </div></li><li><div>
                Done. Thanks :-)
              </div></li><li><div>
                Done per discussion with @wkcn. Now we unify `FromDLPack ` and `FromDLManagedTensor `. Thank you so much!
              </div></li><li><div>
                when doing zero-copy, should the ownership of the data pointer be transferred to the new ndarray? do we need to update the writable flag? what happens when the original data is updated by numpy?
              </div></li><li><div>
                @szha They share the ownership - which means numpy's modification is transparent to mxnet and vice versa. As requested by @reminisce in his review, I added a testcase for this transparency.
              </div></li><li><div>
                how does it work when using the asynchronous engine? since numpy calls happen immediately, allowing shared ownership may give unexpected results. consider the following case:
```
a = np.array(...)
b = nd.from_numpy(a, zero_copy=True)
c = nd.expensive_op(b, in_place=True)
d = np.inplace_op(a) # this is called when c hasn't finished yet
```
given the asynchronous execution, it's hard to tell whether people should expect d to be based on the input before or after `expensive_op`
              </div></li><li><div><div><b>body:</b> (We actually have such discussion long time before, but it's good to make things open and achievable to the community)

My understanding is that your concern is a general question in multi-threading (see the code below). In any multi-threaded system, we may expect this issue to exist -- and MXNet has multi-threaded engine, so this issue exists, right?

We have discussion that how about the content of array is changed somewhere inside the system but user don't know that, should we set up the `WRITEABLE` flag to be true to prevent users from writing, or should we completely disallow users to access this numpy array?

The question, in the high-level, is all about trade-off. Restricting the freedom of users may make things less error-prone, while giving users full freedom may make the system more powerful.

On one hand, in my humble opinion, restricting the freedom of users, like setting up `WRITEABLE` flag, does not completely resolve this issue, because users can still encounter pitfalls when they read the array using the numpy side API. The only helps in this case is to completely disable users from read or write the array, which grant them on access only the numpy side - This is more like guaranteeing safety by disallowing users to create threads/processes.

On the other hand, giving users freedom enables them to do more things, and does not necessarily mean that they will definitely make a mistake, because we will encourage to use mxnet's numpy compatibility, which is on the way.

Anyway, I am open to any discussion and open to possible changes setting up several flags on the numpy array.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                The API should have clear semantics regardless of users’ knowledge on how MXNet execution works. If you worry about flexibility I’d recommend making it an option to not disable writable flag.
              </div></li><li><div>
                No. The point is that writable flag doesn’t address the issue at all.
              </div></li><li><div>
                I think completely transferring the ownership from numpy and disabling access to the original array is preferred. If we are to expose numpy array for joint use, it should be done in asnumpy as another zero-copy creation feature, so that the wait_to_read can be done by us
              </div></li><li><div><div><b>body:</b> Maybe we can add a new bool argument for `from_numpy`. The argument is `True` by default, and it disables access to the original NumPy array. When users know the asychronous risk, they can set this arugument to `False`, in order to access to the original NumPy array.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Once we add a zero-copy option to asnumpy where the ownership is completely transferred back to numpy, I think there wouldn't be any need for co-ownership.
              </div></li><li><div>
                Consider the following case:
```python
# the variable `a` is a `mx.nd.NDArray` object
a = mx.nd.array([1,2,3])
b = a.asnumpy(zero_copy=True)
#  users call `asnumpy(zero_copy=True)` twice
c = a.asnumpy(zero_copy=True)
c[:] = 3
print(a)
print(b)

a[:] = 5
a.wait_to_read()
print(b)
print(c)
```
It's necessary to keep co-ownership.
We can add extra option in `from_numpy` and `asnumpy` to prompt users to call the function `wait_to_read()`.
Alternatively, can we add a hook into `numpy.ndarray`? When users access `numpy.ndarray`, `wait_to_read()` will be called automatically.
Reference: https://docs.scipy.org/doc/numpy/reference/arrays.classes.html
              </div></li><li><div>
                If zero-copy is on, then `a` should not be usable after the `asnumpy` call. The point is to let the  framework deal with all synchronization, and co-ownership doesn't allow that. And for the same case, you can simply copy `c` from `b` (the new numpy array) instead of `a` (the old ndarray), so it's not necessary to keep co-ownership.
              </div></li><li><div><div><b>body:</b> The hook in the context seems to refer to our own subclass of numpy array. Creating a new subclass seems like overkill.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                For reference, @szha's suggestion was addressed in https://github.com/apache/incubator-mxnet/pull/14948
              </div></li></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> Enable zero-copy from numpy to MXNet NDArray
                </div><div><b>description:</b> ## Description ##
This PR allows users to convert numpy's NDArray without copying content data. This is achieved by converting numpy's NDArray to `DLManagedTensor`, which is introduced in https://github.com/dmlc/dlpack/pull/38. We expect that this would help users reduce major performance bottleneck when this conversion is conducted frequently.

## Checklist ##
### Essentials ###
Please feel free to remove inapplicable items for your PR.
- [x] The PR title starts with [MXNET-$JIRA_ID], where $JIRA_ID refers to the relevant [JIRA issue](https://issues.apache.org/jira/projects/MXNET/issues) created (except PRs with tiny changes)
- [x] Changes are complete (i.e. I finished coding on this PR)
- [x] All changes have test coverage:
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- [x] Code is well-documented: 
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
- [x] To the my best knowledge, examples are either not affected by this change, or have been fixed to be compatible with this change

### Changes ###
- [x] Rewrite `NDArray::FromDLPack` to make it resource safe.
- [x] Add DLManagedTensor and its deleter via ctypes that allows C++ side deletes it.

## Comments ##
Nothing.
                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol></ol></div></div></html>