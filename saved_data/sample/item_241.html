<!DOCTYPE html><html><div class="item-title">
        Item 241
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                 Check if the user has supplied a file or directory pattern
              </div></li><li><div>
                
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                 If this processor has an incoming connection, then do not run unless a
 FlowFile is actually sent through
              </div></li><li><div>
                
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> NIFI-2547: Add DeleteHDFS Processor
                </div><div><b>message:</b> NIFI-2547: Add DeleteHDFS Processor

This processor adds the capability to delete files or
directories inside of HDFS.

Paths supports both static and expression language values,
as well as glob support (e.g. /data/for/2016/07/*).

This processor may be used standalone, as well as part of a
downstream connection.

Signed-off-by: Matt Burgess &lt;mattyb149@apache.org&gt;

Add Glob Matcher with Tests

Also set displayName on properties.

Signed-off-by: Matt Burgess &lt;mattyb149@apache.org&gt;

This closes #850

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> NIFI-2547: Add DeleteHDFS Processor
                </div><div><b>body:</b> This processor adds the capability to delete files or
directories inside of HDFS.

Paths supports both static and expression language values,
as well as glob support (e.g. /data/for/2016/07/*).

This processor may be used standalone, as well as part of a
downstream connection.

                </div></div></li><li><div><div><b>title:</b> NIFI-2547: Add DeleteHDFS Processor
                </div><div><b>body:</b> This processor adds the capability to delete files or
directories inside of HDFS.

Paths supports both static and expression language values,
as well as glob support (e.g. /data/for/2016/07/*).

This processor may be used standalone, as well as part of a
downstream connection.

                </div></div></li><li><div><div><b>title:</b> NIFI-2547: Add DeleteHDFS Processor
                </div><div><b>body:</b> This processor adds the capability to delete files or
directories inside of HDFS.

Paths supports both static and expression language values,
as well as glob support (e.g. /data/for/2016/07/*).

This processor may be used standalone, as well as part of a
downstream connection.

                </div></div></li><li><div><div><b>title:</b> NIFI-2547: Add DeleteHDFS Processor
                </div><div><b>body:</b> This processor adds the capability to delete files or
directories inside of HDFS.

Paths supports both static and expression language values,
as well as glob support (e.g. /data/for/2016/07/*).

This processor may be used standalone, as well as part of a
downstream connection.

                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>title:</b> NIFI-2547: Add DeleteHDFS Processor
                </div><div><b>body:</b> This processor adds the capability to delete files or
directories inside of HDFS.

Paths supports both static and expression language values,
as well as glob support (e.g. /data/for/2016/07/*).

This processor may be used standalone, as well as part of a
downstream connection.

                </div></div></li><li><div><div><b>title:</b> NIFI-2547: Add DeleteHDFS Processor
                </div><div><b>body:</b> This processor adds the capability to delete files or
directories inside of HDFS.

Paths supports both static and expression language values,
as well as glob support (e.g. /data/for/2016/07/*).

This processor may be used standalone, as well as part of a
downstream connection.

                </div></div></li><li><div><div><b>title:</b> NIFI-2547: Add DeleteHDFS Processor
                </div><div><b>body:</b> This processor adds the capability to delete files or
directories inside of HDFS.

Paths supports both static and expression language values,
as well as glob support (e.g. /data/for/2016/07/*).

This processor may be used standalone, as well as part of a
downstream connection.

                </div></div></li><li><div><div><b>title:</b> NIFI-2547: Add DeleteHDFS Processor
                </div><div><b>body:</b> This processor adds the capability to delete files or
directories inside of HDFS.

Paths supports both static and expression language values,
as well as glob support (e.g. /data/for/2016/07/*).

This processor may be used standalone, as well as part of a
downstream connection.

                </div></div></li><li><div><div><b>title:</b> NIFI-2547: Add DeleteHDFS Processor
                </div><div><b>body:</b> This processor adds the capability to delete files or
directories inside of HDFS.

Paths supports both static and expression language values,
as well as glob support (e.g. /data/for/2016/07/*).

This processor may be used standalone, as well as part of a
downstream connection.

                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>title:</b> NIFI-2547: Add DeleteHDFS Processor
                </div><div><b>body:</b> This processor adds the capability to delete files or
directories inside of HDFS.

Paths supports both static and expression language values,
as well as glob support (e.g. /data/for/2016/07/*).

This processor may be used standalone, as well as part of a
downstream connection.

                </div></div></li><li><div><div><b>title:</b> NIFI-2547: Add DeleteHDFS Processor
                </div><div><b>body:</b> This processor adds the capability to delete files or
directories inside of HDFS.

Paths supports both static and expression language values,
as well as glob support (e.g. /data/for/2016/07/*).

This processor may be used standalone, as well as part of a
downstream connection.

                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div>
                Just a couple of minor comments; otherwise looks great, thanks!

              </div></li><li><div>
                +1 LGTM, built and ran the tests, and tried on a live system with and without incoming flow files, using different glob patterns, files and directories, etc.  Great job, thanks! Merging to master

              </div></li></ol></div><div><b>github_pulls_reviews:</b> <ol><li><div><div><b>body:</b> Not a requirement, but lately it's been suggested to use displayName() in the PropertyDescriptor.Builder() for the "friendly" name, and name() as something more machine friendly, as we move towards internationalization.

                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Are there other types of glob patterns that could/should be recognized?

              </div></li><li><div>
                Is it better to mock here or use the target/ area (since HDFS defaults to local FS) for file existence, deletion, etc.?

              </div></li><li><div>
                I did some research and it appears that there are some additional characters and patterns...I'll see if I can make a regex for recognizing some of the characters which would infer that a glob was provided. An another option is to have a true/false property which would explicitly specify that a glob is being provided. 

              </div></li><li><div>
                I could rewrite it to use the local fs, but I was just going off how the other tests behaved. 

              </div></li><li><div><div><b>body:</b> This guy appears unused so causes a checkstyle violation when running with the contrib-check profile in Maven

                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Star imports cause a checkstyle violation when running with the contrib-check profile in Maven. I can make the checkstyle changes while merging this PR, just bringing it up here for the future :)

              </div></li></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> Add DeleteHDFS Processor 
                </div><div><b>description:</b> There are times where a user may want to remove a file or directory from HDFS. The reasons for this vary, but to provide some context, I currently have a pipeline where I need to periodically delete files that my NiFi pipeline is producing. In my case, it's a "Delete files after they are 7 days old". 

Currently, I have to use the {{ExecuteStreamCommand}} processor and manually call {{hdfs dfs -rm}}, which is awful when dealing with a large amount of files. For one, an entire JVM is spun up for each delete, and two, when deleting directories with thousands of files, it can sometimes cause the command to hang indefinitely. 

With that being said, I am proposing we add a {{DeleteHDFS}} processor which meets the following criteria. 

* Can delete both directories and files
* Can delete directories recursively
* Supports the dynamic expression language 
* Supports using glob paths (e.g. /data/for/2017/08/*)
* Capable of being a downstream processor as well as a standalone processor


                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol><li><div><div><b>body:</b> I created a pretty polished prototype of this processor and deployed it onto my own production cluster. I swapped the original pipeline which was effectively stuck on over 2,000 directories (containing thousands of files per directory) for the new {{DeleteHDFS}} processor based pipeline. The new pipeline was able to delete all of the remaining directories in about 5 minutes. 

I will clean up the code and write some tests before submitting a PR. 
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                GitHub user rickysaltzer opened a pull request:

    https://github.com/apache/nifi/pull/850

    NIFI-2547: Add DeleteHDFS Processor

    This processor adds the capability to delete files or
    directories inside of HDFS.
    
    Paths supports both static and expression language values,
    as well as glob support (e.g. /data/for/2016/07/*).
    
    This processor may be used standalone, as well as part of a
    downstream connection.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rickysaltzer/nifi NIFI-2547

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/nifi/pull/850.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #850
    
----
commit d42fe48779eefbdfe936f2b3745b7eed1fe31d6e
Author: ricky &lt;ricky@cloudera.com&gt;
Date:   2016-08-10T23:14:39Z

    NIFI-2547: Add DeleteHDFS Processor
    
    This processor adds the capability to delete files or
    directories inside of HDFS.
    
    Paths supports both static and expression language values,
    as well as glob support (e.g. /data/for/2016/07/*).
    
    This processor may be used standalone, as well as part of a
    downstream connection.

----

              </div></li><li><div>
                https://github.com/apache/nifi/pull/850
              </div></li><li><div>
                Github user mattyb149 commented on a diff in the pull request:

    https://github.com/apache/nifi/pull/850#discussion_r75044175
  
    --- Diff: nifi-nar-bundles/nifi-hadoop-bundle/nifi-hdfs-processors/src/main/java/org/apache/nifi/processors/hadoop/DeleteHDFS.java ---
    @@ -0,0 +1,161 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.nifi.processors.hadoop;
    +
    +import java.io.IOException;
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.HashSet;
    +import java.util.List;
    +import java.util.Map;
    +import java.util.Set;
    +
    +import org.apache.hadoop.fs.FileStatus;
    +import org.apache.hadoop.fs.FileSystem;
    +import org.apache.hadoop.fs.Path;
    +import org.apache.nifi.annotation.behavior.InputRequirement;
    +import org.apache.nifi.annotation.behavior.TriggerWhenEmpty;
    +import org.apache.nifi.annotation.documentation.CapabilityDescription;
    +import org.apache.nifi.annotation.documentation.Tags;
    +import org.apache.nifi.components.PropertyDescriptor;
    +import org.apache.nifi.flowfile.FlowFile;
    +import org.apache.nifi.processor.ProcessContext;
    +import org.apache.nifi.processor.ProcessSession;
    +import org.apache.nifi.processor.Relationship;
    +import org.apache.nifi.processor.exception.ProcessException;
    +import org.apache.nifi.processor.util.StandardValidators;
    +
    +import com.google.common.collect.Lists;
    +import com.google.common.collect.Maps;
    +
    +@TriggerWhenEmpty
    +@InputRequirement(InputRequirement.Requirement.INPUT_ALLOWED)
    +@Tags({ "hadoop", "HDFS", "delete", "remove", "filesystem" })
    +@CapabilityDescription("Deletes a file from HDFS. The file can be provided as an attribute from an incoming FlowFile, "
    +        + "or a statically set file that is periodically removed. If this processor has an incoming connection, it"
    +        + "will ignore running on a periodic basis and instead rely on incoming FlowFiles to trigger a delete. "
    +        + "Optionally, you may specify use a wildcard character to match multiple files or directories.")
    +public class DeleteHDFS extends AbstractHadoopProcessor {
    +    public static final Relationship REL_SUCCESS = new Relationship.Builder()
    +            .name("success")
    +            .description("FlowFiles will be routed here if the delete command was successful")
    +            .build();
    +
    +    public static final Relationship REL_FAILURE = new Relationship.Builder()
    +            .name("failure")
    +            .description("FlowFiles will be routed here if the delete command was unsuccessful")
    +            .build();
    +
    +    public static final PropertyDescriptor FILE_OR_DIRECTORY = new PropertyDescriptor.Builder()
    +            .name("File or Directory")
    --- End diff --
    
    Not a requirement, but lately it's been suggested to use displayName() in the PropertyDescriptor.Builder() for the "friendly" name, and name() as something more machine friendly, as we move towards internationalization.

              </div></li><li><div>
                Github user mattyb149 commented on a diff in the pull request:

    https://github.com/apache/nifi/pull/850#discussion_r75044228
  
    --- Diff: nifi-nar-bundles/nifi-hadoop-bundle/nifi-hdfs-processors/src/main/java/org/apache/nifi/processors/hadoop/DeleteHDFS.java ---
    @@ -0,0 +1,161 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.nifi.processors.hadoop;
    +
    +import java.io.IOException;
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.HashSet;
    +import java.util.List;
    +import java.util.Map;
    +import java.util.Set;
    +
    +import org.apache.hadoop.fs.FileStatus;
    +import org.apache.hadoop.fs.FileSystem;
    +import org.apache.hadoop.fs.Path;
    +import org.apache.nifi.annotation.behavior.InputRequirement;
    +import org.apache.nifi.annotation.behavior.TriggerWhenEmpty;
    +import org.apache.nifi.annotation.documentation.CapabilityDescription;
    +import org.apache.nifi.annotation.documentation.Tags;
    +import org.apache.nifi.components.PropertyDescriptor;
    +import org.apache.nifi.flowfile.FlowFile;
    +import org.apache.nifi.processor.ProcessContext;
    +import org.apache.nifi.processor.ProcessSession;
    +import org.apache.nifi.processor.Relationship;
    +import org.apache.nifi.processor.exception.ProcessException;
    +import org.apache.nifi.processor.util.StandardValidators;
    +
    +import com.google.common.collect.Lists;
    +import com.google.common.collect.Maps;
    +
    +@TriggerWhenEmpty
    +@InputRequirement(InputRequirement.Requirement.INPUT_ALLOWED)
    +@Tags({ "hadoop", "HDFS", "delete", "remove", "filesystem" })
    +@CapabilityDescription("Deletes a file from HDFS. The file can be provided as an attribute from an incoming FlowFile, "
    +        + "or a statically set file that is periodically removed. If this processor has an incoming connection, it"
    +        + "will ignore running on a periodic basis and instead rely on incoming FlowFiles to trigger a delete. "
    +        + "Optionally, you may specify use a wildcard character to match multiple files or directories.")
    +public class DeleteHDFS extends AbstractHadoopProcessor {
    +    public static final Relationship REL_SUCCESS = new Relationship.Builder()
    +            .name("success")
    +            .description("FlowFiles will be routed here if the delete command was successful")
    +            .build();
    +
    +    public static final Relationship REL_FAILURE = new Relationship.Builder()
    +            .name("failure")
    +            .description("FlowFiles will be routed here if the delete command was unsuccessful")
    +            .build();
    +
    +    public static final PropertyDescriptor FILE_OR_DIRECTORY = new PropertyDescriptor.Builder()
    +            .name("File or Directory")
    +            .description("The HDFS file or directory to delete. A wildcard expression may be used to only delete certain files")
    +            .required(true)
    +            .addValidator(StandardValidators.NON_EMPTY_VALIDATOR)
    +            .expressionLanguageSupported(true)
    +            .build();
    +
    +    public static final PropertyDescriptor RECURSIVE = new PropertyDescriptor.Builder()
    +            .name("Recursive")
    +            .description("Remove contents of a non-empty directory recursively")
    +            .allowableValues("true", "false")
    +            .required(true)
    +            .defaultValue("true")
    +            .addValidator(StandardValidators.NON_EMPTY_VALIDATOR)
    +            .build();
    +
    +    private static final Set&lt;Relationship&gt; relationships;
    +
    +    static {
    +        final Set&lt;Relationship&gt; relationshipSet = new HashSet&lt;&gt;();
    +        relationshipSet.add(REL_SUCCESS);
    +        relationshipSet.add(REL_FAILURE);
    +        relationships = Collections.unmodifiableSet(relationshipSet);
    +    }
    +
    +    @Override
    +    protected List&lt;PropertyDescriptor&gt; getSupportedPropertyDescriptors() {
    +        List&lt;PropertyDescriptor&gt; props = new ArrayList&lt;&gt;(properties);
    +        props.add(FILE_OR_DIRECTORY);
    +        props.add(RECURSIVE);
    +        return props;
    +    }
    +
    +    @Override
    +    public Set&lt;Relationship&gt; getRelationships() {
    +        return relationships;
    +    }
    +
    +    @Override
    +    public void onTrigger(ProcessContext context, ProcessSession session) throws ProcessException {
    +        String fileOrDirectoryName = null;
    +        FlowFile flowFile = session.get();
    +
    +        // If this processor has an incoming connection, then do not run unless a
    +        // FlowFile is actually sent through
    +        if (flowFile == null &amp;&amp; context.hasIncomingConnection()) {
    +            context.yield();
    +            return;
    +        }
    +
    +        if (flowFile != null) {
    +            fileOrDirectoryName = context.getProperty(FILE_OR_DIRECTORY).evaluateAttributeExpressions(flowFile).getValue();
    +        } else {
    +            fileOrDirectoryName = context.getProperty(FILE_OR_DIRECTORY).evaluateAttributeExpressions().getValue();
    +        }
    +
    +        final FileSystem fileSystem = getFileSystem();
    +        try {
    +            // Check if the user has supplied a file or directory pattern
    +            List&lt;Path&gt; pathList = Lists.newArrayList();
    +            if (fileOrDirectoryName.contains("*")) {
    --- End diff --
    
    Are there other types of glob patterns that could/should be recognized?

              </div></li><li><div>
                Github user mattyb149 commented on a diff in the pull request:

    https://github.com/apache/nifi/pull/850#discussion_r75044365
  
    --- Diff: nifi-nar-bundles/nifi-hadoop-bundle/nifi-hdfs-processors/src/test/java/org/apache/nifi/processors/hadoop/TestDeleteHDFS.java ---
    @@ -0,0 +1,187 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.nifi.processors.hadoop;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.mockito.Mockito.any;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +import java.io.IOException;
    +import java.util.List;
    +import java.util.Map;
    +
    +import org.apache.hadoop.fs.FileStatus;
    +import org.apache.hadoop.fs.FileSystem;
    +import org.apache.hadoop.fs.Path;
    +import org.apache.nifi.flowfile.FlowFile;
    +import org.apache.nifi.hadoop.KerberosProperties;
    +import org.apache.nifi.util.MockFlowFile;
    +import org.apache.nifi.util.NiFiProperties;
    +import org.apache.nifi.util.TestRunner;
    +import org.apache.nifi.util.TestRunners;
    +import org.junit.Before;
    +import org.junit.Test;
    +
    +import com.google.common.collect.Maps;
    +
    +public class TestDeleteHDFS {
    +    private NiFiProperties mockNiFiProperties;
    +    private FileSystem mockFileSystem;
    +    private KerberosProperties kerberosProperties;
    +
    +    @Before
    +    public void setup() throws Exception {
    +        mockNiFiProperties = mock(NiFiProperties.class);
    +        when(mockNiFiProperties.getKerberosConfigurationFile()).thenReturn(null);
    +        kerberosProperties = KerberosProperties.create(mockNiFiProperties);
    +        mockFileSystem = mock(FileSystem.class);
    --- End diff --
    
    Is it better to mock here or use the target/ area (since HDFS defaults to local FS) for file existence, deletion, etc.?

              </div></li><li><div>
                Github user mattyb149 commented on the issue:

    https://github.com/apache/nifi/pull/850
  
    Just a couple of minor comments; otherwise looks great, thanks!

              </div></li><li><div>
                Github user rickysaltzer commented on a diff in the pull request:

    https://github.com/apache/nifi/pull/850#discussion_r75537912
  
    --- Diff: nifi-nar-bundles/nifi-hadoop-bundle/nifi-hdfs-processors/src/main/java/org/apache/nifi/processors/hadoop/DeleteHDFS.java ---
    @@ -0,0 +1,161 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.nifi.processors.hadoop;
    +
    +import java.io.IOException;
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.HashSet;
    +import java.util.List;
    +import java.util.Map;
    +import java.util.Set;
    +
    +import org.apache.hadoop.fs.FileStatus;
    +import org.apache.hadoop.fs.FileSystem;
    +import org.apache.hadoop.fs.Path;
    +import org.apache.nifi.annotation.behavior.InputRequirement;
    +import org.apache.nifi.annotation.behavior.TriggerWhenEmpty;
    +import org.apache.nifi.annotation.documentation.CapabilityDescription;
    +import org.apache.nifi.annotation.documentation.Tags;
    +import org.apache.nifi.components.PropertyDescriptor;
    +import org.apache.nifi.flowfile.FlowFile;
    +import org.apache.nifi.processor.ProcessContext;
    +import org.apache.nifi.processor.ProcessSession;
    +import org.apache.nifi.processor.Relationship;
    +import org.apache.nifi.processor.exception.ProcessException;
    +import org.apache.nifi.processor.util.StandardValidators;
    +
    +import com.google.common.collect.Lists;
    +import com.google.common.collect.Maps;
    +
    +@TriggerWhenEmpty
    +@InputRequirement(InputRequirement.Requirement.INPUT_ALLOWED)
    +@Tags({ "hadoop", "HDFS", "delete", "remove", "filesystem" })
    +@CapabilityDescription("Deletes a file from HDFS. The file can be provided as an attribute from an incoming FlowFile, "
    +        + "or a statically set file that is periodically removed. If this processor has an incoming connection, it"
    +        + "will ignore running on a periodic basis and instead rely on incoming FlowFiles to trigger a delete. "
    +        + "Optionally, you may specify use a wildcard character to match multiple files or directories.")
    +public class DeleteHDFS extends AbstractHadoopProcessor {
    +    public static final Relationship REL_SUCCESS = new Relationship.Builder()
    +            .name("success")
    +            .description("FlowFiles will be routed here if the delete command was successful")
    +            .build();
    +
    +    public static final Relationship REL_FAILURE = new Relationship.Builder()
    +            .name("failure")
    +            .description("FlowFiles will be routed here if the delete command was unsuccessful")
    +            .build();
    +
    +    public static final PropertyDescriptor FILE_OR_DIRECTORY = new PropertyDescriptor.Builder()
    +            .name("File or Directory")
    +            .description("The HDFS file or directory to delete. A wildcard expression may be used to only delete certain files")
    +            .required(true)
    +            .addValidator(StandardValidators.NON_EMPTY_VALIDATOR)
    +            .expressionLanguageSupported(true)
    +            .build();
    +
    +    public static final PropertyDescriptor RECURSIVE = new PropertyDescriptor.Builder()
    +            .name("Recursive")
    +            .description("Remove contents of a non-empty directory recursively")
    +            .allowableValues("true", "false")
    +            .required(true)
    +            .defaultValue("true")
    +            .addValidator(StandardValidators.NON_EMPTY_VALIDATOR)
    +            .build();
    +
    +    private static final Set&lt;Relationship&gt; relationships;
    +
    +    static {
    +        final Set&lt;Relationship&gt; relationshipSet = new HashSet&lt;&gt;();
    +        relationshipSet.add(REL_SUCCESS);
    +        relationshipSet.add(REL_FAILURE);
    +        relationships = Collections.unmodifiableSet(relationshipSet);
    +    }
    +
    +    @Override
    +    protected List&lt;PropertyDescriptor&gt; getSupportedPropertyDescriptors() {
    +        List&lt;PropertyDescriptor&gt; props = new ArrayList&lt;&gt;(properties);
    +        props.add(FILE_OR_DIRECTORY);
    +        props.add(RECURSIVE);
    +        return props;
    +    }
    +
    +    @Override
    +    public Set&lt;Relationship&gt; getRelationships() {
    +        return relationships;
    +    }
    +
    +    @Override
    +    public void onTrigger(ProcessContext context, ProcessSession session) throws ProcessException {
    +        String fileOrDirectoryName = null;
    +        FlowFile flowFile = session.get();
    +
    +        // If this processor has an incoming connection, then do not run unless a
    +        // FlowFile is actually sent through
    +        if (flowFile == null &amp;&amp; context.hasIncomingConnection()) {
    +            context.yield();
    +            return;
    +        }
    +
    +        if (flowFile != null) {
    +            fileOrDirectoryName = context.getProperty(FILE_OR_DIRECTORY).evaluateAttributeExpressions(flowFile).getValue();
    +        } else {
    +            fileOrDirectoryName = context.getProperty(FILE_OR_DIRECTORY).evaluateAttributeExpressions().getValue();
    +        }
    +
    +        final FileSystem fileSystem = getFileSystem();
    +        try {
    +            // Check if the user has supplied a file or directory pattern
    +            List&lt;Path&gt; pathList = Lists.newArrayList();
    +            if (fileOrDirectoryName.contains("*")) {
    --- End diff --
    
    I did some research and it appears that there are some additional characters and patterns...I'll see if I can make a regex for recognizing some of the characters which would infer that a glob was provided. An another option is to have a true/false property which would explicitly specify that a glob is being provided. 


              </div></li><li><div>
                Github user rickysaltzer commented on a diff in the pull request:

    https://github.com/apache/nifi/pull/850#discussion_r75538007
  
    --- Diff: nifi-nar-bundles/nifi-hadoop-bundle/nifi-hdfs-processors/src/test/java/org/apache/nifi/processors/hadoop/TestDeleteHDFS.java ---
    @@ -0,0 +1,187 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.nifi.processors.hadoop;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.mockito.Mockito.any;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +import java.io.IOException;
    +import java.util.List;
    +import java.util.Map;
    +
    +import org.apache.hadoop.fs.FileStatus;
    +import org.apache.hadoop.fs.FileSystem;
    +import org.apache.hadoop.fs.Path;
    +import org.apache.nifi.flowfile.FlowFile;
    +import org.apache.nifi.hadoop.KerberosProperties;
    +import org.apache.nifi.util.MockFlowFile;
    +import org.apache.nifi.util.NiFiProperties;
    +import org.apache.nifi.util.TestRunner;
    +import org.apache.nifi.util.TestRunners;
    +import org.junit.Before;
    +import org.junit.Test;
    +
    +import com.google.common.collect.Maps;
    +
    +public class TestDeleteHDFS {
    +    private NiFiProperties mockNiFiProperties;
    +    private FileSystem mockFileSystem;
    +    private KerberosProperties kerberosProperties;
    +
    +    @Before
    +    public void setup() throws Exception {
    +        mockNiFiProperties = mock(NiFiProperties.class);
    +        when(mockNiFiProperties.getKerberosConfigurationFile()).thenReturn(null);
    +        kerberosProperties = KerberosProperties.create(mockNiFiProperties);
    +        mockFileSystem = mock(FileSystem.class);
    --- End diff --
    
    I could rewrite it to use the local fs, but I was just going off how the other tests behaved. 

              </div></li><li><div>
                Github user mattyb149 commented on a diff in the pull request:

    https://github.com/apache/nifi/pull/850#discussion_r75594156
  
    --- Diff: nifi-nar-bundles/nifi-hadoop-bundle/nifi-hdfs-processors/src/test/java/org/apache/nifi/processors/hadoop/TestDeleteHDFS.java ---
    @@ -0,0 +1,198 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.nifi.processors.hadoop;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.mockito.Mockito.any;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +import java.io.IOException;
    +import java.util.List;
    +import java.util.Map;
    +import java.util.regex.Matcher;
    --- End diff --
    
    This guy appears unused so causes a checkstyle violation when running with the contrib-check profile in Maven

              </div></li><li><div>
                Github user mattyb149 commented on a diff in the pull request:

    https://github.com/apache/nifi/pull/850#discussion_r75594158
  
    --- Diff: nifi-nar-bundles/nifi-hadoop-bundle/nifi-hdfs-processors/src/test/java/org/apache/nifi/processors/hadoop/TestDeleteHDFS.java ---
    @@ -0,0 +1,198 @@
    +/*
    + * Licensed to the Apache Software Foundation (ASF) under one or more
    + * contributor license agreements.  See the NOTICE file distributed with
    + * this work for additional information regarding copyright ownership.
    + * The ASF licenses this file to You under the Apache License, Version 2.0
    + * (the "License"); you may not use this file except in compliance with
    + * the License.  You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package org.apache.nifi.processors.hadoop;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.mockito.Mockito.any;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +import java.io.IOException;
    +import java.util.List;
    +import java.util.Map;
    +import java.util.regex.Matcher;
    +
    +import org.apache.hadoop.fs.FileStatus;
    +import org.apache.hadoop.fs.FileSystem;
    +import org.apache.hadoop.fs.Path;
    +import org.apache.nifi.flowfile.FlowFile;
    +import org.apache.nifi.hadoop.KerberosProperties;
    +import org.apache.nifi.util.MockFlowFile;
    +import org.apache.nifi.util.NiFiProperties;
    +import org.apache.nifi.util.TestRunner;
    +import org.apache.nifi.util.TestRunners;
    +import org.junit.Before;
    +import org.junit.Test;
    +import static org.junit.Assert.*;
    --- End diff --
    
    Star imports cause a checkstyle violation when running with the contrib-check profile in Maven. I can make the checkstyle changes while merging this PR, just bringing it up here for the future :)

              </div></li><li><div>
                Github user mattyb149 commented on the issue:

    https://github.com/apache/nifi/pull/850
  
    +1 LGTM, built and ran the tests, and tried on a live system with and without incoming flow files, using different glob patterns, files and directories, etc.  Great job, thanks! Merging to master

              </div></li><li><div>
                Commit 26d362b144e15ea4a224e346c340d74e978c134c in nifi's branch refs/heads/master from [~rickysaltzer]
[ https://git-wip-us.apache.org/repos/asf?p=nifi.git;h=26d362b ]

NIFI-2547: Add DeleteHDFS Processor

This processor adds the capability to delete files or
directories inside of HDFS.

Paths supports both static and expression language values,
as well as glob support (e.g. /data/for/2016/07/*).

This processor may be used standalone, as well as part of a
downstream connection.

Signed-off-by: Matt Burgess &lt;mattyb149@apache.org&gt;

Add Glob Matcher with Tests

Also set displayName on properties.

Signed-off-by: Matt Burgess &lt;mattyb149@apache.org&gt;

This closes #850

              </div></li><li><div>
                Commit b7dc21bd95f61734fefb7f1d8b6494b4461f84ae in nifi's branch refs/heads/master from [~mattyb149]
[ https://git-wip-us.apache.org/repos/asf?p=nifi.git;h=b7dc21b ]

NIFI-2547: Fixed checkstyle errors and updated to reflect KerberosProperties changes

              </div></li><li><div>
                Github user asfgit closed the pull request at:

    https://github.com/apache/nifi/pull/850

              </div></li></ol></div></div></html>