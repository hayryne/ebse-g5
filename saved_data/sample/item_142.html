<!DOCTYPE html><html><div class="item-title">
        Item 142
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                 fix length to relevant portion of har block
              </div></li><li><div>
                 each line contains a hashcode range and the index file name
              </div></li><li><div>
                 get all part blocks that overlap with the desired file blocks
              </div></li><li><div>
                 the fields below are stored in the file but are currently not used
 by HarFileSystem
 permission = new FsPermission(Short.parseShort(propSplits[1]));
 owner = decodeString(propSplits[2]);
 group = decodeString(propSplits[3]);
              </div></li><li><div>
                 close the master index
              </div></li><li><div>
                 offset 1 past last byte of desired range
              </div></li><li><div>
                 we got the right har Path- now check if this is 
 truly a har filesystem
              </div></li><li><div>
                 desired range includes beginning of this har block
              </div></li><li><div>
                 propSplits is used to retrieve the metainformation that Har versions
 1 &amp; 2 missed (modification time, permission, owner group).
 These fields are stored in an encoded string placed in different
 locations depending on whether it's a file or directory entry.
 If it's a directory, the string will be placed at the partName
 location (directories have no partName because they don't have data
 to be stored). This is done because the number of fields in a
 directory entry is unbounded (all children are listed at the end)
 If it's a file, the string will be the last field.
              </div></li><li><div>
                *
   * Fix offset and length of block locations.
   * Note that this method modifies the original array.
   * @param locations block locations of har part file
   * @param start the start of the desired range in the contained file
   * @param len the length of the desired range
   * @param fileOffsetInHar the offset of the desired file in the har part file
   * @return block locations with fixed offset and length
   
              </div></li><li><div>
                 pointer into the static metadata cache
              </div></li><li><div>
                 desired range starts after beginning of this har block
 fix offset to beginning of relevant range (relative to desired file)
              </div></li><li><div>
                 range ends before end of this har block
 fix length to remove irrelevant portion at the end
              </div></li><li><div>
                *
   * Get filestatuses of all the children of a given directory. This just reads
   * through index file and reads line by line to get all statuses for children
   * of a directory. Its a brute force way of getting all such filestatuses
   * 
   * @param parent
   *          the parent path directory
   * @param statuses
   *          the list to add the children filestatuses to
   * @param children
   *          the string list of children for this parent
   * @param archiveIndexStat
   *          the archive index filestatus
   
              </div></li><li><div>
                *
   * Combine the status stored in the index and the underlying status. 
   * @param h status stored in the index
   * @param cache caching the underlying file statuses
   * @return the combined file status
   * @throws IOException
   
              </div></li><li><div>
                 decode the name
              </div></li><li><div>
                 make it always backwards-compatible
              </div></li><li><div>
                 the version is currently not useful since its the first version
              </div></li><li><div>
                *
   * @return null since no checksum algorithm is implemented.
   
              </div></li><li><div>
                 offset 1 past last byte of har block relative to beginning of
 desired file
              </div></li><li><div>
                 close the archive index
              </div></li><li><div>
                 the archive has been overwritten since we last read it
 remove the entry from the meta data cache
              </div></li><li><div>
                 offset of part block relative to beginning of desired file
 (may be negative if file starts in this part block)
              </div></li><li><div>
                 the first line contains the version of the index file
              </div></li><li><div>
                 make it a har path
              </div></li><li><div>
                check for existence of 3 part files, since part file size == 1 
              </div></li><li><div>
                 check block size for path files 
              </div></li><li><div>
                check for existence of only 1 part file, since part file size == 2GB 
              </div></li><li><div>
                 check bytes in the har output files 
              </div></li><li><div>
                 test archives with a -p option
              </div></li><li><div>
                *
   * check if the block size of the part files is what we had specified
   
              </div></li><li><div>
                 fileb and filec
              </div></li><li><div>
                * now try with different block size and part file size *
              </div></li><li><div>
                assuming all the 6 bytes were read.
              </div></li><li><div>
                *the size of the part files that will be created when archiving *
              </div></li><li><div>
                read the rest of the paths
              </div></li><li><div>
                *
     * the filestatus of this object
     * @return the filestatus of this object
     
              </div></li><li><div>
                 assuming if the user does not specify path for sources
 the whole parent directory needs to be archived. 
              </div></li><li><div>
                check to see if relative parent has been provided or not
this is a required parameter. 
              </div></li><li><div>
                *
     * constructor for filestatusdir
     * @param fstatus the filestatus object that maps to filestatusdir
     * @param children the children list if fs is a directory
     
              </div></li><li><div>
                * HarEntry is used in the {@link HArchivesMapper} as the input value. 
              </div></li><li><div>
                *
     * get rid of / in the beginning of path
     * @param p the path
     * @return return path without /
     
              </div></li><li><div>
                * the size of the blocks that will be created when archiving *
              </div></li><li><div>
                add all the directories 
              </div></li><li><div>
                 find all the common parents of paths that are valid archive
     * paths. The below is done so that we do not add a common path
     * twice and also we need to only add valid child of a path that
     * are specified the user.
     
              </div></li><li><div>
                *
   * truncate the prefix root from the full path
   * @param fullPath the full path
   * @param root the prefix root to be truncated
   * @return the relative path
   
              </div></li><li><div>
                *
   * this method writes all the valid top level directories 
   * into the srcWriter for indexing. This method is a little
   * tricky. example- 
   * for an input with parent path /home/user/ and sources 
   * as /home/user/source/dir1, /home/user/source/dir2 - this 
   * will output &lt;source, dir, dir1, dir2&gt; (dir means that source is a dir
   * with dir1 and dir2 as children) and &lt;source/dir1, file, null&gt;
   * and &lt;source/dir2, file, null&gt;
   * @param srcWriter the sequence file writer to write the
   * directories to
   * @param paths the source paths provided by the user. They
   * are glob free and have full path (not relative paths)
   * @param parentPath the parent path that you wnat the archives
   * to be relative to. example - /home/user/dir1 can be archived with
   * parent as /home or /home/user.
   * @throws IOException
   
              </div></li><li><div>
                *
     * the children list of this object, null if  
     * @return the children list
     
              </div></li><li><div>
                 the largest depth of paths. the max number of times
     * we need to iterate
     
              </div></li><li><div>
                *
     * set children of this object
     * @param listStatus the list of children
     
              </div></li><li><div>
                * size of blocks in hadoop archives *
              </div></li><li><div>
                *
   * A static class that keeps
   * track of status of a path 
   * and there children if path is a dir
   
              </div></li><li><div>
                * size of each part file size *
              </div></li><li><div>
                 just take some effort to do it 
 rather than just using substring 
 so that we do not break sometime later
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> HADOOP-7539. merge hadoop archive goodness from trunk to .20 (John George via mahadev)
                </div><div><b>message:</b> HADOOP-7539. merge hadoop archive goodness from trunk to .20 (John George via mahadev)

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/branch-0.20-security@1163079 13f79535-47bb-0310-9956-ffa450edef68

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol></ol></div><div><b>github_pulls_comments:</b> <ol></ol></div><div><b>github_pulls_reviews:</b> <ol></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div></div></li><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div></div></li><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div></div></li><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div><div><b>label:</b> test
                </div></div></li><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div></div></li><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div></div></li><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div></div></li><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div></div></li><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div></div></li><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div></div></li><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div></div></li><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div></div></li><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div></div></li><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div></div></li><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div></div></li><li><div><div><b>summary:</b> merge hadoop archive goodness from trunk to .20
                </div><div><b>description:</b> hadoop archive in branch-0.20-security is outdated. When run recently, it produced  some bugs which were all fixed in trunk. This JIRA aims to bring in all these JIRAs to branch-0.20-security.

                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol><li><div>
                The following JIRAs were the most interesting ones, but it made sense to bring in most of the others as well, not only because a bunch of them are dependencies of the JIRAs that were needed, but also because it is easier to merge.

MAPREDUCE-1425 :archive throws OutOfMemoryError
MAPREDUCE-2317 :HadoopArchives throwing NullPointerException while creating hadoop archives
MAPREDUCE-1399 : The archive command shows a null error message
MAPREDUCE-1752 :Implement getFileBlockLocations in HarFilesystem


              </div></li><li><div>
                No one has proposed making any more releases out of branch-0.20. Can you generate a patch for the branch-0.20-security line?
              </div></li><li><div>
                Sorry Owen,  I meant to say branch-20-security (not branch-0.20). Fixed "Description". The patch is also meant for branch-.20-security.
              </div></li><li><div><div><b>body:</b> John,
 Since this is a big patch, can you please do some manual testing on a real cluster (could be a single node cluster)? Just run a archive job and then a map reduce job to use the archives as input and verify the results. That should suffice.
                </div><div><b>label:</b> test
                </div></div></li><li><div>
                Yes, I will run the manual testing and post the results here.

I ran "ant test" and it failed the same test that failed without the patch. The results of test-patch is as follows:


   [exec] BUILD SUCCESSFUL
     [exec] Total time: 6 minutes 23 seconds
     [exec] 
     [exec] 
     [exec] 
     [exec] 
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.
     [exec] 
     [exec] 
     [exec] 
     [exec] 
     [exec] ======================================================================
     [exec] ======================================================================
     [exec]     Finished build.
     [exec] ======================================================================
     [exec] ======================================================================



              </div></li><li><div>
                Manual tests run:

- created a har file as follows:
   - hadoop fs -put test /tmp
   - hadoop archive -archiveName test.har -p /tmp test /tmp

- ran the following manual tests:
   - wordcount on a couple of har files
   - streaming on the same har file with: hadoop jar hadoop-streaming.jar -Dmapred.reduce.tasks=1 -input har:///tmp/test.har/test/aa -output /tmp/aaa.2 -mapper cat -reducer "wc -l"


Both of the above jobs completed successfully and had outputs in the corresponding output directory.
              </div></li><li><div>
                The only issue I see is that hadoop archives that already existed on the cluster will become obsolete since the new archive code wont be able to read it? 
              </div></li><li><div>
                Maybe we want to add a utility to upconvert from 1 to 3 version?
              </div></li><li><div>
                Looks like I might be wrong. The patch seems to be able to read the old har archives as well. John, mind testing it out? 
              </div></li><li><div>
                -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12490263/HADOOP-7539-1.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/67//console

This message is automatically generated.
              </div></li><li><div>
                1. Create HAR file using version 1
{quote}
$ hadoop fs -cat /tmp/thisis1.har/_masterindex
1 
0 2127535165 0 1856 
{quote}

2. Install version 3 of HAR
{quote}
$ hadoop fs -cat /tmp/thisis3.har/_masterindex
3 
0 2127535165 0 2610 
{quote}

3. Run ls and wordcount on VERSION 1
{quote}
$ hadoop fs -ls har:///tmp/thisis1.har
$ hadoop jar hadoop-examples.jar wordcount har:///tmp/thisis1.har/x.sh /tmp/out.2
{quote}
              </div></li><li><div>
                looks good to me. Ill run some ant tests and check it in the 0.20 security branch.
              </div></li><li><div>
                I just committed this. Thanks a lot John!
              </div></li><li><div>
                Closed upon release of 0.20.205.0
              </div></li></ol></div></div></html>