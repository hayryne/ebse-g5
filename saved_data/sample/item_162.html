<!DOCTYPE html><html><div class="item-title">
        Item 162
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> MAPREDUCE-1981. Improve getSplits performance by using listLocatedStatus. Contributed by Hairong Kuang and Jason Lowe
                </div><div><b>message:</b> MAPREDUCE-1981. Improve getSplits performance by using listLocatedStatus. Contributed by Hairong Kuang and Jason Lowe


git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/branch-0.23@1507388 13f79535-47bb-0310-9956-ffa450edef68

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol></ol></div><div><b>github_pulls_comments:</b> <ol></ol></div><div><b>github_pulls_reviews:</b> <ol></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> Improve getSplits performance by using listLocatedStatus
                </div><div><b>description:</b> This jira will make FileInputFormat and CombinedFileInputForm to use the new API, thus reducing the number of RPCs to HDFS NameNode.
                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol><li><div><div><b>body:</b> This patch makes FileInputFormat &amp; CombineFileInputFormat to use the new listFiles() API introduced by HADOOP-6870.

Ideally FileInputFormat#listStatus should have the following syntax:
{code}
Iterator&lt;LocatedFileStatus&gt; listStatus(JobConf job) throws IOException;
{code}

But since this is a public interface, I keep it as it is now to keep it backward compatible.

This patch also changes the semantics of listStatus a little bit. When recursive is false, listStatus used to return every child (including subdirectories) of the input directories. But with the new API, it returns only the file children. So it is not able to support this case: throwing an exception when an input directory has a subdirectory but recursive is false. I removed this test case from TestFileInputFormat. If we really want to support this scenario, I could make FileContext#listFiles to throw an exception when recursive is false but there is sbudirectory.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                mapredListFiles1.patch uses the new listFiles API in FileContext. Comparing to the first patch, this one maintains not only the syntax but also the semantics of FileInputFormat#listStatus
              </div></li><li><div><div><b>body:</b> Now HDFS-202 is in, mapredListFiles2.patch is the last piece of code that completes the improvement of getSplits performance.

Could a warm heart give it a review? Thanks.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                I'm pleased to see this feature propagate to MR. The approach looks correct, just a few comments:

* It looks like this change:
{noformat}
-    return result.toArray(new FileStatus[result.size()]);
+    return result.toArray(new LocatedFileStatus[result.size()]);
{noformat}
Causes {{TestMapRed}} to fail. {{SequenceFileInputFormat}} (and, presumably, other supertypes of {{FileInputFormat}}) may rely on the type of the array returned from {{FileInputFormat}} to be {{FileStatus[]}}
* I think the HDFS fault injection is breaking the publishing of that artifact, so the mapred tests currently do not recognize the change to the HDFS ClientProtocol and {{TestSubmitJob}} fails to compile. However, the patch is current with HDFS trunk and disabling the fault injection before running mvn-install, etc. works. Is this fault being tracked in HDFS?
* The patch causes {{TestNoDefaultsJobConf}} to fail:
{noformat}
Testcase: testNoDefaults took 4.489 sec
  Caused an ERROR
No AbstractFileSystem for scheme: hdfs
org.apache.hadoop.fs.UnsupportedFileSystemException: No AbstractFileSystem for scheme: hdfs
  at org.apache.hadoop.fs.AbstractFileSystem.createFileSystem(AbstractFileSystem.java:143)
  at org.apache.hadoop.fs.AbstractFileSystem.get(AbstractFileSystem.java:198)   
  at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:394)      
  at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:409)      
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:188)         
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:234)            
  at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:461)        
  at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:453)
  at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:354)     
  at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1037)                       
  at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1034)                       
  at java.security.AccessController.doPrivileged(Native Method)                 
  at javax.security.auth.Subject.doAs(Subject.java:396)                         
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1030)    
  at org.apache.hadoop.mapreduce.Job.submit(Job.java:1034)
  at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:536)           
  at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:781)              
  at org.apache.hadoop.conf.TestNoDefaultsJobConf.testNoDefaults(TestNoDefaultsJobConf.java:83)       
{noformat}
* Unfortunately, {{FileInputFormat::addInputPathRecursively}} could be overridden by a user. This should either be marked as an incompatible change or the function should be deprecated, but its functionality preserved. It may also be worth confirming that no test relies on it.
              </div></li><li><div>
                Chris, thanks a lot for your review.

This patch fixed comments 1 &amp; 3.

For comment 4, I decided to keep addInputPathRecursively. So listStatus continues to use addInputPathRecursively instead of FileContext#listFiles. This is for maintaining the old semantics to allow recursive listing to have an option to be overridden. 
              </div></li><li><div>
                This patch fixed two failed unit tests: TestCombineFileInputFormat and TestHarFileSystem.

For the first test, I found out that my patch made a subtle change to CombinFileInputFormat. The path filter in CombineFileInputFormat assumes that the path to the filter does not include the schema and hostname etc. This is different from other input formats where they do not have any assumption on Path format. My patch removes this restriction so I have to modify DummyFileInputFormat in TestCombineFileInputFormat not to have this assumption.

For the second test, it turns out HarFileSystem does not have a correct implementation of listLocatedStatus. So this patch adds one.
              </div></li><li><div>
                Chirs, thanks for submitting this patch. But HDFS-1157 is still not resolved yet. My patch won't be able to compile. Let me cancel it and wait until HDFS-1157 is resolved.
              </div></li><li><div>
                Since MAPREDUCE-2022 is committed, this patch is up to the trunk.
              </div></li><li><div>
                Since Hudson does not work, I ran tests locally.

Ant test-patch passed.

Ant test showed failed tests:
    [junit] Test org.apache.hadoop.mapred.TestMiniMRChildTask FAILED
    [junit] Test org.apache.hadoop.mapred.TestMultiFileInputFormat FAILED (timeout)
    [junit] Test org.apache.hadoop.mapred.TestTaskLauncher FAILED
    [junit] Test org.apache.hadoop.mapred.TestTaskTrackerLocalization FAILED
    [junit] Test org.apache.hadoop.mapreduce.filecache.TestTrackerDistributedCacheManager FAILED
    [junit] Test org.apache.hadoop.mapreduce.lib.output.TestJobOutputCommitter FAILED

I saw the last four failed on trunk. Anybody has an idea if TestMiniMRChildTask and TestMultiFileInputFormat also sometimes fail in trunk?
              </div></li><li><div><div><b>body:</b> Applied patches from this issuse and HDFS-202 on Hadoop v0.19.1, an exception throwed when running nnbench

Exception in thread "IPC Client (47) connection to nn151/192.168.201.151:9020 from zhoumin" java.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.fs.LocatedFileStatus.&lt;init&gt;()
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:81)
        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:53)
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:236)
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:171)
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:219)
        at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:66)
        at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:509)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:439)
Caused by: java.lang.NoSuchMethodException: org.apache.hadoop.fs.LocatedFileStatus.&lt;init&gt;()
        at java.lang.Class.getConstructor0(Class.java:2706)
        at java.lang.Class.getDeclaredConstructor(Class.java:1985)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:75)
        ... 7 more

LocatedFileStatus is a Writable, it should implement a constructor with no params.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Lines listed below will caused a NullPointerException because EMPTY_BLOCK_LOCS  will return null when calling blocks.getLocatedBlocks()
{noformat}
   /** a default LocatedBlocks object, its content should not be changed */
   private final static LocatedBlocks EMPTY_BLOCK_LOCS = new LocatedBlocks();
{noformat}
here is an example of this exception
{noformat}
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hdfs.DFSUtil.locatedBlocks2Locations(DFSUtil.java:84)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getListing(FSDirectory.java:731)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListing(FSNamesystem.java:2015)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getLocatedListing(NameNode.java:494)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:481)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:894)
{noformat}
              </div></li><li><div>
                Min, thanks for trying this patch.

For the first bug, my patch does not transfer LocatedFileStatus over the wire. Did you modify my patch to use LocatedFileStatus instead?

The second is truely a bug. I will get it fixed.

BTW, I have a slightly different implementation of getListing for our internal branch, which can avoid type casting, and therefore provides better performance. I will share this with the community hopefully soon.
              </div></li><li><div>
                Created HDFS-1550 to fix the NPE bug.
              </div></li><li><div>
                @Hairong

Thanks for your share, it greatly helps.  We currently use 0.19.1,  and our namenode will use LocatedFileStatus array over wire after applied your patch  rather than DirectoryListing object. So the first bug happened. 

I have another idea for shorting client's getListing time by caching split files into DistributedCache.  We always scan the same Hive table(or HDFS directory) many times, it needn't call Namenode's getListing again and again if the directory doesn't  have any changes. My idea is getListing once, then cache the result splits,  the subsequent job submissions reuse this cache without any getListing calls. 

              </div></li><li><div>
                -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12452643/mapredListFiles5.patch
  against trunk revision 1074251.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://hudson.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/72//console

This message is automatically generated.
              </div></li><li><div>
                This looks like it would still be useful, but the patch has fallen out of date. Also, how's this related to MAPREDUCE-2349 (if at all)?
              </div></li><li><div>
                we hope this patch will check into trunk.
              </div></li><li><div>
                bq. This looks like it would still be useful, but the patch has fallen out of date.

Agree this would be very useful.  [~hairong] are you planning to update this anytime soon, or would you mind if I push it across the finish line?

bq. Also, how's this related to MAPREDUCE-2349 (if at all)?

MAPREDUCE-2349 involves using threads to pipeline the RPC overhead between independent calls.  Neither requires the other, so I think it's best to keep them separate.  Implementing this JIRA may make MAPREDUCE-2349 unimportant for the common case of a small number of input directories leading to a large number of overall files that need to be located.  However that JIRA could still be useful for the case where the input is a large list.  Without some bulk API in the filesystem there's no getting around doing all the RPC calls, and the thread idea in that JIRA can improve the latency of processing all those calls.
              </div></li><li><div><div><b>body:</b> Refreshed patch and added some unit tests.
                </div><div><b>label:</b> test
                </div></div></li><li><div>
                Would like to get this into 0.23 as well, but it will need a separate patch.  Once we're happy with the trunk patch, I'll build the equivalent for branch-0.23.
              </div></li><li><div>
                {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12592473/MAPREDUCE-1981.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3854//testReport/
Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3854//console

This message is automatically generated.
              </div></li><li><div>
                +1 The patch looks good. I also ran some tests and they worked successfully. Thanks for fixing both mapred and mapreduce. 
              </div></li><li><div>
                Thanks for the review, Kihwal.  Here's the equivalent patch for branch-0.23.
              </div></li><li><div>
                {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12593773/MAPREDUCE-1981.branch-0.23.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3885//console

This message is automatically generated.
              </div></li><li><div>
                {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12593773/MAPREDUCE-1981.branch-0.23.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/3893//console

This message is automatically generated.
              </div></li><li><div>
                +1 The patch for branch-0.23 looks good too.
              </div></li><li><div>
                Thanks for the reviews, Kihwal.  Committing this.
              </div></li><li><div>
                Thanks Hairong, and thanks to everyone that contributed to reviews of various versions of the patch.  I committed this to trunk, branch-2, and branch-0.23.
              </div></li><li><div>
                I merged this into branch-2.1-beta as well.
              </div></li></ol></div></div></html>