<!DOCTYPE html><html><div class="item-title">
        Item 189
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                 Set up the Hadoop Input Format
              </div></li><li><div>
                
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                 Set up the Hadoop Input Format
              </div></li><li><div>
                
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> [FLINK-1271] [hadoop] Remove Writable limitation from Hadoop format and function wrappers
                </div><div><b>message:</b> [FLINK-1271] [hadoop] Remove Writable limitation from Hadoop format and function wrappers

This closes #287

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> [FLINK-1271] Remove writable limitation
                </div><div><b>body:</b> This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

                </div></div></li><li><div><div><b>title:</b> [FLINK-1271] Remove writable limitation
                </div><div><b>body:</b> This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

                </div></div></li><li><div><div><b>title:</b> [FLINK-1271] Remove writable limitation
                </div><div><b>body:</b> This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

                </div></div></li><li><div><div><b>title:</b> [FLINK-1271] Remove writable limitation
                </div><div><b>body:</b> This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

                </div></div></li><li><div><div><b>title:</b> [FLINK-1271] Remove writable limitation
                </div><div><b>body:</b> This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

                </div></div></li><li><div><div><b>title:</b> [FLINK-1271] Remove writable limitation
                </div><div><b>body:</b> This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

                </div></div></li><li><div><div><b>title:</b> [FLINK-1271] Remove writable limitation
                </div><div><b>body:</b> This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

                </div></div></li><li><div><div><b>title:</b> [FLINK-1271] Remove writable limitation
                </div><div><b>body:</b> This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

                </div></div></li><li><div><div><b>title:</b> [FLINK-1271] Remove writable limitation
                </div><div><b>body:</b> This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

                </div></div></li><li><div><div><b>title:</b> [FLINK-1271] Remove writable limitation
                </div><div><b>body:</b> This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

                </div></div></li><li><div><div><b>title:</b> [FLINK-1271] Remove writable limitation
                </div><div><b>body:</b> This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>title:</b> [FLINK-1271] Remove writable limitation
                </div><div><b>body:</b> This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

                </div></div></li><li><div><div><b>title:</b> [FLINK-1271] Remove writable limitation
                </div><div><b>body:</b> This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

                </div></div></li><li><div><div><b>title:</b> [FLINK-1271] Remove writable limitation
                </div><div><b>body:</b> This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

                </div></div></li><li><div><div><b>title:</b> [FLINK-1271] Remove writable limitation
                </div><div><b>body:</b> This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div>
                Hi,

the changes look good. Can you apply the changes also to the "mapred" input/output format? https://github.com/apache/flink/blob/master/flink-addons/flink-hadoop-compatibility/src/main/java/org/apache/flink/hadoopcompatibility/mapred/HadoopInputFormat.java#L53

Also, it would be nice if you could squash your changes into one commit.

              </div></li><li><div>
                Looks good to me.

Can we add a simple test that instantiates the input format wrapper with a MapRed input format &lt;Void, Long&gt; (as in the buggy case) and see that it returns the correct type?

              </div></li><li><div>
                I have three questions: 
1) Shall I also remove the limitations from HadoopMapFunction.java, and HadoopReduceFunction.java, ...
2) With "add a simple test that instantiates the input format wrapper" you mean another Wordcount example with the corresponding "buggy" configuration or can you explain that in more detail?

3) What about a Parquet example?

              </div></li><li><div>
                To clean up history of the changes, could you munge the commits into one?

              </div></li><li><div>
                I will try ;)

              </div></li><li><div>
                1) Should be possible right? Our system supports all types. So I would do it.
2) You probably don't need a full word count for testing the instantiation and type extraction.
3) In the flink-java-examples? I'm not so sure about that because it adds a lot of new dependencies to the project. If you want you can write a blog post about using Parquet with Flink.

              </div></li><li><div>
                Concerning (2) - Let's not add another WordCount - simply making sure that the right type info is found should be sufficient.

              </div></li><li><div>
                I added a simple unit test: HadoopInputFormatTest.java
Did you mean this test?

              </div></li><li><div>
                That looks good, thank you!

              </div></li><li><div>
                Shall I also add the same test for the mapred package?

              </div></li><li><div><div><b>body:</b> Yes, that would be nice. The test looks good.

A bit of style cleanup and some commit squashing and this is good to merge in my opinion.

                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                I rebased some of the commits as requested

              </div></li><li><div>
                LGTM
Will do some tests, clean-up and merge if everything is fine

              </div></li></ol></div><div><b>github_pulls_reviews:</b> <ol></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> Extend HadoopOutputFormat and HadoopInputFormat to handle Void.class 
                </div><div><b>description:</b> Parquet, one of the most famous and efficient column store formats in Hadoop uses Void.class as Key!

At the moment there are only keys allowed which extend Writable.

For example, we would need to be able to do something like:

HadoopInputFormat hadoopInputFormat = new HadoopInputFormat(new ParquetThriftInputFormat(), Void.class, AminoAcid.class, job);
ParquetThriftInputFormat.addInputPath(job, new Path("newpath"));
ParquetThriftInputFormat.setReadSupportClass(job, AminoAcid.class);

// Create a Flink job with it
DataSet&lt;Tuple2&lt;Void, AminoAcid&gt;&gt; data = env.createInput(hadoopInputFormat);

Where AminoAcid is a generated Thrift class in this case.

However, I figured out how to output Parquet files with Parquet by creating a class which extends HadoopOutputFormat.

Now we will have to discuss, what's the best approach to make the Parquet integration happen



                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol><li><div>
                I think we could drop the limitation to {Writable} in the Hadoop formats. The Writable limitation does not really come from the input format, but only from the data movement of Hadoop. Since we operate differently anyways, I see no fundamental reason to restrict the Hadoop Input Format types to Writables.
              </div></li><li><div>
                I think this is also blocked by [FLINK-1273]
              </div></li><li><div>
                I implemented a workaround, which enables you to use Parquet on Flink. 

https://github.com/FelixNeutatz/incubator-flink/tree/ParquetAtFlink/flink-addons/flink-hadoop-compatibility/src/main/java/org/apache/flink/hadoopcompatibility/mapreduce

On my Git repository you will find:
- FlinkParquetOutputFormat.java
- FlinkParquetInputFormat.java

Moreover you find the examples here: 

https://github.com/FelixNeutatz/incubator-flink/tree/ParquetAtFlink/flink-addons/flink-hadoop-compatibility/src/main/java/org/apache/flink/hadoopcompatibility/mapreduce/example

I know that this is just a short term fix, but in my point of view it is good to see that it actually works on Flink :)
              </div></li><li><div>
                Since Void is now supported, we can now do the next step and change Hadoop Input/Output Format to support Parquet.

I did the necessary changes here:
https://github.com/FelixNeutatz/incubator-flink/blob/ParquetAtFlink/flink-addons/flink-hadoop-compatibility/src/main/java/org/apache/flink/hadoopcompatibility/mapreduce/HadoopInputFormat.java

https://github.com/FelixNeutatz/incubator-flink/blob/ParquetAtFlink/flink-addons/flink-hadoop-compatibility/src/main/java/org/apache/flink/hadoopcompatibility/mapreduce/HadoopOutputFormat.java

Do you want to change it Fabian, or shall I push the corresponding changes?
              </div></li><li><div>
                Sorry, just realised that I did not hit the Add button for the comment I wrote yesterday and just reassigned the ticket.

Anyway, I haven't started with this issue. So please go ahead :-)
Cheers, Fabian
              </div></li><li><div>
                GitHub user FelixNeutatz opened a pull request:

    https://github.com/apache/flink/pull/287

    [FLINK-1271] Remove writable limitation

    This pull request will remove the limitation of the Hadoop Format to use writables. This makes it possible to use Parquet.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/FelixNeutatz/incubator-flink RemoveWritableLimitation

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/flink/pull/287.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #287
    
----
commit b5f399d933f0d0697c7b17752277ad4f751eb2c2
Author: FelixNeutatz &lt;neutatz@googlemail.com&gt;
Date:   2015-01-06T19:47:00Z

    [FLINK-1271] Remove Writable limitation from Hadoop Format

commit 43be886042cb145b0a4677e7e5528ea7eb1fedb0
Author: FelixNeutatz &lt;neutatz@googlemail.com&gt;
Date:   2015-01-06T19:58:45Z

    [FLINK-1271] clean format

commit 44da293b6a872e2a97665088af6b7088ef4befe6
Author: FelixNeutatz &lt;neutatz@googlemail.com&gt;
Date:   2015-01-06T20:09:52Z

    [FLINK-1271] clean format 2

commit 8001adb5272e3e2d866025445a4aa32fa82c6329
Author: FelixNeutatz &lt;neutatz@googlemail.com&gt;
Date:   2015-01-06T20:16:48Z

    [FLINK-1271] clean3

commit 6f634c6950901f63288113786e9f4afb4c32ca97
Author: FelixNeutatz &lt;neutatz@googlemail.com&gt;
Date:   2015-01-06T20:22:53Z

    [FLINK-1271] clean 4

commit e9d3b7bd6e578aafe14935fe0d3aa7daa8a4d311
Author: FelixNeutatz &lt;neutatz@googlemail.com&gt;
Date:   2015-01-06T20:27:43Z

    [FLINK-1271] clean 5

commit 0670c4cc967700cd7ba685e3e2085950bca26aa8
Author: FelixNeutatz &lt;neutatz@googlemail.com&gt;
Date:   2015-01-06T20:31:56Z

    [FLINK-1271] clean 5

commit fefb880f496043d7fc9cac896210740dfa18f57e
Author: FelixNeutatz &lt;neutatz@googlemail.com&gt;
Date:   2015-01-06T20:36:17Z

    [FLINK-1271] clean 7

commit 0eb74d2929dd4c5659a9f05064d32fbeb9bec5c8
Author: FelixNeutatz &lt;neutatz@googlemail.com&gt;
Date:   2015-01-06T20:54:04Z

    [FLINK-1271] clean up +1

commit b50324c751a52b31d8533a2d2116191715f504b0
Author: FelixNeutatz &lt;neutatz@googlemail.com&gt;
Date:   2015-01-06T20:58:16Z

    [FLINK-1271] clean up

----

              </div></li><li><div>
                Github user rmetzger commented on the pull request:

    https://github.com/apache/flink/pull/287#issuecomment-68994150
  
    Hi,
    
    the changes look good. Can you apply the changes also to the "mapred" input/output format? https://github.com/apache/flink/blob/master/flink-addons/flink-hadoop-compatibility/src/main/java/org/apache/flink/hadoopcompatibility/mapred/HadoopInputFormat.java#L53
    
    Also, it would be nice if you could squash your changes into one commit.

              </div></li><li><div>
                Github user StephanEwen commented on the pull request:

    https://github.com/apache/flink/pull/287#issuecomment-69049055
  
    Looks good to me.
    
    Can we add a simple test that instantiates the input format wrapper with a MapRed input format &lt;Void, Long&gt; (as in the buggy case) and see that it returns the correct type?

              </div></li><li><div>
                Github user FelixNeutatz commented on the pull request:

    https://github.com/apache/flink/pull/287#issuecomment-69113149
  
    I have three questions: 
    1) Shall I also remove the limitations from HadoopMapFunction.java, and HadoopReduceFunction.java, ...
    2) With "add a simple test that instantiates the input format wrapper" you mean another Wordcount example with the corresponding "buggy" configuration or can you explain that in more detail?
    
    3) What about a Parquet example?

              </div></li><li><div>
                Github user hsaputra commented on the pull request:

    https://github.com/apache/flink/pull/287#issuecomment-69120239
  
    To clean up history of the changes, could you munge the commits into one?

              </div></li><li><div>
                Github user FelixNeutatz commented on the pull request:

    https://github.com/apache/flink/pull/287#issuecomment-69153105
  
    I will try ;)

              </div></li><li><div>
                Github user rmetzger commented on the pull request:

    https://github.com/apache/flink/pull/287#issuecomment-69160636
  
    1) Should be possible right? Our system supports all types. So I would do it.
    2) You probably don't need a full word count for testing the instantiation and type extraction.
    3) In the flink-java-examples? I'm not so sure about that because it adds a lot of new dependencies to the project. If you want you can write a blog post about using Parquet with Flink.

              </div></li><li><div>
                Github user StephanEwen commented on the pull request:

    https://github.com/apache/flink/pull/287#issuecomment-69161456
  
    Concerning (2) - Let's not add another WordCount - simply making sure that the right type info is found should be sufficient.

              </div></li><li><div>
                Github user FelixNeutatz commented on the pull request:

    https://github.com/apache/flink/pull/287#issuecomment-69220758
  
    I added a simple unit test: HadoopInputFormatTest.java
    Did you mean this test?

              </div></li><li><div>
                Github user StephanEwen commented on the pull request:

    https://github.com/apache/flink/pull/287#issuecomment-69221721
  
    That looks good, thank you!

              </div></li><li><div>
                Github user FelixNeutatz commented on the pull request:

    https://github.com/apache/flink/pull/287#issuecomment-69222057
  
    Shall also add the same test for the mapred package?

              </div></li><li><div>
                Github user StephanEwen commented on the pull request:

    https://github.com/apache/flink/pull/287#issuecomment-69358588
  
    Yes, that would be nice. The test looks good.
    
    A bit of style cleanup and some commit squashing and this is good to merge in my opinion.

              </div></li><li><div>
                Github user FelixNeutatz commented on the pull request:

    https://github.com/apache/flink/pull/287#issuecomment-69513699
  
    I rebased some of the commits as requested

              </div></li><li><div>
                Github user fhueske commented on the pull request:

    https://github.com/apache/flink/pull/287#issuecomment-70112509
  
    LGTM
    Will do some tests, clean-up and merge if everything is fine

              </div></li><li><div>
                Github user asfgit closed the pull request at:

    https://github.com/apache/flink/pull/287

              </div></li><li><div>
                Fixed in ba7a19c10df9f2abb5fe9828e57f46a49cbcfd18
Thanks!
              </div></li><li><div>
                Github user MohamedNadjibMAMI commented on the pull request:

    https://github.com/apache/flink/pull/287#issuecomment-94259921
  
    "If you want you can write a blog post about using Parquet with Flink." This would a great plus for the project. Hope it goes a bit deeper and enters the official documentation.

              </div></li></ol></div></div></html>