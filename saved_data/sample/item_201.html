<!DOCTYPE html><html><div class="item-title">
        Item 201
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                
              </div></li><li><div>
                 This benchmarks acts as a reference to the native std::vector
 implementation. It appends kRounds chunks into a vector.
              </div></li><li><div>
                 Insert an element late in the game that does not fit in the 8bit
 representation. This forces AdaptiveIntBuilder's to resize.
              </div></li><li><div>
                 Make a vector out of `kDistinctElements` string values
              </div></li><li><div>
                 Pattern matching that extracts the vector element type of Benchmark::Args()
 Benchmark changed its parameter type between releases from
 int to int64_t. As it doesn't have version macros, we need
 to apply C++ template magic.
              </div></li><li><div>
                 Regression do not need to account for cache hierarchy, thus optimize for
 the best case.
              </div></li><li><div>
                 ARROW_WITH_BENCHMARKS_REFERENCE
              </div></li><li><div><div><b>comment:</b>  Provides better regression stability with timings rather than bogus
 bandwidth.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                 Linter stipulates:
 &gt;&gt; For a static/global string constant, use a C style string instead
              </div></li><li><div>
                 ARROW_WITH_BENCHMARKS_REFERENCE
              </div></li><li><div>
                 pretty 
              </div></li><li><div>
                 Trigger the slow path where the buffer is not byte aligned.
 NOLINT non-const reference
              </div></li><li><div>
                 ARROW_WITH_BENCHMARKS_REFERENCE
              </div></li><li><div>
                 ARROW_WITH_BENCHMARKS_REFERENCE
              </div></li><li><div>
                 This benchmark simply provides a baseline indicating the raw cost of our workload
 depending on the workload size.  Number of items / second in this (serial)
 benchmark can be compared to the numbers obtained in ThreadPoolSpawn.
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> ARROW-5269: [C++][Archery] Mark relevant benchmarks as regression
                </div><div><b>message:</b> ARROW-5269: [C++][Archery] Mark relevant benchmarks as regression

The goal of this change is to mark benchmarks candidate for automated regression checks. Some benchmarks were refactored for various reasons:

- Code deduplication where needed
- Uniform input size for benchmarks in the same suite, usually to minimize the effect of cache hierarchy.
- Favor external repetitions over manual repetitions and mintime when possible.
- Add cmake ARROW_BUILD_BENCHMARKS_REFERENCE to toggle reference benchmarks.
- Remove default benchmark filter of `^Regression`.
- Remove `BM_` prefix from benchmark names
- Fixes and addons to `archery` to support the `--pdb` option for debugging and add support for benchmarks reported as `items_per_seconds`.
- Adds `archery benchmark list` sub-command to list suites and benchmarks

Author: François Saint-Jacques &lt;fsaintjacques@gmail.com&gt;

Closes #4285 from fsaintjacques/ARROW-5269-regression-candidates and squashes the following commits:

83780304d &lt;François Saint-Jacques&gt; Address comments
4a570ab90 &lt;François Saint-Jacques&gt; Reformat
a9ecffd98 &lt;François Saint-Jacques&gt; address comments
d8e779aab &lt;François Saint-Jacques&gt; Reformat
8334c68f7 &lt;François Saint-Jacques&gt; Remove BM_ benchmark prefix
5cebe2c23 &lt;François Saint-Jacques&gt; Normalize benchmarks
908f05a5b &lt;François Saint-Jacques&gt; Change default repetitions to 10 instead of 20
ffe723f05 &lt;François Saint-Jacques&gt; Add `benchmark list` sub-command.
e0876474b &lt;François Saint-Jacques&gt; Refactor JSON benchmarks
a7b4f5f50 &lt;François Saint-Jacques&gt; Improve archery benchmark support
bea76ed0e &lt;François Saint-Jacques&gt; Refactor csv benchmarks
f207974c0 &lt;François Saint-Jacques&gt; Refactor CompareFilter benchmarks
b7911df3d &lt;François Saint-Jacques&gt; Refactor Bitmap benchmarks
1b5135ebd &lt;François Saint-Jacques&gt; Refactor Builder benchmarks

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> ARROW-5269: [C++][Archery] Mark relevant benchmarks as regression
                </div><div><b>body:</b> The goal of this change is to mark benchmarks candidate for automated regression checks. Some benchmarks were refactored for various reasons:

- Code deduplication where needed
- Uniform input size for benchmarks in the same suite, usually to minimize the effect of cache hierarchy.
- Favor external repetitions over manual repetitions and mintime when possible.
- Add cmake ARROW_BUILD_BENCHMARKS_REFERENCE to toggle reference benchmarks.
- Remove default benchmark filter of `^Regression`.
- Remove `BM_` prefix from benchmark names
- Fixes and addons to `archery` to support the `--pdb` option for debugging and add support for benchmarks reported as `items_per_seconds`.
- Adds `archery benchmark list` sub-command to list suites and benchmarks
                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div><div><b>body:</b> I'm not sure why the "Regression" designation is necessary. It seems like we would want to monitor the performance of all of our benchmarks for significant changes. 
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> Multiple reasons:

- some benchmarks are flaky.
- some benchmarks exists only for point of reference (anything with Naive/Dummy in the name) for other benchmarks or hardware discover-ability, e.g. the memory latency and bandwidth benchmarks.
- some benchmarks are heavyweight

I think it's preferable to white list than include everything and possibly increase false positive regression alerts. It also lower the time to run benchmarks since archery is explicitly passing (by default) the `--benchmark_filter="^Regression"` options.
                </div><div><b>label:</b> test
                </div></div></li><li><div>
                By the way, I have no attachment to `Regression` it can be something else, but the function name (and hence the benchmark name) paired with a regex are the easiest way to label and filter.
              </div></li><li><div><div><b>body:</b> If it's an issue with reporting on flaky benchmarks, we could create a "blacklist file" to instruct the reporter not to report regressions in known flakes. 
                </div><div><b>label:</b> test
                </div></div></li><li><div><div><b>body:</b> Some thoughts, for what they're worth:

* A benefit of blacklisting vs. whitelisting (whatever the mechanism for indicating it) is that the default is to include benchmarks in regression testing, and I think that's the right default--if you're adding a benchmark, most likely the purpose is to make sure performance doesn't go down in the future. The bias should be in favor of this, and make developers have to opt out by blacklisting.
* I'm generally of the opinion that flaky tests have negative value--you don't notice them when they pass, and when they fail, you don't believe them--and should either be fixed immediately or deleted (or skipped, if you're into that). My instinct is that the same is true for flaky benchmarks. 
* Maybe heavy benchmarks belong in a different place altogether since we aren't going to run them routinely.
                </div><div><b>label:</b> test
                </div></div></li><li><div>
                Ok, I'll just remove the filtering by default. Should we strip the `BM_` out of every benchmark names? I don't see the value in keeping this.
              </div></li><li><div><div><b>body:</b> It's been several days and I'm still having a hard time swallowing the "Regression*" naming convention
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                @ursabot benchmark
              </div></li><li><div>
                I've successfully started builds for this PR
              </div></li><li><div>
                [AMD64 Ubuntu 18.04 C++ Benchmark](https://ci.ursalabs.org/#builders/73/builds/19)

Build failed.
              </div></li><li><div>
                # [Codecov](https://codecov.io/gh/apache/arrow/pull/4285?src=pr&amp;el=h1) Report
&gt; Merging [#4285](https://codecov.io/gh/apache/arrow/pull/4285?src=pr&amp;el=desc) into [master](https://codecov.io/gh/apache/arrow/commit/68329e728c5356f85c4a492c9749ea9125b06993?src=pr&amp;el=desc) will **increase** coverage by `0.98%`.
&gt; The diff coverage is `n/a`.

[![Impacted file tree graph](https://codecov.io/gh/apache/arrow/pull/4285/graphs/tree.svg?width=650&amp;token=LpTCFbqVT1&amp;height=150&amp;src=pr)](https://codecov.io/gh/apache/arrow/pull/4285?src=pr&amp;el=tree)

```diff
@@            Coverage Diff             @@
##           master    #4285      +/-   ##
==========================================
+ Coverage    88.3%   89.28%   +0.98%     
==========================================
  Files         780      636     -144     
  Lines       98400    87138   -11262     
  Branches     1251        0    -1251     
==========================================
- Hits        86891    77801    -9090     
+ Misses      11273     9337    -1936     
+ Partials      236        0     -236
```


| [Impacted Files](https://codecov.io/gh/apache/arrow/pull/4285?src=pr&amp;el=tree) | Coverage Δ | |
|---|---|---|
| [cpp/src/plasma/thirdparty/ae/ae.c](https://codecov.io/gh/apache/arrow/pull/4285/diff?src=pr&amp;el=tree#diff-Y3BwL3NyYy9wbGFzbWEvdGhpcmRwYXJ0eS9hZS9hZS5j) | `70.75% &lt;0%&gt; (-0.95%)` | :arrow_down: |
| [cpp/src/plasma/test/external\_store\_tests.cc](https://codecov.io/gh/apache/arrow/pull/4285/diff?src=pr&amp;el=tree#diff-Y3BwL3NyYy9wbGFzbWEvdGVzdC9leHRlcm5hbF9zdG9yZV90ZXN0cy5jYw==) | `100% &lt;0%&gt; (ø)` | :arrow_up: |
| [cpp/src/plasma/test/client\_tests.cc](https://codecov.io/gh/apache/arrow/pull/4285/diff?src=pr&amp;el=tree#diff-Y3BwL3NyYy9wbGFzbWEvdGVzdC9jbGllbnRfdGVzdHMuY2M=) | `100% &lt;0%&gt; (ø)` | :arrow_up: |
| [cpp/src/plasma/test/serialization\_tests.cc](https://codecov.io/gh/apache/arrow/pull/4285/diff?src=pr&amp;el=tree#diff-Y3BwL3NyYy9wbGFzbWEvdGVzdC9zZXJpYWxpemF0aW9uX3Rlc3RzLmNj) | `100% &lt;0%&gt; (ø)` | :arrow_up: |
| [go/arrow/ipc/writer.go](https://codecov.io/gh/apache/arrow/pull/4285/diff?src=pr&amp;el=tree#diff-Z28vYXJyb3cvaXBjL3dyaXRlci5nbw==) | | |
| [go/arrow/math/uint64\_amd64.go](https://codecov.io/gh/apache/arrow/pull/4285/diff?src=pr&amp;el=tree#diff-Z28vYXJyb3cvbWF0aC91aW50NjRfYW1kNjQuZ28=) | | |
| [go/arrow/memory/memory\_avx2\_amd64.go](https://codecov.io/gh/apache/arrow/pull/4285/diff?src=pr&amp;el=tree#diff-Z28vYXJyb3cvbWVtb3J5L21lbW9yeV9hdngyX2FtZDY0Lmdv) | | |
| [go/arrow/ipc/file\_reader.go](https://codecov.io/gh/apache/arrow/pull/4285/diff?src=pr&amp;el=tree#diff-Z28vYXJyb3cvaXBjL2ZpbGVfcmVhZGVyLmdv) | | |
| [js/src/enum.ts](https://codecov.io/gh/apache/arrow/pull/4285/diff?src=pr&amp;el=tree#diff-anMvc3JjL2VudW0udHM=) | | |
| [go/arrow/array/builder.go](https://codecov.io/gh/apache/arrow/pull/4285/diff?src=pr&amp;el=tree#diff-Z28vYXJyb3cvYXJyYXkvYnVpbGRlci5nbw==) | | |
| ... and [144 more](https://codecov.io/gh/apache/arrow/pull/4285/diff?src=pr&amp;el=tree-more) | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/4285?src=pr&amp;el=continue).
&gt; **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
&gt; `Δ = absolute &lt;relative&gt; (impact)`, `ø = not affected`, `? = missing data`
&gt; Powered by [Codecov](https://codecov.io/gh/apache/arrow/pull/4285?src=pr&amp;el=footer). Last update [68329e7...27633c9](https://codecov.io/gh/apache/arrow/pull/4285?src=pr&amp;el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).

              </div></li><li><div>
                @ursabot benchmark
              </div></li><li><div>
                @pitrou note that I added the cmake configuration flag `ARROW_BUILD_BENCHMARKS_REFERENCE` which toggles a C/C++ define `ARROW_WITH_BENCHMARKS_REFERENCE`. The goal is to avoid compiling (and running) benchmarks which are not used for monitoring purposes but only point of reference, e.g. memory latency/bandwidth, naive implementations of algorithms...

Thus if you add a new benchmark and you know that it's only used for a point of reference of the true benchmark, you should probably wrap it with said `#ifdef`.
              </div></li><li><div>
                I've successfully started builds for this PR
              </div></li><li><div>
                [AMD64 Ubuntu 18.04 C++ Benchmark](https://ci.ursalabs.org/#builders/73/builds/20)

Build failed with an exception.
              </div></li><li><div>
                @ursabot benchmark
              </div></li><li><div>
                I've successfully started builds for this PR
              </div></li><li><div>
                Why did We get empty output in https://ci.ursalabs.org/#builders/73/builds/20 ?
              </div></li><li><div>
                @kszucs because benchmarks were renamed and thus the intersection between 2 run is empty. See https://github.com/apache/arrow/blob/master/dev/archery/archery/benchmark/compare.py#L115-L116
              </div></li><li><div>
                [AMD64 Ubuntu 18.04 C++ Benchmark](https://ci.ursalabs.org/#builders/73/builds/21)

Build failed with an exception.
              </div></li><li><div>
                @pitrou @kszucs I addressed the comments.
              </div></li></ol></div><div><b>github_pulls_reviews:</b> <ol><li><div>
                `BuildIntArrayNoNulls`?
              </div></li><li><div>
                Is this equal to `kBytesProcessPerRound`?
              </div></li><li><div><div><b>body:</b> I find `kBytesProcessPerRound` and `kBytesProcessed` a bit confusing, at first `round` and `iteration` seem synonyms, but they mean different loops.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> A bit more descriptive name?
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                `click.echo`?
              </div></li><li><div><div><b>body:</b> Really just a nit, but `ARROW_WITH_REFERENCE_BENCHMARKS` might be a better choice.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Well, there's `ARROW_BUILD_BENCHMARKS` already. The convention seems to be `ARROW_BUILD_xxx` to enable the `xxx` target, and `ARROW_WITH_yyy` to enable support for the `yyy` external library.
              </div></li><li><div>
                What is this already? I can't find a reference for the git pseudo-revisions.
              </div></li><li><div>
                Or simply:
```python
return self.bytes_per_second or self.items_per_second or self.time
```
;-)
              </div></li><li><div>
                I'm curious, why is this a reference benchmark?
              </div></li><li><div>
                Shouldn't the reference benchmarks here be built conditionally?
              </div></li><li><div><div><b>body:</b> I think these are pretty much reference benchmarks, unless we find out that compression performance is affected by our abstraction layer...
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Is it intended to force the number of repetitions?
              </div></li><li><div>
                Ditto here.
              </div></li><li><div>
                The "inlined trie" benchmarks are reference benchmarks (to compare against a hand-written specialized version of trie lookup).
              </div></li><li><div>
                I think this is a reference benchmark as well.
              </div></li><li><div>
                Are you suggesting changing the s/BUILD/WITH/ or the swap of BENCHMARKS/REFERENCE?
              </div></li><li><div>
                Correct, `iterations` is a google benchmark concept. Round is used to make a distinction between batch APIs versus point APIs.
              </div></li><li><div>
                I'll rename `kFinalSize` to `kRounds` for this.
              </div></li><li><div>
                it's not in git pseudo revisions, it's a magic keyword that only `archery` knows, it basically says, don't touch the git repository, use the workspace as-is (with dirty worktree). In `archery benchmark diff --help`:

&gt; The special token "WORKSPACE" is reserved to specify the current git
  workspace. This imply that no clone will be performed.

              </div></li><li><div>
                I decided not to include it yet because the n_proc inference isn't accounting for cpuset (cgroup). Thus I fear this benchmark is highly variable if it runs in cgroups, which I've prepared on dgx2 for benchmarks.

Since we don't have a database, I'll revert this and leave it as a normal benchmark.
              </div></li><li><div>
                Hmm... if the baseline is the workspace, what is it compared against?
              </div></li><li><div>
                That's the run sub-command, so it's not comparing against anything. Baseline might be mis-leading here, it probably should be revision?
              </div></li><li><div>
                Yes, that would be better :)
              </div></li><li><div>
                Only this one, or all of them?
              </div></li><li><div>
                This one only.
              </div></li><li><div><div><b>body:</b> Yes, wanted to minimize the time it takes to run and this one was in the heavy. I'll remove it since it doesn't change by that much.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> Hmm... why? You could use `const std::string`.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> Nit: "BuildCSVData".
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                You're not enclosing this is in an `#ifdef ARROW_WITH_BENCHMARKS_REFERENCE`. Is it an oversight?
              </div></li><li><div>
                That's extremely small.
              </div></li><li><div>
                Due to linter, I'll add a comment.
              </div></li><li><div><div><b>body:</b> Wanted to fit in L1, do you suggest a better size? Local tests shows not much difference by increasing this.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                At least 64 kiB. Fitting in L1 is not really a goal here.
              </div></li><li><div>
                yes.
              </div></li><li><div>
                I'm not saying that fitting in L1 is the goal, I meant that some benchmarks (e.g. the kernels), are more limited by the memory bandwidth so lowering the input does show the "maximal" rate. In this case, it doesn't change anything so I'll leave it to 64kb.

Note that my process was to go through all benchmarks and lower the inputs to get a faster running suite. This change is a collateral damage.
              </div></li></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> [C++] Whitelist benchmarks candidates for regression checks
                </div><div><b>description:</b> Rename all benchmarks candidate for regression with the `Regression` prefix.
                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol><li><div>
                Issue resolved by pull request 4285
[https://github.com/apache/arrow/pull/4285]
              </div></li></ol></div></div></html>