<!DOCTYPE html><html><div class="item-title">
        Item 171
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                 pylint: disable=assignment-from-no-return
              </div></li><li><div>
                 Positional relay function to create conv2d NCHWc operator
 used by frontend FFI.
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> [RELAY][TOPI] `alter_op_layout` for x86 (#2602)
                </div><div><b>message:</b> [RELAY][TOPI] `alter_op_layout` for x86 (#2602)

* alter_op_layout for x86

* cleanup

* cleanup

* fix lint

* fix lint

* fix lint

* fix lint

* change support level

* change other support levels

                </div><div><b>label:</b> code-design
                </div></div></li></ol></div><div><b>github_issues:</b> <ol><li><div><div><b>title:</b> [Relay] x86 conv2d_alter_layout support
                </div><div><b>body:</b> Got the warning message "Only support alter layout for x86 in NNVM now. This pass is ignored in relay" when running codes in from_tensorflow tutorial. 

https://github.com/dmlc/tvm/blob/master/topi/python/topi/x86/conv2d.py#L287-L291
                </div></div></li><li><div><div><b>title:</b> [Relay] x86 conv2d_alter_layout support
                </div><div><b>body:</b> Got the warning message "Only support alter layout for x86 in NNVM now. This pass is ignored in relay" when running codes in from_tensorflow tutorial. 

https://github.com/dmlc/tvm/blob/master/topi/python/topi/x86/conv2d.py#L287-L291
                </div></div></li><li><div><div><b>title:</b> [Relay] x86 conv2d_alter_layout support
                </div><div><b>body:</b> Got the warning message "Only support alter layout for x86 in NNVM now. This pass is ignored in relay" when running codes in from_tensorflow tutorial. 

https://github.com/dmlc/tvm/blob/master/topi/python/topi/x86/conv2d.py#L287-L291
                </div></div></li><li><div><div><b>title:</b> [Relay] x86 conv2d_alter_layout support
                </div><div><b>body:</b> Got the warning message "Only support alter layout for x86 in NNVM now. This pass is ignored in relay" when running codes in from_tensorflow tutorial. 

https://github.com/dmlc/tvm/blob/master/topi/python/topi/x86/conv2d.py#L287-L291
                </div></div></li><li><div><div><b>title:</b> [Relay] x86 conv2d_alter_layout support
                </div><div><b>body:</b> Got the warning message "Only support alter layout for x86 in NNVM now. This pass is ignored in relay" when running codes in from_tensorflow tutorial. 

https://github.com/dmlc/tvm/blob/master/topi/python/topi/x86/conv2d.py#L287-L291
                </div></div></li></ol></div><div><b>github_issues_comments:</b> <ol><li><div>
                Would you mind shoot a PR and test for this? I believe the functionality is all prepared.
              </div></li><li><div>
                @yzhliu  Sure, I am working on another pr, will work on this fix after that, hopefully in this week.
              </div></li><li><div>
                Actually I need this support for some experiments with AutoTVM, so I opened a PR for this #2602.

(It was a little more involved than I anticipated, because TOPI still separates x86 NCHWc from vanilla conv2d)
              </div></li></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> [RELAY][TOPI] `alter_op_layout` for x86
                </div><div><b>body:</b> Adds support for `alter_op_layout` for x86 that was discussed in #2585.


Additional comments:
Currently we have an issue where the "vanilla" conv2d operator can be instantiated with some "NCHWc" style layouts, such as "NCHW4c," but not others (e.g., "NCHW8c" or "NCHW16c"). This seems to be due to an issue where the latter two are treated as a special case and handled by the x86 operator. This version seems to share basically all of the same properties as vanilla conv2d, but is not merged together with vanilla conv2d, and other parts of the stack (e.g., AutoTVM) treat these variants of conv2d as separate. It seems theoretically possible to merge the two, but that would likely come at the cost of breaking/changing a few other things (e.g., AutoTVM) and currently TOPI (shared between NNVM and Relay) treats them as separate. This PR mostly preserves the current "organization," but it reuses the Conv2DAttrs node for Conv2DNCHWc, as it seems very excessive to create another identical definition. However, please let me know if reusing the attribute node type is not kosher.

                </div></div></li><li><div><div><b>title:</b> [RELAY][TOPI] `alter_op_layout` for x86
                </div><div><b>body:</b> Adds support for `alter_op_layout` for x86 that was discussed in #2585.


Additional comments:
Currently we have an issue where the "vanilla" conv2d operator can be instantiated with some "NCHWc" style layouts, such as "NCHW4c," but not others (e.g., "NCHW8c" or "NCHW16c"). This seems to be due to an issue where the latter two are treated as a special case and handled by the x86 operator. This version seems to share basically all of the same properties as vanilla conv2d, but is not merged together with vanilla conv2d, and other parts of the stack (e.g., AutoTVM) treat these variants of conv2d as separate. It seems theoretically possible to merge the two, but that would likely come at the cost of breaking/changing a few other things (e.g., AutoTVM) and currently TOPI (shared between NNVM and Relay) treats them as separate. This PR mostly preserves the current "organization," but it reuses the Conv2DAttrs node for Conv2DNCHWc, as it seems very excessive to create another identical definition. However, please let me know if reusing the attribute node type is not kosher.

                </div></div></li><li><div><div><b>title:</b> [RELAY][TOPI] `alter_op_layout` for x86
                </div><div><b>body:</b> Adds support for `alter_op_layout` for x86 that was discussed in #2585.


Additional comments:
Currently we have an issue where the "vanilla" conv2d operator can be instantiated with some "NCHWc" style layouts, such as "NCHW4c," but not others (e.g., "NCHW8c" or "NCHW16c"). This seems to be due to an issue where the latter two are treated as a special case and handled by the x86 operator. This version seems to share basically all of the same properties as vanilla conv2d, but is not merged together with vanilla conv2d, and other parts of the stack (e.g., AutoTVM) treat these variants of conv2d as separate. It seems theoretically possible to merge the two, but that would likely come at the cost of breaking/changing a few other things (e.g., AutoTVM) and currently TOPI (shared between NNVM and Relay) treats them as separate. This PR mostly preserves the current "organization," but it reuses the Conv2DAttrs node for Conv2DNCHWc, as it seems very excessive to create another identical definition. However, please let me know if reusing the attribute node type is not kosher.

                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>title:</b> [RELAY][TOPI] `alter_op_layout` for x86
                </div><div><b>body:</b> Adds support for `alter_op_layout` for x86 that was discussed in #2585.


Additional comments:
Currently we have an issue where the "vanilla" conv2d operator can be instantiated with some "NCHWc" style layouts, such as "NCHW4c," but not others (e.g., "NCHW8c" or "NCHW16c"). This seems to be due to an issue where the latter two are treated as a special case and handled by the x86 operator. This version seems to share basically all of the same properties as vanilla conv2d, but is not merged together with vanilla conv2d, and other parts of the stack (e.g., AutoTVM) treat these variants of conv2d as separate. It seems theoretically possible to merge the two, but that would likely come at the cost of breaking/changing a few other things (e.g., AutoTVM) and currently TOPI (shared between NNVM and Relay) treats them as separate. This PR mostly preserves the current "organization," but it reuses the Conv2DAttrs node for Conv2DNCHWc, as it seems very excessive to create another identical definition. However, please let me know if reusing the attribute node type is not kosher.

                </div></div></li><li><div><div><b>title:</b> [RELAY][TOPI] `alter_op_layout` for x86
                </div><div><b>body:</b> Adds support for `alter_op_layout` for x86 that was discussed in #2585.


Additional comments:
Currently we have an issue where the "vanilla" conv2d operator can be instantiated with some "NCHWc" style layouts, such as "NCHW4c," but not others (e.g., "NCHW8c" or "NCHW16c"). This seems to be due to an issue where the latter two are treated as a special case and handled by the x86 operator. This version seems to share basically all of the same properties as vanilla conv2d, but is not merged together with vanilla conv2d, and other parts of the stack (e.g., AutoTVM) treat these variants of conv2d as separate. It seems theoretically possible to merge the two, but that would likely come at the cost of breaking/changing a few other things (e.g., AutoTVM) and currently TOPI (shared between NNVM and Relay) treats them as separate. This PR mostly preserves the current "organization," but it reuses the Conv2DAttrs node for Conv2DNCHWc, as it seems very excessive to create another identical definition. However, please let me know if reusing the attribute node type is not kosher.

                </div></div></li><li><div><div><b>title:</b> [RELAY][TOPI] `alter_op_layout` for x86
                </div><div><b>body:</b> Adds support for `alter_op_layout` for x86 that was discussed in #2585.


Additional comments:
Currently we have an issue where the "vanilla" conv2d operator can be instantiated with some "NCHWc" style layouts, such as "NCHW4c," but not others (e.g., "NCHW8c" or "NCHW16c"). This seems to be due to an issue where the latter two are treated as a special case and handled by the x86 operator. This version seems to share basically all of the same properties as vanilla conv2d, but is not merged together with vanilla conv2d, and other parts of the stack (e.g., AutoTVM) treat these variants of conv2d as separate. It seems theoretically possible to merge the two, but that would likely come at the cost of breaking/changing a few other things (e.g., AutoTVM) and currently TOPI (shared between NNVM and Relay) treats them as separate. This PR mostly preserves the current "organization," but it reuses the Conv2DAttrs node for Conv2DNCHWc, as it seems very excessive to create another identical definition. However, please let me know if reusing the attribute node type is not kosher.

                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div>
                @eqy Could you also help to change the other contrib oplevels in this file?
              </div></li><li><div>
                Thanks @eqy @tqchen This is now merged
              </div></li></ol></div><div><b>github_pulls_reviews:</b> <ol><li><div>
                maybe use level 5 instead for contrib op?
              </div></li><li><div>
                contrib level is 10
              </div></li></ol></div><div><b>jira_issues:</b> <ol></ol></div><div><b>jira_issues_comments:</b> <ol></ol></div></div></html>