<!DOCTYPE html><html><div class="item-title">
        Item 94
      </div> <div class="item-details"><div><b>git_comments:</b> <ol></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> [SPARK-27812][K8S][2.4] Bump K8S client version to 4.6.1
                </div><div><b>message:</b> [SPARK-27812][K8S][2.4] Bump K8S client version to 4.6.1

# What changes were proposed in this pull request?

Backport of #26093 to `branch-2.4`

### Why are the changes needed?

https://issues.apache.org/jira/browse/SPARK-27812
https://issues.apache.org/jira/browse/SPARK-27927

We need this fix https://github.com/fabric8io/kubernetes-client/pull/1768 that was released on version 4.6 of the client. The root cause of the problem is better explained in https://github.com/apache/spark/pull/25785

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

This patch was tested manually using a simple pyspark job

```python
from pyspark.sql import SparkSession

if __name__ == '__main__':
    spark = SparkSession.builder.getOrCreate()
```

The expected behaviour of this "job" is that both python's and jvm's process exit automatically after the main runs. This is the case for spark versions &lt;= 2.4. On version 2.4.3, the jvm process hangs because there's a non daemon thread running

```
"OkHttp WebSocket https://10.96.0.1/..." #121 prio=5 os_prio=0 tid=0x00007fb27c005800 nid=0x24b waiting on condition [0x00007fb300847000]
"OkHttp WebSocket https://10.96.0.1/..." #117 prio=5 os_prio=0 tid=0x00007fb28c004000 nid=0x247 waiting on condition [0x00007fb300e4b000]
```
This is caused by a bug on `kubernetes-client` library, which is fixed on the version that we are upgrading to.

When the mentioned job is run with this patch applied, the behaviour from spark &lt;= 2.4.0 is restored and both processes terminate successfully

Closes #26152 from igorcalabria/k8s-client-update-2.4.

Authored-by: igor.calabria &lt;igor.calabria@ubee.in&gt;
Signed-off-by: Dongjoon Hyun &lt;dhyun@apple.com&gt;

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> [SPARK-27812][K8S] Bump K8S client version to 4.6.1
                </div><div><b>body:</b> ### What changes were proposed in this pull request?

Updated kubernetes client. 

### Why are the changes needed?

https://issues.apache.org/jira/browse/SPARK-27812
https://issues.apache.org/jira/browse/SPARK-27927

We need this fix https://github.com/fabric8io/kubernetes-client/pull/1768 that was released on version 4.6 of the client. The root cause of the problem is better explained in https://github.com/apache/spark/pull/25785

### Does this PR introduce any user-facing change?

Nope, it should be transparent to users

### How was this patch tested?

This patch was tested manually using a simple pyspark job

```python
from pyspark.sql import SparkSession

if __name__ == '__main__':
    spark = SparkSession.builder.getOrCreate()
```

The expected behaviour of this "job" is that both python's and jvm's process exit automatically after the main runs. This is the case for spark versions &lt;= 2.4. On version 2.4.3, the jvm process hangs because there's a non daemon thread running 

```
"OkHttp WebSocket https://10.96.0.1/..." #121 prio=5 os_prio=0 tid=0x00007fb27c005800 nid=0x24b waiting on condition [0x00007fb300847000]
"OkHttp WebSocket https://10.96.0.1/..." #117 prio=5 os_prio=0 tid=0x00007fb28c004000 nid=0x247 waiting on condition [0x00007fb300e4b000]
```
This is caused by a bug on `kubernetes-client` library, which is fixed on the version that we are upgrading to.

When the mentioned job is run with this patch applied, the behaviour from spark &lt;= 2.4.3 is restored and both processes terminate successfully  
                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div>
                @igorcalabria thank you for writing this patch

I have ran this internally on my cluster and can confirm that non-daemon threads no longer block the JVM from exiting.

Against branch 2.4 and master:
=======================
 I see the following non-daemon threads exist when running 
spark-submit against the following pyspark job:
```
from pyspark.sql import SparkSession

if __name__ == '__main__':
    spark = SparkSession.builder.getOrCreate()
```

After the python process exited, the JVM process remained and I see the following using `jstack`:
```
"OkHttp WebSocket https://10.96.0.1/..." #121 prio=5 os_prio=0 tid=0x00007fb27c005800 nid=0x24b waiting on condition [0x00007fb300847000]
"OkHttp WebSocket https://10.96.0.1/..." #117 prio=5 os_prio=0 tid=0x00007fb28c004000 nid=0x247 waiting on condition [0x00007fb300e4b000]
```

Against this patch:
==============
I no longer see the following non-daemon threads and the JVM is able to exit after the completion of the python process. 


@mccheah @erikerlandson @felixcheung @holdenk we should backport this fix to branch 2.4 as well and cut a new release with this patch. 

              </div></li><li><div>
                ok to test
              </div></li><li><div>
                @igorcalabria . The following claims is not matched with your PR description.
&gt; Standard kubernetes tests should validate this patch.

Please describe how to test this for the following for the reviewer. (Technically, this PR need a test case for that. Or, at least, manual testing process should be described in the PR description and should be the commit log.)
&gt; https://issues.apache.org/jira/browse/SPARK-27812
&gt; https://issues.apache.org/jira/browse/SPARK-27927
              </div></li><li><div>
                **[Test build #111979 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/111979/testReport)** for PR 26093 at commit [`7f15bac`](https://github.com/apache/spark/commit/7f15bacbb9b8ac4a448e3f0fca8213632c07c52d).
 * This patch passes all tests.
 * This patch merges cleanly.
 * This patch adds no public classes.
              </div></li><li><div>
                &gt; @igorcalabria . The following claims is not matched with your PR description.
&gt; 
&gt; &gt; Standard kubernetes tests should validate this patch.
&gt; 
&gt; Please describe how to test this for the following for the reviewer. (Technically, this PR need a test case for that. Or, at least, manual testing process should be described in the PR description and should be the commit log.)
&gt; 
&gt; &gt; https://issues.apache.org/jira/browse/SPARK-27812
&gt; &gt; https://issues.apache.org/jira/browse/SPARK-27927

@dongjoon-hyun A manual test is described above by @ifilonenko. If you want, I could add an integration test for this specific case. I skipped it because other PRs updating kubernetes-client version didn't add any tests and the README in `kubernetes/integration-tests` states that the framework is under review and subject to major changes
              </div></li><li><div>
                **[Test build #112032 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/112032/testReport)** for PR 26093 at commit [`5d6bcd7`](https://github.com/apache/spark/commit/5d6bcd790b2d37fcce45852dbd03b4e5959bd298).
 * This patch **fails build dependency tests**.
 * This patch merges cleanly.
 * This patch adds no public classes.
              </div></li><li><div>
                If it's easy to write a test for this, OK. If it's complex or would take a long time to run, I can see just verifying the small behavior change with a manual test.
              </div></li><li><div>
                Kubernetes integration test starting
URL: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder-K8s/17039/

              </div></li><li><div>
                Kubernetes integration test status success
URL: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder-K8s/17039/

              </div></li><li><div>
                It's pretty difficult to cover all the code paths for changes in a downstream library. @dongjoon-hyun @srowen is there a standard protocol requiring an additional test for downstream dependencies? In this case, we're picking up a fix to an internal implementation detail in the downstream library, and I'd imagine we'd trust the downstream library to have already done its due diligence to test their fix. Regression tests exist to catch new problems introduced by the dependency bumps as we go along.

I'd agree that a manual test demonstrating the desired end behavior is sufficient here.
              </div></li><li><div>
                Kubernetes integration test starting
URL: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder-K8s/17042/

              </div></li><li><div>
                Kubernetes integration test status success
URL: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder-K8s/17042/

              </div></li><li><div>
                Hi, @igorcalabria and @mccheah .
I'm okay as long as the manual description becomes the PR description and the commit log.
&gt; Please describe how to test this for the following for the reviewer. (Technically, this PR need a test case for that. Or, at least, manual testing process should be described in the PR description and should be the commit log.)
              </div></li><li><div>
                **[Test build #112035 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/112035/testReport)** for PR 26093 at commit [`b31b375`](https://github.com/apache/spark/commit/b31b375266b0a4d9d717f95aa1af8d4a02b05015).
 * This patch passes all tests.
 * This patch merges cleanly.
 * This patch adds no public classes.
              </div></li><li><div>
                +1 on my end, I have tested using manual tests. An integration test seems like a bit of overkill, although a simple jstack check in the test would be sufficient.
              </div></li><li><div>
                &gt; @mccheah @erikerlandson @felixcheung @holdenk we should backport this fix to branch 2.4 as well and cut a new release with this patch.

Will there be a 2.4.5 ?
              </div></li><li><div>
                @erikerlandson I think there should be. This bug will cause pyspark resources to hang unless some external process does resource reaping. 

@dongjoon-hyun I agree, the PR description should change
              </div></li><li><div>
                There will be a 2.4.5 eventually, I'm sure. 2.4.x is a sort of "LTS" branch.
              </div></li><li><div>
                Of course, @erikerlandson . As @srowen said, it's the `LTS` branch.

Since `2.4.0` was released on `November, 2018`, `branch-2.4` should be maintained at least `May, 2020` (18 months). After that, as a `LTS` branch, I believe `branch-2.4` will be maintained for a while.

Since `2.4.4` was released on September, 2019. `2.4.5` will arrive early 2020 and so on. I hope more regular releases. But, the `2.4.x` release schedule heavily depends on the patches on `branche-2.4` and the volunteer for release manager.
              </div></li><li><div>
                @dongjoon-hyun I have updated the description to include a manual test. I believe that both jira issues fix claims are now supported by the manual test case provided in the description. Please let me know if you need anything else.

Cheers,
              </div></li><li><div>
                Thank you for updating, @igorcalabria ! I'll follow the verification steps and try to land this.
              </div></li><li><div>
                Also, cc @jiangxb1987 since he is a release manager for `3.0.0-preview`.
              </div></li><li><div>
                I'm +1 on backporting even though it's a version change I think we need this in 2.4.X
              </div></li><li><div>
                Actually if were going to update to 4.6.1 (and it looks like might as well) @ifilonenko would you have the cycles to manually verify with that version as well?
              </div></li><li><div>
                @dongjoon-hyun bumped version to 4.6.1. Following other reviewers suggestions, I also removed explicit okhttp dep.  
              </div></li><li><div>
                Kubernetes integration test starting
URL: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder-K8s/17209/

              </div></li><li><div>
                Kubernetes integration test status success
URL: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder-K8s/17209/

              </div></li><li><div>
                **[Test build #112221 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/112221/testReport)** for PR 26093 at commit [`a5eb702`](https://github.com/apache/spark/commit/a5eb702a6f41f6e76934d1ec5eee3f7a0374690b).
 * This patch passes all tests.
 * This patch merges cleanly.
 * This patch adds no public classes.
              </div></li><li><div>
                Is this tested with `Python 3` on the master branch?
              </div></li><li><div>
                Thank you so much everyone.
Especially, welcome, @igorcalabria . I added you to the Apache Spark contributor group.
Could you make a backporting PR against branch-2.4, @igorcalabria ?
              </div></li></ol></div><div><b>github_pulls_reviews:</b> <ol><li><div>
                You need to change the `kubernetes.client.version` in `resource-managers/kubernetes/integration-tests/pom.xml` as well. 

and bump: 
```
    &lt;dependency&gt;
      &lt;groupId&gt;com.squareup.okhttp3&lt;/groupId&gt;
      &lt;artifactId&gt;okhttp&lt;/artifactId&gt;
      - &lt;version&gt;3.8.1&lt;/version&gt;
      + &lt;version&gt;3.12.0&lt;/version&gt;
    &lt;/dependency&gt;
```
as I believe there is a dependency here https://mvnrepository.com/artifact/io.fabric8/kubernetes-client/4.6.0 
              </div></li><li><div>
                Ok. Is there a reason why okhttp is added as an explicit dependency(kubernetes-client already brings it)? If we really need a specific version, shouldn't we at least exclude it from `kubernetes-client`? I'm saying this because it's very easy to end up with a unexpected version on the classpath. `kubernetes-client` uses okhttp's version 3.12 since version 4.1.2 but we're only updating it now. 
              </div></li><li><div>
                It may have been to make sure that another dependency on okhttp didn't win out and override to a lower version. It can just be updated here, or you can try removing it and examining the transitive dependencies afterwards. If it's already at 3.12.0, then it can likely be omitted.
              </div></li><li><div>
                That is a good point, @mccheah do we have an opinion on this? I am okay with removing the explicit dependency since `kubernetes-client` already brings it in. If there was a reason, I can't seem to remember, to explicitly state, let's exclude it? 
              </div></li><li><div>
                This one looks okay to me.
              </div></li><li><div>
                agreed we should remove explicit dep to avoid a mismatch failure in the future
              </div></li></ol></div><div><b>jira_issues:</b> <ol></ol></div><div><b>jira_issues_comments:</b> <ol></ol></div></div></html>