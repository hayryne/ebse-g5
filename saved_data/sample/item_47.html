<!DOCTYPE html><html><div class="item-title">
        Item 47
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                 Python's logging library doesn't define anything more detailed than DEBUG, but we'd like a finer-grained setting for
 for highly detailed messages, e.g. logging every single incoming request.
              </div></li><li><div>
                 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
              </div></li><li><div>
                 TODO: currently we maintain just a simple map from all key info -&gt; value. However, some key fields are far
 more common to filter on, so we'd want to index by them, e.g. host, client.id, metric name.
              </div></li><li><div>
                 Don't do any logging here so we get rid of the mostly useless per-request Apache log-style info that spams
 the debug log
              </div></li><li><div>
                 The port to listen on on the worker node, which will be forwarded to the port listening on this driver node
              </div></li><li><div>
                 Convert to tuple of pairs because dicts &amp; lists are unhashable
              </div></li><li><div>
                 Non-final because these are set via configure()
              </div></li><li><div>
                 Static package-private so unit tests can mock reading response
              </div></li><li><div>
                 The set of metrics are updated in init/metricChange/metricRemoval
              </div></li><li><div>
                
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License. You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                 Static package-private so unit tests can use a mock connection
              </div></li><li><div>
                *
 * MetricsReporter that aggregates metrics data and reports it via HTTP requests to a configurable
 * webhook endpoint in JSON format.
 *
 * This is an internal class used for system tests and does not provide any compatibility guarantees.
 
              </div></li><li><div>
                 connection.getResponseCode() implicitly calls getInputStream, so always set to true.
 On the other hand, leaving this out breaks nothing.
              </div></li><li><div>
                 For error conditions, we expect them to come with a response body that we can read &amp; log
              </div></li><li><div>
                 return value not expected to be used
              </div></li><li><div>
                
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License. You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                 We should be left with the modified version of metric1 and metric3
              </div></li><li><div>
                 added in init, modified
 added by change
 added in init, deleted by removal
              </div></li><li><div>
                 Expect that a request is made with the given response code
              </div></li><li><div>
                 Should contain an empty list of metrics, i.e. we report updates even if there are no metrics to report to
 indicate liveness
              </div></li><li><div>
                 Should contain client info...
              </div></li><li><div>
                 public for testing
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> MINOR: Add HttpMetricsReporter for system tests
                </div><div><b>message:</b> MINOR: Add HttpMetricsReporter for system tests

Author: Ewen Cheslack-Postava &lt;me@ewencp.org&gt;

Reviewers: Apurva Mehta &lt;apurva@confluent.io&gt;, Ismael Juma &lt;ismael@juma.me.uk&gt;

Closes #4072 from ewencp/http-metrics

(cherry picked from commit 718dda1144629d824f4bdb8ff73fbd531a22723a)
Signed-off-by: Ewen Cheslack-Postava &lt;me@ewencp.org&gt;

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> MINOR: Add HttpMetricsReporter for system tests
                </div><div><b>body:</b> 
                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div>
                @ijuma @apurvam Working on this for some downstream tests, but I'm looking to add an alternative way of collecting metrics that is less painful than using JmxTool. This adds an HttpReporter and ports one service to use it. I think this is a better long term solution, but didn't want to necessarily port everything over immediately.
              </div></li><li><div>
                Thanks for the PR. 

If I understand correctly, Here is the way it's supposed to work:

1. Each service will have to be modified to report metrics using the `HttpMetricsReporter` java class right.
2. The `HttpMetricsReporter` class needs to be configured to submit metrics using `POST` requests to a python http server.
3. Python service classes access the metrics from the `metrics` method of the http service.

If so, I expected  the `ProducerPerformance` code be updated to write to  the `HttpMetricsReporter`. Or am I missing something?
              </div></li><li><div>
                I think you got the setup right but am not sure if we're interpreting one of the steps differently. You have the flow of data correct (push from the process containing the metrics -&gt; Python web server). ProducerPerformance now [inherits from the HttpMetricsCollector mixin instead of the JmxMixin](https://github.com/apache/kafka/pull/4072/files#diff-5132132f3a20daf38c398cab25152dc6R27). That mixin runs the http webserver and [stores and exposes the metrics](https://github.com/apache/kafka/pull/4072/files#diff-69c8a16f2e231f17c84bcbf90f21b929R95) so that tests can [query the service for relevant metrics](https://github.com/apache/kafka/pull/4072/files#diff-c0b3be3803017fe10aed86e31fbb91b0R180). The only work the ProducerPerformance service needs to do is make sure the HttpMetricsCollector mixin is properly initialized (to start the http server), that it configures it's own process to point at that http server with the HttpMetricsReporter, and then stop the server when it is done with the service.

There are a few differences from the JMX version that might be worth considering:

* This opts in for all metrics without any filtering by default.
* We store a complete history of all samples. I think JMX was only storing something like min/max/current.

I think the main concern here would be for long running tests. For the first issue we can easily add filtering on the storage side in a similar way we had jmx_attributes flags for JMX, though I kind of like minimizing the user effort. For the second one, I am not sure if we really need all of them stored, but another option would be to only save latest by default and log the rest. For debugging purposes, it seems like it would be nice to have them logged.

I also considered if a mixin was the right approach here or not. I don't feel that strongly about it, but I'd like to minimize the effort required to get metrics out of a service as much as possible. The main drawback is there is some awkwardness in trying to make sure method names will be properly namespaced/isolated.

There are also some utilities that I'm sure we'd eventually want here but I haven't needed for the minimal changes I made so far. In particular, a `wait_until` that works specifically for waiting on metric values could be used elsewhere (the one service I changed just happened not to need it).
              </div></li><li><div>
                Thanks for the explanation @ewencp. I was under the impression that this patch would also have to update https://github.com/apache/kafka/blob/trunk/tools/src/main/java/org/apache/kafka/tools/ProducerPerformance.java to populate metrics in the `HttpMetricsReporter.java`, otherwise the metrics would never be exported. Or is this handled implicitly somehow?

In particular, we need the java `HttpMetricsReporter` to make the `POST` call to the python http server periodically to export the metrics. So it at least needs the connection details for the http sever, and also periodic updates from the service whose metrics are being reported. As such, my assumption was that the Java service implementations would need to be updated to do this as well.
              </div></li><li><div>
                I kicked off a system test run with this branch, it is waiting in the queue. 
              </div></li><li><div>
                Looks like the throttling test and quota test failed with:

```
[2017-10-18 16:05:28,847] INFO  values:
        client.id = some_id
        metrics.host = null
        metrics.period = 1
        metrics.url = http://0e12511cc22b:38928
 (org.apache.kafka.tools.HttpMetricsReporter$1)
[2017-10-18 16:05:28,850] INFO Configured HttpMetricsReporter for http://0e12511cc22b:38928 to report every 1 seconds (org.apache.kafka.tools.HttpMetricsReporter)
[2017-10-18 16:05:28,952] WARN The configuration 'sasl.mechanism.inter.broker.protocol' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2017-10-18 16:05:28,953] INFO Kafka version : 1.1.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser)
[2017-10-18 16:05:28,953] INFO Kafka commitId : 8d76b41e61dce8c4 (org.apache.kafka.common.utils.AppInfoParser)
[2017-10-18 16:05:30,004] ERROR Error reporting metrics (org.apache.kafka.tools.HttpMetricsReporter)
java.net.UnknownHostException: 0e12511cc22b
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:178)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:579)
        at java.net.Socket.connect(Socket.java:528)
        at sun.net.NetworkClient.doConnect(NetworkClient.java:180)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
        at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:211)
        at sun.net.www.http.HttpClient.New(HttpClient.java:308)
        at sun.net.www.http.HttpClient.New(HttpClient.java:326)
        at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:997)
        at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:933)
        at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:851)
        at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1092)
        at org.apache.kafka.tools.HttpMetricsReporter$HttpReporter.run(HttpMetricsReporter.java:185)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
[2017-10-18 16:05:54,962] INFO [Producer clientId=some_id] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
(END)
```
It looks like HttpMetricsReporter isn't being configured correctly.
              </div></li><li><div>
                Interesting, it looks like it is because the hostname that it gets when running from a docker container is just the docker container's ID. If you run from a normal EC2 instance it works fine. That's actually quite an annoying issue because it means there's no way to contact into the container running for Jenkins unless Jenkins knows to map specific ports. It's a purely Jenkins problem -- running with ducker or from a real ec2 instance works fine. I'll need to think about how to fix that, though it may be about the infrastructure rather than this patch. I could also make the driver pull from the HttpMetricsReporter, but I didn't really want to have to embed an entire webserver in it...
              </div></li><li><div>
                @ijuma @apurvam A few updates:

* Minor modifications to the reporter and I've added unit tests that provide decent line % coverage
* Updated the vagrant setup to arrange for the driver hostname -&gt; ip mapping to be setup automatically. I'm open to other options, but this seems like the best option to me given the various test configurations we see today (completely local Vagrant setup, docker driver + Vagrant ec2 VMs, ducker)
* Moved shutdown for the metrics listener to the `stop()` method for services. This is the right place for it, but does also assume subclasses call super(Class, self).stop() properly
* Not directly related, but the ducker commands were assuming `releaseTarGz` target should be executed, but only the `systemTestLibs` target is actually necessary. I adjusted this in the scripts, but we can refactor this to a separate PR if we want to keep the changes separated.

I kicked off a previous round of tests w/ an earlier version of patches https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1113/console and they seem to be good. Plan to do the same with these updates once I'm back on the VPN.
              </div></li><li><div>
                Thanks @ewencp. With regards to ducker, you need to update the readme as well. cc @cmccabe
              </div></li><li><div>
                @ijuma ack, updated the README as well as the `.travis.yml` that was doing more than it needs to.
              </div></li><li><div>
                Thanks for the clarifications, @ewencp . My comments were just that: I am happy with things as they are. The patch LGTM!
              </div></li><li><div>
                Cool, was looking for a clean build but Jenkins seems to be in a screwy state. Also, need a +1 from another committer -- @ijuma or @hachikuji perhaps?
              </div></li><li><div>
                SUCCESS 
 8063 tests run, 5 skipped, 0 failed.
--none--
              </div></li><li><div>
                SUCCESS 
 8063 tests run, 5 skipped, 0 failed.
--none--
              </div></li><li><div>
                SUCCESS 
 8063 tests run, 5 skipped, 0 failed.
--none--
              </div></li><li><div>
                Thanks for the reviews. Merging this to trunk and 1.0. I'd also like this on 0.11 but it doesn't cherry-pick cleanly. I will follow up with a PR for that just so I can verify clean unit and system test runs for the cherry-picked and conflict-resolved version.
              </div></li></ol></div><div><b>github_pulls_reviews:</b> <ol><li><div>
                It seems a bit asymmetric to have to call `stop` in the subclass when we never had to call start. It might be missed in future subclasses, causing the thread to leak.
              </div></li><li><div>
                It would be good to have unit tests for this class. Also, do we want to make it clear from the name that this pushes data to an endpoint? One could also imagine a HTTP reporter that returns metrics via a `GET`.
              </div></li><li><div>
                Perhaps it would be worth mentioning that that passing these properties directly into `producer.properties` or `consumer.properties` would mean that those clients will automatically use the the `PushHttpMetricsReporter` configured for pushing their metrics to this server.

I had missed this link in the first pass since I missed the `%s(metrics-props)s` argument added to the `--producer-props` of the performance producer. So it seemed like it was working by magic. 
              </div></li><li><div>
                To clarify for my understanding, the worker which runs the service uses HTTP on the reverse tunnel to push metrics to the server running on the test driver?
              </div></li><li><div>
                The `ProducerPerformanceService` would have to call `super.stop()` for this to be invoked right? I don't think it does it presently. Please correct me if I am wrong.
              </div></li><li><div>
                Sure thing, added a note.
              </div></li><li><div>
                It would need to if it overrode `stop()`. However, the method resolution order for multiple inheritance will make this run first (because the mixin is listed in the super classes of `ProducerPerformanceService` first), then this `super` call will pass it along. Here's a quick example showing the equivalent class hierarchy:

```
&gt;&gt;&gt; class Base(object):
...     pass
...
&gt;&gt;&gt; class Mixin(object):
...     pass
...
&gt;&gt;&gt; class Concrete(Mixin, Base):
...     pass
...
&gt;&gt;&gt; Concrete.mro()
[&lt;class '__main__.Concrete'&gt;, &lt;class '__main__.Mixin'&gt;, &lt;class '__main__.Base'&gt;, &lt;type 'object'&gt;]
```

When python dispatches the initial call and any `super` calls, it works in the order of that list.

Putting mixins first usually makes life easier since the mixins have to take care to properly call `super` but classes using the mixin often don't, or only need to for methods they would have to call it for under normal circumstances in order to let the super class do its work.

This is also why the constructor is written the way it is with the flexible `**kwargs` -- it allows the mixin to work with pretty much any other constructor parameters from the class using the mixin.
              </div></li><li><div>
                Btw, the call order being confusing is a downside of mixins in python, I was also debating whether we should just use composition and have the classes using this do everything explicitly. The nice thing with this approach is that as long as this class implements start + clean up properly, there isn't any burden on the class using it. 
              </div></li><li><div>
                That's correct.
              </div></li><li><div>
                Thanks for that note ewen. I learned something!
              </div></li><li><div>
                Is there a reason to start the server thread here in stead of in the `start_node` method?
              </div></li><li><div>
                Oh my, that is quite some mock code!
              </div></li><li><div>
                In this an the next test, I am not sure what exactly is being tested. 
              </div></li><li><div>
                main reason was that we only need to start one server thread but there could be N nodes reporting to it (so `start_node` would be called N times). I can move it to the `start` method if preferred. that's really the right way to handle this, though I'm not sure there's much effect to moving it. only thing i can think of is if done here we may be unnecessarily binding a bunch of ports early based on when the test instances are instantiated, but I honestly can't remember when exactly instantiation occurs in ducktape (during scanning to build out the list of tests or at the time of the test run).
              </div></li><li><div>
                It's just testing that when there is an error, we read the response. Mainly we want to verify that we are getting and logging the response when there is an error, but testing for the logging step is harder. Even the actual `readResponse` is mocked out because mocking the calls that `Scanner` makes seemed way messier, error prone, and possibly even JVM-specific.
              </div></li><li><div>
                I did consider a non-strict mock here since there are so many calls. Some are useful to verify (e.g. content type), others seem less so (e.g. `setUseCaches`. I'm happy to switch to a non-strict mock to cut down on test noise if that's preferred.
              </div></li><li><div>
                Interesting, so `releaseTarGz` was never needed? I had assumed that there was an actual reason why this was different when the system tests were run via docker.
              </div></li><li><div>
                What's our compatibility guarantees for this class? If we offer none, it might be worth mentioning it in the class javadoc.
              </div></li><li><div>
                Nit: can we move all non-final fields after the final ones? Also, we can maybe add a comment above the non-final ones stating that they are set via `configure`. It might also be worth mentioning that `metrics` gets updated via `init`/`metricChange`/`metricRemoval`.

That makes it easier for someone to understand the lifecycle without having to look at the whole class.
              </div></li><li><div>
                Can we use try with resources?
              </div></li><li><div>
                Are we using `PowerMock` just because of the static methods that we're mocking? Do they have to be static methods?

I've had the pleasure of debugging `PowerMock` issues when upgrading to Java 9 and I think it's best to restrict usage to those cases where it's really beneficial.
              </div></li><li><div>
                We've had the `systemTestLibs` target for quite a long time for this. There definitely shouldn't be a need for different targets between regular and Docker versions of the tests, so I think this just slipped in accidentally. If we do diverge, we're better off just getting them consistent than having different requirements.
              </div></li><li><div>
                None, it's not public API or anything so the guarantees are just as lacking as any other internal class :) I've added a note to the javadoc about this.
              </div></li><li><div>
                Ack
              </div></li><li><div>
                good point, fixed
              </div></li><li><div>
                Hmm, interesting. Are you suggesting a partial mock instead? I usually avoid them because the setup is way messier than mocking statics.
              </div></li><li><div>
                It could be a separate class/interface for the the methods that need to be mocked.

We don't have to do it here (I approved the PR), but I just wanted to raise that there's a cost to PowerMock's magic and that it may be better to use it in exceptional cases instead of by default.
              </div></li></ol></div><div><b>jira_issues:</b> <ol></ol></div><div><b>jira_issues_comments:</b> <ol></ol></div></div></html>