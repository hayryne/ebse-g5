<!DOCTYPE html><html><div class="item-title">
        Item 278
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                *
             * Returns true if x1 is less than x2, when both values are treated as unsigned.
             
              </div></li><li><div><div><b>comment:</b> 
                 * Compare 8 bytes at a time. Benchmarking shows comparing 8 bytes at a time is no slower than comparing
                 * 4 bytes at a time even on 32-bit. On the other hand, it is substantially faster on 64-bit.
                 
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                * The offset to the first element in a byte array. 
              </div></li><li><div><div><b>comment:</b>  It doesn't matter what we throw;
 it's swallowed in getBestComparer().
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                 The epilogue to cover the last (minLength % 8) elements.
              </div></li><li><div>
                *
 * Utility code to do optimized byte-array comparison.
 * This is borrowed from org.apache.hadoop.io.FastByteComparisons
 * which was borrowed and slightly modified from Guava's {@link UnsignedBytes}
 * class to be able to compare arrays that start at non-zero offsets.
 * 
 * The only difference is that we sort a smaller length bytes as *larger*
 * than longer length bytes when all the bytes are the same.
 
              </div></li><li><div>
                 yes, UnsafeComparer does implement Comparer&lt;byte[]&gt;
              </div></li><li><div>
                 Short circuit equal case
              </div></li><li><div>
                 Bring WritableComparator code local
              </div></li><li><div>
                 Use binary search
              </div></li><li><div>
                 used via reflection
              </div></li><li><div>
                *
     * Provides a lexicographical comparer implementation; either a Java implementation or a faster implementation based
     * on {@link Unsafe}.
     * &lt;p&gt;
     * Uses reflection to gracefully fall back to the Java implementation if {@code Unsafe} isn't available.
     
              </div></li><li><div>
                 sanity check - this should never fail
              </div></li><li><div>
                *
         * Returns the Unsafe-using Comparer, or falls back to the pure-Java implementation if unable to do so.
         
              </div></li><li><div>
                 ensure we really catch *everything*
              </div></li><li><div>
                *
             * Lexicographically compare two arrays.
             *
             * @param buffer1
             *            left operand
             * @param buffer2
             *            right operand
             * @param offset1
             *            Where to start comparing in the left buffer
             * @param offset2
             *            Where to start comparing in the right buffer
             * @param length1
             *            How much to compare from the left buffer
             * @param length2
             *            How much to compare from the right buffer
             * @return 0 if equal, &lt; 0 if left is less than right, etc.
             
              </div></li><li><div>
                *
     * Lexicographically compare two byte arrays.
     
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                 If we have a schema where column A is DESC, reverse the sort order and nulls last
 since this is the order they actually are in.
              </div></li><li><div>
                 Treat as binary if descending because we've got a separator byte at the end
 which is not part of the value.
              </div></li><li><div>
                 Don't return if evaluated to null
              </div></li><li><div><div><b>comment:</b>  Special hack for PHOENIX-2067 to change the constant array to match
 the separators used for descending, variable length arrays.
 Note that there'd already be a coerce expression around the constant
 to convert it to the right type, so we shouldn't do that here.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                 Constants are always build with rowKeyOptimizable as true, using the correct separators
 We only need to do this conversion if we have a table that has not yet been converted.
              </div></li><li><div>
                 Add cell for ROW_KEY_ORDER_OPTIMIZABLE = true, as we know that new tables
 conform the correct row key. The exception is for a VIEW, which the client
 sends over depending on its base physical table.
              </div></li><li><div><div><b>comment:</b>  FIXME: we should allow the update, but just not propagate it to this view
 The one exception is PK changes which need to be propagated to diverged views as well
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                 Don't allow view PK to diverge from table PK as our upgrade code
 does not handle this.
              </div></li><li><div>
                 Size for worst case - all new columns are PK column
              </div></li><li><div>
                 Column to track tables that have been upgraded based on PHOENIX-2067
              </div></li><li><div>
                 We're starting a rebuild of the index, so add our rowKeyOrderOptimizable cell
 so that the row keys get generated using the new row key format
              </div></li><li><div>
                 We may be adding a DESC column, so if table is already
 able to be rowKeyOptimized, it should continue to be so.
              </div></li><li><div>
                 We may be adding a DESC column, so if index is already
 able to be rowKeyOptimized, it should continue to be so.
              </div></li><li><div><div><b>comment:</b>  TODO: Switch this to Region#batchMutate when we want to support indexes on the
                </div><div><b>label:</b> requirement
                </div></div></li><li><div>
                 Copy existing cell but with new row key
              </div></li><li><div>
                 for local indexes (prepend region start key)
              </div></li><li><div>
                 force to use correct separator byte
              </div></li><li><div>
                 Special case for re-writing DESC ARRAY, as the actual byte value needs to change in this case
              </div></li><li><div>
                 If Put, point delete old Put
              </div></li><li><div><div><b>comment:</b> *
     * &lt;code&gt;optional bool rowKeyOrderOptimizable = 26;&lt;/code&gt;
     
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                 optional bool rowKeyOrderOptimizable = 26;
              </div></li><li><div><div><b>comment:</b> *
       * &lt;code&gt;optional bool rowKeyOrderOptimizable = 26;&lt;/code&gt;
       
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                 Flip back here based on sort order, as the compiler
 flips this, but we want to display the original back
 to the user.
              </div></li><li><div>
                 Don't remove trailing separator byte unless it's the one for ASC
 as otherwise we need it to ensure sort order is correct
              </div></li><li><div><div><b>comment:</b>  FIXME: calling version of coerceBytes that takes into account the separator used by LHS
 If the RHS does not have the same separator, it'll be coerced to use it. It's unclear
 if we should do the same for all classes derived from the base class.
 Coerce RHS to LHS type
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>comment:</b>  FIXME: concatArrays will be fine if it's copying the separator bytes, including the terminating bytes.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>comment:</b>  TODO: remove when rowKeyOrderOptimizable hack no longer needed
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>comment:</b>  Hack to serialize whether the index row key is optimizable
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                 Write separator byte if variable length unless it's the last field in the schema
 (but we still need to write it if it's DESC to ensure sort order is correct).
              </div></li><li><div><div><b>comment:</b>  Hack for PHOENIX-2067 to force raw scan over all KeyValues to fix their row keys
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                 We project *all* KeyValues across all column families as we make a pass over
 a physical table and we want to make sure we catch all KeyValues that may be
 dynamic or part of an updatable view.
              </div></li><li><div>
                 Remove any filter
 Traverse (and subsequently clone) all KeyValues
 Pass over PTable so we can re-write rows according to the row key schema
              </div></li><li><div>
                
     * Same as regular comparator, but if all the bytes match and the length is
     * different, returns the longer length as bigger.
     
              </div></li><li><div>
                *
     * Returns true if this connection is being used to upgrade the
     * data due to PHOENIX-2067 and false otherwise.
     * @return
     
              </div></li><li><div><div><b>comment:</b>  Manually transfer the ROW_KEY_ORDER_OPTIMIZABLE_BYTES from parent as we don't
 want to add this hacky flag to the schema (see PHOENIX-2067).
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                 Set so that we get the table below with the potentially modified rowKeyOrderOptimizable flag set
              </div></li><li><div>
                *
     * Determines whether or not we may optimize out an ORDER BY or do a GROUP BY
     * in-place when the optimizer tells us it's possible. This is due to PHOENIX-2067
     * and only applicable for tables using DESC primary key column(s) which have
     * not been upgraded.
     * @return true if optimizations row key order optimizations are possible
     
              </div></li><li><div>
                 Need trailing byte for DESC columns
              </div></li><li><div>
                 after hasDescVarLengthColumns is calculated
              </div></li><li><div>
                 Last field has no terminator unless it's descending sort order
              </div></li><li><div>
                 Separator always zero byte if zero length
              </div></li><li><div>
                 First byte 
              </div></li><li><div>
                 Only applicable for RowKeySchema (and only due to PHOENIX-2067), but
 added here as this is where serialization is done (and we need to
 maintain the same serialization shape for b/w compat).
              </div></li><li><div>
                 assumes stats table columns not DESC
 assumes stats table columns not DESC
              </div></li><li><div>
                 Separator for new value
 Double byte separator
              </div></li><li><div>
                 Increase of length required to store nulls
 Length increase incremented by one when there were no nulls at the beginning of array and when there are
 nulls at the end of array 1 as we need to allocate a byte for separator byte in this case.
              </div></li><li><div>
                 Creates a byte array to store the concatenated array
              </div></li><li><div>
                 Write separator explicitly, as it may not be 0
              </div></li><li><div>
                 offsets for nulls in the middle
              </div></li><li><div>
                 Writes nulls in the middle of the array.
 Copies the elements from array 2 beginning from the first non null element.
              </div></li><li><div>
                 In both case the elements and the value array will be the same but the Array 1 is actually smaller because it
 has more nulls.
 Now we should have mechanism to show that we treat arrays with more nulls as lesser. Hence in the above case
 as
 For Array 2, by inverting we would get a -ve value. On comparison Array 2 &gt; Array 1.
 Now for cases where the number of nulls is greater than 255, we would write an those many (byte)1, it is
 bigger than 255.
 This would ensure that we don't compare with triple zero which is used as an end byte
              </div></li><li><div><div><b>comment:</b>  FIXME: remove this duplicate code
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                 offsets for the elements from array 1. Simply copied.
              </div></li><li><div>
                 Explicitly set separator byte since for DESC it won't be 0.
              </div></li><li><div>
                 offsets for the elements from the first non null element from array 2
              </div></li><li><div>
                 Coerce to new max length when only max lengths differ
              </div></li><li><div>
                 padding
              </div></li><li><div>
                 Copies all the elements from array 1 to new array
              </div></li><li><div>
                 serialize nulls at the beginning
              </div></li><li><div>
                 Subtract one and invert so that more remaining nulls becomes smaller than less
 remaining nulls and min byte value is always greater than 1, the repeating value
 The reason we invert is that an array with less null elements has a non
              </div></li><li><div>
                 Here the int for noofelements, byte for the version, int for the offsetarray position and 2 bytes for the
 end seperator
              </div></li><li><div>
                 writes the new offset and changes the previous offsets
              </div></li><li><div>
                *
     * creates array bytes
     * @param rowKeyOrderOptimizable TODO
     
              </div></li><li><div>
                 handle the case where prepended element is null
 counts the number of nulls which are already at the beginning of the array
 gets the offset of the first element after nulls at the beginning
 Calculates the increase in length due to prepending the null
 There is a length increase only when nRemainingNulls == 1
 nRemainingNulls == 1 and nMultiplesOver255 == 0 means there were no nulls at the beginning
 previously.
 At that case we need to increase the length by two bytes, one for separator byte and one for null
 count.
 ex: initial array - 65 0 66 0 0 0 after prepending null - 0 1(inverted) 65 0 66 0 0 0
 nRemainingNulls == 1 and nMultiplesOver255 != 0 means there were null at the beginning previously.
 In this case due to prepending nMultiplesOver255 is increased by 1.
 We need to increase the length by one byte to store increased that.
 ex: initial array - 0 1 65 0 66 0 0 0 after prepending null - 0 1 1(inverted) 65 0 66 0 0 0
 nRemainingNulls == 0 case.
 ex: initial array - 0 254(inverted) 65 0 66 0 0 0 after prepending null - 0 1 65 0 66 0 0 0
 nRemainingNulls &gt; 1 case.
 ex: initial array - 0 45(inverted) 65 0 66 0 0 0 after prepending null - 0 46(inverted) 65 0 66 0 0 0
              </div></li><li><div>
                 checks whether offset array consists of shorts or integers
 count nulls at the end of array 1
 count nulls at the beginning of the array 2
              </div></li><li><div>
                 checks whether offset array consists of shorts or integers
              </div></li><li><div>
                 Assume an offset array that fit into Short.MAX_VALUE. Also not considering nulls that could be &gt; 255
 In both of these cases, finally an array copy would happen
              </div></li><li><div>
                 Writing offset arrays
 offsets for the elements from array 1. Simply copied.
              </div></li><li><div>
                 a, b, null, null, c, null would be
 Follow the above example to understand how this works
              </div></li><li><div>
                 A match for IS NULL or IS NOT NULL should not have a DESC_SEPARATOR_BYTE as nulls sort first
              </div></li><li><div>
                *
     * Return the separator byte to use based on:
     * @param rowKeyOrderOptimizable whether or not the table may optimize descending row keys. If the
     *  table has no descending row keys, this will be true. Also, if the table has been upgraded (using
     *  a new -u option for psql.py), then it'll be true
     * @param isNullValue whether or not the value is null. We use a null byte still if the value is null
     * regardless of sort order since nulls will always sort first this way.
     * @param sortOrder whether the value sorts ascending or descending.
     * @return the byte to use as the separator
     
              </div></li><li><div>
                 Write trailing separator if last expression was variable length and descending
              </div></li><li><div>
                 Must be a table
 Must be global
 Must be the physical table
              </div></li><li><div>
                 Find views to mark as upgraded
              </div></li><li><div>
                 Open tenant-specific connection when we find a new one
              </div></li><li><div>
                 Replace trailing , with ) to end IN expression
              </div></li><li><div>
                 Upgrade view indexes
              </div></li><li><div>
                 Run query
              </div></li><li><div>
                 Find tables/views for index
              </div></li><li><div>
                 Add any tables (which will all be physical tables) which have not already been upgraded.
              </div></li><li><div>
                *
     * Upgrade tables and their indexes due to a bug causing descending row keys to have a row key that
     * prevents them from being sorted correctly (PHOENIX-2067).
     
              </div></li><li><div>
                 Find the header rows for tables that have not been upgraded already.
 We don't care about views, as the row key cannot be different than the table.
 We need this query to find physical tables which won't have a link row.
              </div></li><li><div>
                 First query finds column rows of tables that need to be upgraded.
 We cannot tell if the column is from a table, view, or index however.
              </div></li><li><div>
                 Mark the table and views as upgraded now
              </div></li><li><div>
                *
     * Upgrade shared indexes by querying for all that are associated with our
     * physical table.
     * @return true if any upgrades were performed and false otherwise.
     
              </div></li><li><div>
                 Find physical table name from views, splitting on '.' to get schema name and table name
              </div></li><li><div>
                 Upgrade global indexes
              </div></li><li><div>
                *
     * Identify the tables that need to be upgraded due to PHOENIX-2067
     
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> PHOENIX-2067 Sort order incorrect for variable length DESC columns
                </div><div><b>message:</b> PHOENIX-2067 Sort order incorrect for variable length DESC columns

Conflicts:
	phoenix-core/src/main/java/org/apache/phoenix/coprocessor/MetaDataEndpointImpl.java
	phoenix-core/src/main/java/org/apache/phoenix/coprocessor/UngroupedAggregateRegionObserver.java
	phoenix-core/src/main/java/org/apache/phoenix/execute/BaseQueryPlan.java
	phoenix-core/src/main/java/org/apache/phoenix/jdbc/PhoenixConnection.java

Conflicts:
	phoenix-core/src/main/java/org/apache/phoenix/coprocessor/MetaDataEndpointImpl.java
	phoenix-core/src/main/java/org/apache/phoenix/jdbc/PhoenixConnection.java

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol></ol></div><div><b>github_pulls_comments:</b> <ol></ol></div><div><b>github_pulls_reviews:</b> <ol></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> Sort order incorrect for variable length DESC columns
                </div><div><b>description:</b> Steps to reproduce:
1. Create a table: 
CREATE TABLE mytable (id BIGINT not null PRIMARY KEY, timestamp BIGINT, log_message varchar) IMMUTABLE_ROWS=true, SALT_BUCKETS=16;
2. Create two indexes:
CREATE INDEX mytable_index_search ON mytable(timestamp,id) INCLUDE (log_message) SALT_BUCKETS=16;
CREATE INDEX mytable_index_search_desc ON mytable(timestamp DESC,id DESC) INCLUDE (log_message) SALT_BUCKETS=16;
3. Upsert values:
UPSERT INTO mytable VALUES(1, 1434983826018, 'message1');
UPSERT INTO mytable VALUES(2, 1434983826100, 'message2');
UPSERT INTO mytable VALUES(3, 1434983826101, 'message3');
UPSERT INTO mytable VALUES(4, 1434983826202, 'message4');
4. Sort DESC by timestamp:
select timestamp,id,log_message from mytable ORDER BY timestamp DESC;

Failure: data is sorted incorrectly. In case when we have two longs which  are different only by last two digits (e.g. 1434983826155, 1434983826100)  and one of the long ends with '00' we receive incorrect order. 
Sorting result:
1434983826202
1434983826100
1434983826101
1434983826018
                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol><li><div>
                [~giacomotaylor], please take a look at this issue.
              </div></li><li><div>
                This bug potentially affects any PK column of variable length that is declared as DESC sort order. The particular case is when one value is a subpart of another value. Simplest case would be this:
{code}
CREATE TABLE t (k VARCHAR DESC PRIMARY KEY);
UPSERT INTO t VALUES ('a');
UPSERT INTO t VALUES('ab');
SELECT * FROM t ORDER BY k;
{code}
The 'ab' row should appear before the 'a' row. This is due to there being no terminator at the end of the row key as well as because we're not inverting the null/zero terminator/separator byte between parts of the row key.

The fix is to:
- use a 255 byte separator byte instead of a 0 byte separator between row key parts
- include a 255 byte terminator at the end of the row key
- include a 255 byte for any null value at the end of the row key (without this, row keys with null values in the middle of the row key might sort before a row key with null values at the end of the row key)
- disallow a DESC pk column to be added to the PK in an ALTER TABLE &lt;table&gt; ADD &lt;column&gt; as it would require updating existing data.

We need a script that users can run to fix their existing data (or at least identify that there's an issue).
              </div></li><li><div>
                WIP patch. Unit tests pass along with new ones that demonstrate the issue.
              </div></li><li><div>
                Parking partial patch
              </div></li><li><div>
                More WIP. With just a few test failures, but no upgrade or conditional optimization for existing data. This is with nulls last when DESC, but there's a problem with this - we'd need to include trailing nulls until the last DESC row key column and you wouldn't be able to add a new DESC row key column without mucking with the data (which is a showstopper).

I'm going to instead use a null separator with DESC for null values and otherwise a 0xFF. That way, nulls will sort first for ASC and DESC, but DESC sort order will work for all values.
              </div></li><li><div>
                Another WIP patch. All unit tests passing except one.
              </div></li><li><div>
                All unit tests passing.
              </div></li><li><div>
                Includes upgrade ability and warning message
              </div></li><li><div>
                With upgrade
              </div></li><li><div>
                Fix OrderedResultIterator for DESC variable length data
              </div></li><li><div>
                Working upgrade, but still needs some refinement.
              </div></li><li><div>
                Refined upgrade code
              </div></li><li><div>
                Working and tested patch
              </div></li><li><div>
                [~samarthjain] - please review. Here's an overview:
- No change in behavior for existing tables. Queries that have an ORDER BY for a variable length, descending row key will now sort correctly, but at the expense of forcing an ORDER BY (since they aren't sorted correctly in their natural order, we can't optimize out the ORDER BY).
- New tables (or indexes) will use the correct separator for DESC variable length row keys, so they won't be hit with the ORDER BY cost. See SchemaUtil.getSeparatorByte() for an overview of the logic to determine the separator byte.
- A new utility (psql.py -u option) is available to 1) display the physical tables affected by this bug, and 2) to optionally re-write them so that they sort correctly.
              </div></li><li><div>
                As far as testing the upgrade, I did a bunch of manual testing:
- Created table with descending, variable length row key, table without but with an index that has one, table without but with a view that has a local and shared index, multi-tenant tables against the same.
- Verified correct physical tables identified that need upgrading.
- Verified upgrade worked correctly for all of above
              </div></li><li><div>
                Fixed test failure due to this change for adding column to base table. Also disallowing VARBINARY DESC which can't really work correctly since we can't control what bytes are used and thus cannot guarantee the sort order is correct. A workaround for users would be to upsert using INVERT which is the same as what would occur today. Shorter but equal byte values would sort above longer equal byte values, though.
              </div></li><li><div>
                Addendum patch for handling DESC ARRAY. Still needs a few more tweaks, but parking here for now.
              </div></li><li><div><div><b>body:</b> [~Dumindux] and [~ram_krish] - would you guys mind reviewing this patch? This ensures that descending, variable length arrays sort correctly. The change is the use a 255 byte as the separator for non null values (including the terminators). See the couple of new tests in ArrayIT.

Much of the type changes are just formatting and moving a couple of duplicated methods where they belong at the base type. The other changes are to ensure we keep the same separator - for example if a table has not been upgraded, we need to keep using the 0 byte separator. That's where most of the complication comes in.

Much appreciated. [~Dumindux] - if you have time perhaps you can write a couple of lower level unit tests to confirm that my isRowKeyOrderOptimized works in all situations and that array_cat, prepend, and append work in that they maintain the same separator byte that the array is already using.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Went through the patch, on a high level
-&gt;We will rewrite the array bytes to use the new seperator byte if it is of type DESC.
-&gt; for the array_cat - if the existing array to which we will append a new array is of the old type we will coerce it to use the new sepeartor and the new array that we add should automatically use the new seperator (if the overall sort order is DESC) right?
-&gt;same with the prepend and append.
But one question regarding the other operations where we try to use the SEPERATOR_BYTE to find if we have reached the end of the array - in all such places we should not blindly check with SEPERTOR_BYTE right - instead try to decide it based on the order of the current byte[]?
              </div></li><li><div>
                Thanks for the review, [~ram_krish].

bq. should not blindly check with SEPERTOR_BYTE right - instead try to decide it based on the order of the current byte[]?

Luckily, for arrays we navigate using the offsets in the header, so this part didn't need to change. The separator bytes are purely to make sure variable length arrays sort correctly relative to each other.

I'll check in shortly unless you have other feedback?
              </div></li><li><div>
                SUCCESS: Integrated in Phoenix-master #834 (See [https://builds.apache.org/job/Phoenix-master/834/])
PHOENIX-2067 Sort order incorrect for variable length DESC columns (jtaylor: rev 2620a80c1e35c0d214f06a1b16e99da5415a1a2c)
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PUnsignedTinyintArray.java
* phoenix-core/src/main/java/org/apache/phoenix/exception/SQLExceptionCode.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PBooleanArray.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/DelegateTable.java
* phoenix-core/src/main/java/org/apache/phoenix/util/ScanUtil.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PUnsignedIntArray.java
* phoenix-core/src/main/java/org/apache/phoenix/coprocessor/MetaDataEndpointImpl.java
* phoenix-core/src/main/java/org/apache/phoenix/compile/OrderPreservingTracker.java
* phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
* phoenix-core/src/main/java/org/apache/phoenix/expression/util/regex/JONIPattern.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/stats/StatisticsUtil.java
* phoenix-core/src/main/java/org/apache/phoenix/iterate/OrderedResultIterator.java
* phoenix-core/src/test/java/org/apache/phoenix/compile/QueryCompilerTest.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PLongArray.java
* phoenix-core/src/main/java/org/apache/phoenix/util/SchemaUtil.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PTimeArray.java
* phoenix-core/src/main/java/org/apache/phoenix/compile/TupleProjectionCompiler.java
* phoenix-core/src/main/java/org/apache/phoenix/expression/RowValueConstructorExpression.java
* phoenix-core/src/main/java/org/apache/phoenix/iterate/BaseResultIterators.java
* phoenix-core/src/main/java/org/apache/phoenix/coprocessor/generated/PTableProtos.java
* phoenix-core/src/main/java/org/apache/phoenix/util/MetaDataUtil.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PFloatArray.java
* phoenix-core/src/main/java/org/apache/phoenix/util/PhoenixRuntime.java
* phoenix-core/src/main/java/org/apache/phoenix/compile/ScanRanges.java
* phoenix-core/src/it/java/org/apache/phoenix/end2end/LpadFunctionIT.java
* phoenix-core/src/it/java/org/apache/phoenix/end2end/ReverseScanIT.java
* phoenix-core/src/it/java/org/apache/phoenix/end2end/SortOrderIT.java
* phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/RowKeyValueAccessor.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PArrayDataType.java
* phoenix-core/src/it/java/org/apache/phoenix/end2end/RowValueConstructorIT.java
* phoenix-core/src/main/java/org/apache/phoenix/util/TupleUtil.java
* phoenix-core/src/main/java/org/apache/phoenix/util/UpgradeUtil.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PDoubleArray.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/PTable.java
* phoenix-core/src/main/java/org/apache/phoenix/util/ByteUtil.java
* phoenix-core/src/main/java/org/apache/phoenix/compile/FromCompiler.java
* phoenix-core/src/main/java/org/apache/phoenix/filter/SkipScanFilter.java
* phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java
* phoenix-core/src/main/java/org/apache/phoenix/expression/function/LpadFunction.java
* phoenix-core/src/main/java/org/apache/phoenix/jdbc/PhoenixConnection.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/RowKeySchema.java
* phoenix-core/src/main/java/org/apache/phoenix/expression/ArrayConstructorExpression.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PUnsignedSmallintArray.java
* phoenix-core/src/main/java/org/apache/phoenix/compile/UpsertCompiler.java
* phoenix-core/src/it/java/org/apache/phoenix/end2end/ArrayIT.java
* phoenix-protocol/src/main/PTable.proto
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PUnsignedDoubleArray.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PVarbinaryArray.java
* phoenix-core/src/main/java/org/apache/phoenix/expression/function/ArrayModifierFunction.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PUnsignedDateArray.java
* phoenix-core/src/main/java/org/apache/phoenix/compile/UnionCompiler.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PUnsignedTimestampArray.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PTimestampArray.java
* phoenix-core/src/main/java/org/apache/phoenix/expression/function/ArrayConcatFunction.java
* phoenix-core/src/main/java/org/apache/phoenix/query/KeyRange.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PDecimalArray.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PIntegerArray.java
* phoenix-core/src/main/java/org/apache/phoenix/compile/WhereOptimizer.java
* phoenix-core/src/it/java/org/apache/phoenix/end2end/IsNullIT.java
* phoenix-core/src/main/java/org/apache/phoenix/execute/DescVarLengthFastByteComparisons.java
* phoenix-core/src/test/java/org/apache/phoenix/query/OrderByTest.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PCharArray.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PDataType.java
* phoenix-core/src/main/java/org/apache/phoenix/compile/OrderByCompiler.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PUnsignedTimeArray.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PVarcharArray.java
* phoenix-core/src/main/java/org/apache/phoenix/coprocessor/BaseScannerRegionObserver.java
* dev/eclipse_prefs_phoenix.epf
* phoenix-core/src/main/java/org/apache/phoenix/schema/ValueSchema.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PBinaryArray.java
* phoenix-core/src/it/java/org/apache/phoenix/end2end/SortOrderFIT.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PUnsignedLongArray.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PDateArray.java
* phoenix-core/src/main/java/org/apache/phoenix/execute/BaseQueryPlan.java
* phoenix-core/src/main/java/org/apache/phoenix/query/QueryConstants.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/PTableImpl.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PUnsignedFloatArray.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PTinyintArray.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PSmallintArray.java
* phoenix-core/src/main/java/org/apache/phoenix/coprocessor/UngroupedAggregateRegionObserver.java
* phoenix-core/src/main/java/org/apache/phoenix/expression/OrderByExpression.java
* phoenix-core/src/test/java/org/apache/phoenix/schema/types/PDataTypeTest.java

              </div></li><li><div>
                SUCCESS: Integrated in Phoenix-master #835 (See [https://builds.apache.org/job/Phoenix-master/835/])
PHOENIX-2067 Sort order incorrect for variable length DESC columns (jtaylor: rev 4b99c632c5e40251451e69fbe6d108f51e549e9e)
* phoenix-core/src/main/java/org/apache/phoenix/util/UpgradeUtil.java

              </div></li><li><div>
                [~jamestaylor] sorry for the late response. Here are some tests for the built-ins. I have found a small issue. As we are converting double separator bytes at the end also to new separator bytes for DESC arrays, there was a problem in counting trailing nulls in first array(in ARRAY_CAT). I changed the code to consider the new separator byte also. 
              </div></li><li><div>
                As [~ram_krish] suggested I looked at the other parts of the code also. The trailing null issue was in two other places. In array deserialization and positionAtArrayElement method. This patch has two tests which demonstrate the cases and the fix.
              </div></li><li><div>
                +1. Please commit to 4.x and master branch, [~ram_krish] (and you're welcome to review as well too, of course).
              </div></li><li><div>
                +1 . Nice tests. Thanks for covering up all the cases [~Dumindux].
[~giacomotaylor]
I was trying to suggest this point only in my review. 
              </div></li><li><div>
                SUCCESS: Integrated in Phoenix-master #841 (See [https://builds.apache.org/job/Phoenix-master/841/])
PHOENIX-2067 Sort order incorrect for variable length DESC columns - ARRAY (ramkrishna: rev 33d60506c5f2d4408a1df79f278d7a45d3401a27)
* phoenix-core/src/test/java/org/apache/phoenix/expression/ArrayAppendFunctionTest.java
* phoenix-core/src/main/java/org/apache/phoenix/schema/types/PArrayDataType.java
* phoenix-core/src/test/java/org/apache/phoenix/schema/types/PDataTypeForArraysTest.java
* phoenix-core/src/test/java/org/apache/phoenix/expression/ArrayPrependFunctionTest.java
* phoenix-core/src/test/java/org/apache/phoenix/expression/ArrayConcatFunctionTest.java

              </div></li><li><div>
                [~jamestaylor] Here is the backport patch for 4.4.1. Most of the patch applied manually. I ran all the tests and they have passed. 
Can you please review it. Will commit if it's ok.

Thanks.
              </div></li><li><div>
                Patch looks good, [~rajeshbabu]. Thanks for back porting. One thing that needs to be manually tested is converting old tables to the correct row key using "psql.py -u localhost". You can wait to do this until after back porting PHOENIX-2171 so you only have to do it once. You'd want to create various tables with DESC row keys in pre 4.4.1 and then run the script to ensure that the conversion works correctly and you get the expected query plan post conversion. 
              </div></li><li><div>
                Bulk close of all issues that has been resolved in a released version. 
              </div></li><li><div>
                I think this patch broke PK constraints for NOT NULL, non-fixed-width VARCHAR columns.

{noformat}
            // If some non null pk values aren't set, then throw
            if (i &lt; nColumns) {
                PColumn column = columns.get(i);
                if (column.getDataType().isFixedWidth() || !column.isNullable()) {
                    throw new ConstraintViolationException(name.getString() + "." + column.getName().getString() + " may not be null");
                }
            }
{noformat}

The constraint we're managing is that of non-null for PK columns. For VARCHAR, fixed-width meets this criteria because there's no available representation for NULL. I assume variable-length columns can represent null (though I don't know the encoding details off the top of my head), so we must look for the `NOT NULL` constraint added in schema.

I think the if condition should be
{noformat}
if (!column.getDataType.isFixedWidth() || column.isNullable()) { throw... }
{noformat}

Also, can this be folded into a single {{column.isNullable()}} -- shouldn't that method check the datatype on the caller's behalf? Or is there a scenario where we want to know what additional constraints the schema defined vs. what the datatype offers?
              </div></li><li><div>
                Nope, this patch didn't change the logic on that line: https://github.com/apache/phoenix/commit/2620a80c1e35c0d214f06a1b16e99da5415a1a2c#diff-d87ee86bba434603ba73b6a85a139529
              </div></li></ol></div></div></html>