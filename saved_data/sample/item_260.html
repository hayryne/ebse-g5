<!DOCTYPE html><html><div class="item-title">
        Item 260
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                *
   * Get instance of HiveMaterializedViewsRegistry.
   *
   * @return the singleton
   
              </div></li><li><div>
                 Build Druid query
              </div></li><li><div>
                *
   * Adds the materialized view to the cache.
   *
   * @param materializedViewTable the materialized view
   
              </div></li><li><div>
                 Add to cache
              </div></li><li><div>
                 Key is the database name. Value a map from a unique identifier for the view comprising
   * the qualified name and the creation time, to the view object.
   * Since currently we cannot alter a materialized view, that should suffice to identify
   * whether the cached view is up to date or not.
   * Creation time is useful to ensure correctness in case multiple HS2 instances are used. 
              </div></li><li><div>
                 Build Hive Table Scan Rel
              </div></li><li><div>
                 We could not parse the view
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
   * Returns the materialized views in the cache for the given database.
   *
   * @param dbName the database
   * @return the collection of materialized views, or the empty collection if none
   
              </div></li><li><div>
                 1.1 Add Column info for non partion cols (Object Inspector fields)
              </div></li><li><div>
                 timestamp
              </div></li><li><div>
                 2. Build RelOptAbstractTable
              </div></li><li><div>
                 Bail out
              </div></li><li><div>
                *
   * Initialize the registry for the given database. It will extract the materialized views
   * that are enabled for rewriting from the metastore for the current user, parse them,
   * and register them in this cache.
   *
   * The loading process runs on the background; the method returns in the moment that the
   * runnable task is created, thus the views will still not be loaded in the cache when
   * it does.
   
              </div></li><li><div><div><b>comment:</b>  TODO: We should enhance metastore API such that it returns only
 materialized views instead of all tables
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                 1.3 Build row type from field &lt;type, name&gt;
              </div></li><li><div>
                *
   * Removes the materialized view from the cache.
   *
   * @param materializedViewTable the materialized view to remove
   
              </div></li><li><div>
                 3. Build operator
              </div></li><li><div>
                * 
 * Registry for materialized views. The goal of this cache is to avoid parsing and creating
 * logical plans for the materialized views at query runtime. When a query arrives, we will
 * just need to consult this cache and extract the logical plans for the views (which had
 * already been parsed) from it.
 
              </div></li><li><div>
                 dimension
              </div></li><li><div>
                 1.2 Add column info corresponding to partition columns
              </div></li><li><div>
                 Bail out if it already exists
              </div></li><li><div>
                 0. Recreate cluster
              </div></li><li><div>
                 1. Create column schema
              </div></li><li><div>
                 Singleton 
              </div></li><li><div>
                 Bail out if it is not enabled for rewriting
              </div></li><li><div>
                 Costing is done in transformTo(), so we call it repeatedly with all applicable
 materialized views and cheapest one will be picked
              </div></li><li><div>
                ~ Methods ----------------------------------------------------------------
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                ~ Constructors -----------------------------------------------------------
              </div></li><li><div>
                *
 * Planner rule that replaces (if possible)
 * a {@link org.apache.calcite.rel.core.Project}
 * on a {@link org.apache.calcite.rel.core.Filter}
 * on a {@link org.apache.calcite.rel.core.TableScan}
 * to use a Materialized View.
 
              </div></li><li><div>
                * Creates a HiveMaterializedViewFilterScanRule. 
              </div></li><li><div>
                *
 * Extension to {@link SubstitutionVisitor}.
 *
 * TODO: Remove when we upgrade to Calcite version using builders.
 
              </div></li><li><div>
                *
   * Implementation of {@link SubstitutionVisitor.UnifyRule} that matches a
   * {@link SubstitutionVisitor.MutableFilter} to a
   * {@link SubstitutionVisitor.MutableFilter} where the condition of the target
   * relation is weaker.
   *
   * &lt;p&gt;Example: target has a weaker condition&lt;/p&gt;
   * &lt;ul&gt;
   * &lt;li&gt;query:   Filter(condition: &amp;gt;($1, 20))
   *                Scan(table: [hr, emps])&lt;/li&gt;
   * &lt;li&gt;target:  Filter(condition: &amp;gt;($1, 10))
   *                Scan(table: [hr, emps])&lt;/li&gt;
   * &lt;/ul&gt;
   
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
   * Implementation of {@link SubstitutionVisitor.UnifyRule} that matches a
   * {@link SubstitutionVisitor.MutableProject} to a
   * {@link SubstitutionVisitor.MutableProject} where the condition of the target
   * relation is weaker.
   *
   * &lt;p&gt;Example: target has a weaker condition and contains all columns selected
   * by query&lt;/p&gt;
   * &lt;ul&gt;
   * &lt;li&gt;query:   Project(projects: [$2, $0])
   *                Filter(condition: &amp;gt;($1, 20))
   *                  Scan(table: [hr, emps])&lt;/li&gt;
   * &lt;li&gt;target:  Project(projects: [$0, $1, $2])
   *                Filter(condition: &amp;gt;($1, 10))
   *                  Scan(table: [hr, emps])&lt;/li&gt;
   * &lt;/ul&gt;
   
              </div></li><li><div>
                *
   * Implementation of {@link SubstitutionVisitor.UnifyRule} that matches a
   * {@link SubstitutionVisitor.MutableFilter} to a
   * {@link SubstitutionVisitor.MutableProject} on top of a
   * {@link SubstitutionVisitor.MutableFilter} where the condition of the target
   * relation is weaker.
   *
   * &lt;p&gt;Example: target has a weaker condition and is a permutation projection of
   * its child relation&lt;/p&gt;
   * &lt;ul&gt;
   * &lt;li&gt;query:   Filter(condition: &amp;gt;($1, 20))
   *                Scan(table: [hr, emps])&lt;/li&gt;
   * &lt;li&gt;target:  Project(projects: [$1, $0, $2, $3, $4])
   *                Filter(condition: &amp;gt;($1, 10))
   *                  Scan(table: [hr, emps])&lt;/li&gt;
   * &lt;/ul&gt;
   
              </div></li><li><div>
                 End MaterializedViewSubstitutionVisitor.java
              </div></li><li><div>
                 A real substitution happens. We purge the attempted
 replacement list and add them into substitution list.
 Meanwhile we stop matching the descendants and jump
 to the next subtree in pre-order traversal.
              </div></li><li><div>
                 First, try splitting into ORs.
 Given target    c1 OR c2 OR c3 OR c4
 and condition   c2 OR c4
 residue is      NOT c1 AND NOT c3
 Also deals with case target [x] condition [x] yields residue [true].
              </div></li><li><div><div><b>comment:</b>  TODO: make sure that constants are ok
                </div><div><b>label:</b> requirement
                </div></div></li><li><div>
                 no children - nothing to do
              </div></li><li><div>
                ~ Instance fields --------------------------------------------------------
              </div></li><li><div>
                 Target is coarser level of aggregation. Generate an aggregate.
              </div></li><li><div>
                 Populate the list of leaves in the tree under "target".
 Leaves are all nodes that are not parents.
 For determinism, it is important that the list is in scan order.
              </div></li><li><div>
                * Abstract base class for implementing {@link UnifyRule}. 
              </div></li><li><div>
                 Optimize if there are 0 or 1 slots.
              </div></li><li><div><div><b>comment:</b>  Quit the entire loop if:
 1) we have walked the entire query tree with one or more successful
    substitutions, thus count != 0 &amp;&amp; attempted.isEmpty();
 2) we have walked the entire query tree but have made no replacement
    attempt, thus count == 0 &amp;&amp; attempted.isEmpty();
 3) we had done some replacement attempt in a previous walk, but in
    this one we have not found any potential matches or substitutions,
    thus count == 0 &amp;&amp; !attempted.isEmpty().
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                * Implementation of {@link UnifyRule} that matches if the query is already
   * equal to the target.
   *
   * &lt;p&gt;Matches scans to the same table, because these will be
   * {@link MutableScan}s with the same
   * {@link org.apache.calcite.rel.logical.LogicalTableScan} instance.&lt;/p&gt;
   
              </div></li><li><div>
                 Replace previous equivalents with new equivalents, higher up
 the tree.
              </div></li><li><div>
                 equivalent to "query", contains "result"
              </div></li><li><div>
                * Creates an operand that doesn't check inputs. 
              </div></li><li><div>
                *
     * &lt;p&gt;Applies this rule to a particular node in a query. The goal is
     * to convert {@code query} into {@code target}. Before the rule is
     * invoked, Calcite has made sure that query's children are equivalent
     * to target's children.
     *
     * &lt;p&gt;There are 3 possible outcomes:&lt;/p&gt;
     *
     * &lt;ul&gt;
     *
     * &lt;li&gt;{@code query} already exactly matches {@code target}; returns
     * {@code target}&lt;/li&gt;
     *
     * &lt;li&gt;{@code query} is sufficiently close to a match for
     * {@code target}; returns {@code target}&lt;/li&gt;
     *
     * &lt;li&gt;{@code query} cannot be made to match {@code target}; returns
     * null&lt;/li&gt;
     *
     * &lt;/ul&gt;
     *
     * &lt;p&gt;REVIEW: Is possible that we match query PLUS one or more of its
     * ancestors?&lt;/p&gt;
     *
     * @param call Input parameters
     
              </div></li><li><div>
                * Utilities for dealing with {@link MutableRel}s. 
              </div></li><li><div>
                * Operand to a {@link UnifyRule} that matches a relational expression of a
   * given type. It has zero or more child operands. 
              </div></li><li><div>
                * Mutable equivalent of
   * {@link org.apache.calcite.rel.logical.LogicalFilter}. 
              </div></li><li><div>
                * Mutable equivalent of {@link SingleRel}. 
              </div></li><li><div>
                * Implementation of {@link UnifyRule} that matches a
   * {@link MutableAggregate} on
   * a {@link MutableProject} query to an {@link MutableAggregate} target.
   *
   * &lt;p&gt;The rule is necessary when we unify query=Aggregate(x) with
   * target=Aggregate(x, y). Query will tend to have an extra Project(x) on its
   * input, which this rule knows is safe to ignore.&lt;/p&gt; 
              </div></li><li><div>
                * Abstract base class for implementations of {@link MutableRel} that have
   * no inputs. 
              </div></li><li><div>
                * Creates a SubstitutionVisitor. 
              </div></li><li><div>
                * Visitor over {@link MutableRel}. 
              </div></li><li><div>
                *
   * Returns whether a boolean expression ever returns true.
   *
   * &lt;p&gt;This method may give false positives. For instance, it will say
   * that {@code x = 5 AND x &gt; 10} is satisfiable, because at present it
   * cannot prove that it is not.&lt;/p&gt;
   
              </div></li><li><div>
                * Workspace while rule is being matched.
   * Careful, re-entrant!
   * Assumes no rule needs more than 2 slots. 
              </div></li><li><div>
                * Creates an operand with given inputs. 
              </div></li><li><div>
                 End SubstitutionVisitor.java
              </div></li><li><div>
                 in.query can be rewritten in terms of in.target if its condition
 is weaker. For example:
   query: SELECT * FROM t WHERE x = 1 AND y = 2
   target: SELECT * FROM t WHERE x = 1
 transforms to
   result: SELECT * FROM (target) WHERE y = 2
              </div></li><li><div>
                *
   * Nodes in {@link #query} that have no children.
   
              </div></li><li><div>
                
        if (child.parent != null) {
          child.parent.setInput(child.ordinalInParent, null);
          child.parent = null;
        }

              </div></li><li><div>
                 Populate "equivalents" with (q, t) for each query descendant q and
 target descendant t that are equal.
              </div></li><li><div>
                *
   * Arguments to an application of a {@link UnifyRule}.
   
              </div></li><li><div>
                *
   * Maps a condition onto a target.
   *
   * &lt;p&gt;If condition is stronger than target, returns the residue.
   * If it is equal to target, returns the expression that evaluates to
   * the constant {@code true}. If it is weaker than target, returns
   * {@code null}.&lt;/p&gt;
   *
   * &lt;p&gt;The terms satisfy the relation&lt;/p&gt;
   *
   * &lt;pre&gt;
   *     {@code condition = target AND residue}
   * &lt;/pre&gt;
   *
   * &lt;p&gt;and {@code residue} must be as weak as possible.&lt;/p&gt;
   *
   * &lt;p&gt;Example #1: condition stronger than target&lt;/p&gt;
   * &lt;ul&gt;
   * &lt;li&gt;condition: x = 1 AND y = 2&lt;/li&gt;
   * &lt;li&gt;target: x = 1&lt;/li&gt;
   * &lt;li&gt;residue: y = 2&lt;/li&gt;
   * &lt;/ul&gt;
   *
   * &lt;p&gt;Note that residue {@code x &amp;gt; 0 AND y = 2} would also satisfy the
   * relation {@code condition = target AND residue} but is stronger than
   * necessary, so we prefer {@code y = 2}.&lt;/p&gt;
   *
   * &lt;p&gt;Example #2: target weaker than condition (valid, but not currently
   * implemented)&lt;/p&gt;
   * &lt;ul&gt;
   * &lt;li&gt;condition: x = 1&lt;/li&gt;
   * &lt;li&gt;target: x = 1 OR z = 3&lt;/li&gt;
   * &lt;li&gt;residue: NOT (z = 3)&lt;/li&gt;
   * &lt;/ul&gt;
   *
   * &lt;p&gt;Example #3: condition and target are equivalent&lt;/p&gt;
   * &lt;ul&gt;
   * &lt;li&gt;condition: x = 1 AND y = 2&lt;/li&gt;
   * &lt;li&gt;target: y = 2 AND x = 1&lt;/li&gt;
   * &lt;li&gt;residue: TRUE&lt;/li&gt;
   * &lt;/ul&gt;
   *
   * &lt;p&gt;Example #4: condition weaker than target&lt;/p&gt;
   * &lt;ul&gt;
   * &lt;li&gt;condition: x = 1&lt;/li&gt;
   * &lt;li&gt;target: x = 1 AND y = 2&lt;/li&gt;
   * &lt;li&gt;residue: null (i.e. no match)&lt;/li&gt;
   * &lt;/ul&gt;
   *
   * &lt;p&gt;There are many other possible examples. It amounts to solving
   * whether {@code condition AND NOT target} can ever evaluate to
   * true, and therefore is a form of the NP-complete
   * &lt;a href="http://en.wikipedia.org/wiki/Satisfiability"&gt;Satisfiability&lt;/a&gt;
   * problem.&lt;/p&gt;
   
              </div></li><li><div>
                *
   * Returns a list of all possible rels that result from substituting the
   * matched RelNode with the replacement RelNode within the query.
   *
   * &lt;p&gt;For example, the substitution result of A join B, while A and B
   * are both a qualified match for replacement R, is R join B, R join R,
   * A join R.
   
              </div></li><li><div>
                * Exception thrown to exit a matcher. Not really an error. 
              </div></li><li><div>
                 nothing to do
              </div></li><li><div>
                * Equivalence that compares {@link Lists}s by the
   * {@link Object#toString()} of their elements. 
              </div></li><li><div>
                *
   * Substitutes the query with replacement whenever possible but meanwhile
   * keeps track of all the substitutions and their original rel before
   * replacement, so that in later processing stage, the replacement can be
   * recovered individually to produce a list of all possible rels with
   * substitution in different places.
   
              </div></li><li><div>
                * Creates an operand that matches a relational expression in the
     * target. 
              </div></li><li><div>
                * Visitor that prints an indented tree of {@link MutableRel}s. 
              </div></li><li><div>
                * Mutable equivalent of
   * {@link org.apache.calcite.rel.logical.LogicalProject}. 
              </div></li><li><div>
                noinspection unchecked
              </div></li><li><div>
                 Child of projectTarget is equivalent to child of filterQuery.
              </div></li><li><div>
                 Same level of aggregation. Generate a project.
              </div></li><li><div>
                * Implementation of {@link MutableRel} whose only purpose is to have a
   * child. Used as the root of a tree. 
              </div></li><li><div>
                 We had done some replacement attempt in the previous walk, but that
 did not lead to any substitutions in this walk, so we need to recover
 the replacement.
              </div></li><li><div><div><b>comment:</b>  We do not need to check query's parent type to avoid duplication
 of ProjectToProjectUnifyRule or FilterToProjectUnifyRule, since
 SubstitutionVisitor performs a top-down match.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                * Replaces this {@code MutableRel} in its parent with another node at the
     * same position.
     *
     * &lt;p&gt;Before the method, {@code child} must be an orphan (have null parent)
     * and after this method, this {@code MutableRel} is an orphan.
     *
     * @return The parent
     
              </div></li><li><div>
                          ProjectToFilterUnifyRule.INSTANCE,
          FilterToFilterUnifyRule.INSTANCE,
              </div></li><li><div>
                * Implementation of {@link UnifyRule} that matches
   * {@link org.apache.calcite.rel.logical.LogicalProject}. 
              </div></li><li><div>
                 CHECKSTYLE: IGNORE 1
              </div></li><li><div>
                * Implementation of {@link UnifyRule} that matches a {@link MutableProject}
   * to a {@link MutableFilter}. 
              </div></li><li><div>
                * Implementation of {@link UnifyRule} that matches
   * {@link org.apache.calcite.rel.logical.LogicalTableScan}. 
              </div></li><li><div>
                *
   * Factory for a builder for relational expressions.
   * &lt;p&gt;The actual builder is available via {@link RelOptRuleCall#builder()}.
   
              </div></li><li><div>
                 not called
              </div></li><li><div>
                * Implementation of {@link UnifyRule} that matches a
   * {@link org.apache.calcite.rel.logical.LogicalAggregate} to a
   * {@link org.apache.calcite.rel.logical.LogicalAggregate}, provided
   * that they have the same child. 
              </div></li><li><div>
                 If one of the not-disjunctions is a disjunction that is wholly
 contained in the disjunctions list, the expression is not
 satisfiable.

 Example #1. x AND y AND z AND NOT (x AND y)  - not satisfiable
 Example #2. x AND y AND NOT (x AND y)        - not satisfiable
 Example #3. x AND y AND NOT (x AND y AND z)  - may be satisfiable
              </div></li><li><div>
                *
   * Nodes in {@link #target} that have no children.
   
              </div></li><li><div>
                * Helper for {@link #replace}. 
              </div></li><li><div>
                *
 * Substitutes part of a tree of relational expressions with another tree.
 *
 * &lt;p&gt;The call {@code new SubstitutionVisitor(target, query).go(replacement))}
 * will return {@code query} with every occurrence of {@code target} replaced
 * by {@code replacement}.&lt;/p&gt;
 *
 * &lt;p&gt;The following example shows how {@code SubstitutionVisitor} can be used
 * for materialized view recognition.&lt;/p&gt;
 *
 * &lt;ul&gt;
 * &lt;li&gt;query = SELECT a, c FROM t WHERE x = 5 AND b = 4&lt;/li&gt;
 * &lt;li&gt;target = SELECT a, b, c FROM t WHERE x = 5&lt;/li&gt;
 * &lt;li&gt;replacement = SELECT * FROM mv&lt;/li&gt;
 * &lt;li&gt;result = SELECT a, c FROM mv WHERE b = 4&lt;/li&gt;
 * &lt;/ul&gt;
 *
 * &lt;p&gt;Note that {@code result} uses the materialized view table {@code mv} and a
 * simplified condition {@code b = 4}.&lt;/p&gt;
 *
 * &lt;p&gt;Uses a bottom-up matching algorithm. Nodes do not need to be identical.
 * At each level, returns the residue.&lt;/p&gt;
 *
 * &lt;p&gt;The inputs must only include the core relational operators:
 * {@link org.apache.calcite.rel.logical.LogicalTableScan},
 * {@link org.apache.calcite.rel.logical.LogicalFilter},
 * {@link org.apache.calcite.rel.logical.LogicalProject},
 * {@link org.apache.calcite.rel.logical.LogicalJoin},
 * {@link org.apache.calcite.rel.logical.LogicalUnion},
 * {@link org.apache.calcite.rel.logical.LogicalAggregate}.&lt;/p&gt;
 *
 * TODO: Remove when we upgrade to Calcite version using builders.
 
              </div></li><li><div>
                * Mutable equivalent of {@link RelNode}.
   *
   * &lt;p&gt;Each node has mutable state, and keeps track of its parent and position
   * within parent.
   * It doesn't make sense to canonize {@code MutableRels},
   * otherwise one node could end up with multiple parents.
   * It follows that {@code #hashCode} and {@code #equals} are less efficient
   * than their {@code RelNode} counterparts.
   * But, you don't need to copy a {@code MutableRel} in order to change it.
   * For this reason, you should use {@code MutableRel} for short-lived
   * operations, and transcribe back to {@code RelNode} when you are done.&lt;/p&gt;
   
              </div></li><li><div>
                * Based on
     * {@link org.apache.calcite.rel.rules.ProjectRemoveRule#strip}. 
              </div></li><li><div>
                * Operand that assigns a particular relational expression to a variable.
   *
   * &lt;p&gt;It is applied to a descendant of the query, writes the operand into the
   * slots array, and always matches.
   * There is a corresponding operand of type {@link TargetOperand} that checks
   * whether its relational expression, a descendant of the target, is
   * equivalent to this {@code QueryOperand}'s relational expression.
   
              </div></li><li><div>
                * Base class for set-operations. 
              </div></li><li><div>
                 replacementMap is always empty
 result =
              </div></li><li><div>
                 Example:
  e: x = 1 AND y = 2 AND z = 3 AND NOT (x = 1 AND y = 2)
  disjunctions: {x = 1, y = 2, z = 3}
  notDisjunctions: {x = 1 AND y = 2}
              </div></li><li><div>
                 replaceAncestors(node0);
              </div></li><li><div>
                * Implementation of {@link UnifyRule} that matches a {@link MutableFilter}
   * to a {@link MutableProject}. 
              </div></li><li><div>
                * Visitor that counts how many {@link QueryOperand} and
   * {@link TargetOperand} in an operand tree. 
              </div></li><li><div>
                 in.query can be rewritten in terms of in.target if its groupSet is
 a subset, and its aggCalls are a superset. For example:
   query: SELECT x, COUNT(b) FROM t GROUP BY x
   target: SELECT x, y, SUM(a) AS s, COUNT(b) AS cb FROM t GROUP BY x, y
 transforms to
   result: SELECT x, SUM(cb) FROM (target) GROUP BY x
              </div></li><li><div>
                * Operand to a {@link UnifyRule}. 
              </div></li><li><div>
                * Mutable equivalent of
   * {@link org.apache.calcite.rel.logical.LogicalUnion}. 
              </div></li><li><div>
                * Mutable equivalent of {@link org.apache.calcite.rel.core.Values}. 
              </div></li><li><div>
                 If there is a single FALSE or NOT TRUE, the whole expression is
 always false.
              </div></li><li><div><div><b>comment:</b>  Short-cut common case.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                * Operand to a {@link UnifyRule} that matches a relational expression of a
   * given type. 
              </div></li><li><div>
                * Creates a SubstitutionVisitor with the default rule set. 
              </div></li><li><div>
                * Equivalent to
     * {@link RelOptUtil#createProject(org.apache.calcite.rel.RelNode, java.util.List)}
     * for {@link MutableRel}. 
              </div></li><li><div>
                * Equivalence to {@link org.apache.calcite.plan.RelOptUtil#createCastRel}
     * for {@link MutableRel}. 
              </div></li><li><div>
                * Builds a shuttle that stores a list of expressions, and can map incoming
   * expressions to references to them. 
              </div></li><li><div>
                 nothing
              </div></li><li><div>
                * Operand that checks that a relational expression matches the corresponding
   * relational expression that was passed to a {@link QueryOperand}. 
              </div></li><li><div>
                *
     * Creates a {@link UnifyRuleCall} based on the parent of {@code query}.
     
              </div></li><li><div>
                *
   * Represents a replacement action: before &amp;rarr; after.
   
              </div></li><li><div>
                * Creates an operand that matches a relational expression in the query. 
              </div></li><li><div>
                * Implementation of {@link UnifyRule} that matches a
   * {@link MutableFilter}. 
              </div></li><li><div>
                * Based on
     * {@link org.apache.calcite.rel.rules.ProjectRemoveRule#isTrivial(org.apache.calcite.rel.core.Project)}. 
              </div></li><li><div>
                * Returns if one rel is weaker than another. 
              </div></li><li><div>
                *
   * Rule that converts a {@link org.apache.calcite.rel.logical.LogicalFilter}
   * on top of a {@link org.apache.calcite.rel.logical.LogicalProject} into a
   * trivial filter (on a boolean column).
   
              </div></li><li><div>
                 We will try walking the query tree all over again to see
 if there can be any substitutions after the replacement
 attempt.
              </div></li><li><div>
                *
     * Values must be of enumeration {@link JoinRelType}, except that
     * {@link JoinRelType#RIGHT} is disallowed.
     
              </div></li><li><div>
                * Within a relational expression {@code query}, replaces occurrences of
     * {@code find} with {@code replace}.
     *
     * &lt;p&gt;Assumes relational expressions (and their descendants) are not null.
     * Does not handle cycles. 
              </div></li><li><div>
                * Type of {@code MutableRel}. 
              </div></li><li><div>
                * Equivalent to
     * {@link RelOptUtil#createProject(org.apache.calcite.rel.RelNode, java.util.List, java.util.List)}
     * for {@link MutableRel}. 
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div><div><b>comment:</b>  This node has been replaced by previous iterations in the
 hope to match its ancestors, so the node itself should not
 be matched again.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                * Base Class for relations with two inputs 
              </div></li><li><div>
                *
   * Result of an application of a {@link UnifyRule} indicating that the
   * rule successfully matched {@code query} against {@code target} and
   * generated a {@code result} that is equivalent to {@code query} and
   * contains {@code target}.
   
              </div></li><li><div>
                noinspection AssertWithSideEffects
              </div></li><li><div>
                 Could not map query onto target.
              </div></li><li><div>
                * Mutable equivalent of
   * {@link org.apache.calcite.rel.logical.LogicalJoin}. 
              </div></li><li><div>
                * Mutable equivalent of
   * {@link org.apache.calcite.rel.logical.LogicalTableScan}. 
              </div></li><li><div>
                * Rule that attempts to match a query relational expression
   * against a target relational expression.
   *
   * &lt;p&gt;The rule declares the query and target types; this allows the
   * engine to fire only a few rules in a given context.&lt;/p&gt;
   
              </div></li><li><div>
                * Returns whether two relational expressions have the same row-type. 
              </div></li><li><div>
                * Returns a list of (expression, name) pairs. 
              </div></li><li><div>
                * Equivalence that compares objects by their {@link Object#toString()}
   * method. 
              </div></li><li><div>
                * Mutable equivalent of
   * {@link org.apache.calcite.rel.logical.LogicalAggregate}. 
              </div></li><li><div>
                * Mutable equivalent of {@link org.apache.calcite.rel.core.Sort}. 
              </div></li><li><div>
                 materialized views
              </div></li><li><div>
                *
       * &lt;code&gt;optional bool is_rewrite_enabled = 15;&lt;/code&gt;
       
              </div></li><li><div>
                 optional bool is_rewrite_enabled = 15;
              </div></li><li><div>
                *
     * &lt;code&gt;optional bool is_rewrite_enabled = 15;&lt;/code&gt;
     
              </div></li><li><div>
                 optional
              </div></li><li><div>
                * Returns true if field rewriteEnabled is set (has been assigned a value) and false otherwise 
              </div></li><li><div>
                 REWRITE_ENABLED
              </div></li><li><div>
                *
   * @param rewriteEnabled whether the view can be used for rewriting queries
   
              </div></li><li><div>
                *
   * @return whether the view can be used for rewriting queries
   
              </div></li><li><div>
                 Remove from cache if it is a materialized view
              </div></li><li><div>
                 Add to cache if it is a materialized view
              </div></li><li><div>
                 Cached views (includes all)
              </div></li><li><div>
                *
   * Get the materialized views that have been enabled for rewriting from the
   * metastore. If the materialized view is in the cache, we do not need to
   * parse it to generate a logical plan for the rewriting. Instead, we
   * return the version present in the cache.
   *
   * @return the list of materialized views available for rewriting
   * @throws HiveException
   
              </div></li><li><div>
                 Add to final result set
              </div></li><li><div>
                 Compose qualified name
              </div></li><li><div>
                *
   * Get all tables for the specified database.
   * @param dbName
   * @return List of table names
   * @throws HiveException
   
              </div></li><li><div>
                 Final result
              </div></li><li><div>
                 From metastore (for security)
              </div></li><li><div>
                 Bail out: empty list
              </div></li><li><div>
                *
   * @return whether this view can be used for rewriting queries
   
              </div></li><li><div>
                *
   * @param rewriteEnabled
   *          whether this view can be used for rewriting queries
   
              </div></li><li><div>
                 5. Materialized view based rewriting
 We disable it for CTAS and MV creation queries (trying to avoid any problem
 due to data freshness)
              </div></li><li><div>
                 7.convert Join + GBy to semijoin
 8. Run rule to fix windowing issue when it is done over
 9. Apply Druid transformation rules
 10. Run rules to aid in translation from Calcite tree to Hive tree
 10.1. Merge join into multijoin operators (if possible)
              </div></li><li><div>
                 6. Run aggregate-join transpose (cost based)
              </div></li><li><div>
                 Add views to planner
              </div></li><li><div>
                *
   * Get Optimized AST for the given QB tree in the semAnalyzer.
   *
   * @return Optimized operator tree translated in to Hive AST
   * @throws SemanticException
   
              </div></li><li><div>
                 Restore default cost model
              </div></li><li><div>
                 Optimize plan
              </div></li><li><div>
                 Remove view-based rewriting rules from planner
              </div></li><li><div>
                 We need to use the current cluster for the scan operator on views,
 otherwise the planner will throw an Exception (different planners)
              </div></li><li><div>
                 10.2.  Introduce exchange operators below join/multijoin operators
              </div></li><li><div>
                *
   * This method is useful if we want to obtain the logical plan after being parsed and
   * optimized by Calcite.
   *
   * @return the Calcite plan for the query, null if it could not be generated
   
              </div></li><li><div>
                 Use Calcite cost model for view rewriting
              </div></li><li><div>
                 nodeOfInterest is the query
              </div></li><li><div>
                 Add view-based rewriting rules to planner
              </div></li><li><div>
                 validate the (materialized) view statement
              </div></li><li><div>
                 Create views registry
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> HIVE-14496: Enable Calcite rewriting with materialized views (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)
                </div><div><b>message:</b> HIVE-14496: Enable Calcite rewriting with materialized views (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol></ol></div><div><b>github_pulls_comments:</b> <ol></ol></div><div><b>github_pulls_reviews:</b> <ol></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> Enable Calcite rewriting with materialized views
                </div><div><b>description:</b> Calcite already supports query rewriting using materialized views. We will use it to support this feature in Hive.

In order to do that, we need to register the existing materialized views with Calcite view service and enable the materialized views rewriting rules. 

We should include a HiveConf flag to completely disable query rewriting using materialized views if necessary.
                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol><li><div>
                Patch includes HIVE-14497.
              </div></li><li><div>
                

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12834195/HIVE-14496.patch

{color:green}SUCCESS:{color} +1 due to 18 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 279 failed/errored test(s), 10571 tests executed
*Failed tests:*
{noformat}
TestBeelineWithHS2ConnectionFile - did not produce a TEST-*.xml file (likely timed out) (batchId=199)
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_custom_key2] (batchId=208)
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_custom_key] (batchId=208)
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_joins] (batchId=208)
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_predicate_pushdown] (batchId=208)
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_queries] (batchId=208)
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_single_sourced_multi_insert] (batchId=208)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[allcolref_in_udf] (batchId=47)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[annotate_stats_join] (batchId=48)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[annotate_stats_join_pkfk] (batchId=13)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[autoColumnStats_9] (batchId=33)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join17] (batchId=73)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join1] (batchId=69)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join22] (batchId=50)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join24] (batchId=67)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join2] (batchId=57)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join3] (batchId=72)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join_reordering_values] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join_stats2] (batchId=77)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join_stats] (batchId=43)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join_without_localtask] (batchId=1)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_sortmerge_join_12] (batchId=30)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_SortUnionTransposeRule] (batchId=14)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_rp_auto_join17] (batchId=23)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_rp_cross_product_check_2] (batchId=18)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_rp_join1] (batchId=64)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_rp_subq_exists] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_subq_exists] (batchId=69)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[constprog2] (batchId=12)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[constprog_partitioner] (batchId=64)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[correlationoptimizer10] (batchId=69)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[correlationoptimizer11] (batchId=19)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[correlationoptimizer15] (batchId=24)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cross_join] (batchId=26)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cross_join_merge] (batchId=6)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cross_product_check_1] (batchId=42)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cross_product_check_2] (batchId=79)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cteViews] (batchId=69)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cte_mat_3] (batchId=21)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cte_mat_4] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[decimal_join2] (batchId=35)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[deleteAnalyze] (batchId=28)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[escape_comments] (batchId=68)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[explain_ddl] (batchId=42)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[filter_cond_pushdown] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[filter_join_breaktask] (batchId=67)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[groupby_join_pushdown] (batchId=71)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[index_auto_self_join] (batchId=53)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[infer_bucket_sort] (batchId=74)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[innerjoin] (batchId=30)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[input23] (batchId=42)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join17] (batchId=63)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join22] (batchId=14)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join2] (batchId=78)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join3] (batchId=70)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join40] (batchId=48)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join41] (batchId=45)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join_alt_syntax] (batchId=70)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join_cond_pushdown_1] (batchId=32)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join_cond_pushdown_2] (batchId=53)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join_cond_pushdown_3] (batchId=22)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join_cond_pushdown_4] (batchId=73)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join_cond_pushdown_unqual2] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join_cond_pushdown_unqual4] (batchId=3)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join_filters_overlap] (batchId=31)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join_merge_multi_expressions] (batchId=19)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join_merging] (batchId=71)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join_parse] (batchId=37)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[join_vc] (batchId=4)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[limit_pushdown_negative] (batchId=36)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mapjoin_mapjoin] (batchId=45)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mapjoin_memcheck] (batchId=37)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mergejoin] (batchId=53)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mergejoins] (batchId=2)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mergejoins_mixed] (batchId=64)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[nonmr_fetch] (batchId=18)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[optional_outer] (batchId=4)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[parallel_join1] (batchId=4)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[pcr] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[pointlookup2] (batchId=72)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[pointlookup3] (batchId=6)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_join5] (batchId=32)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_outer_join1] (batchId=38)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_outer_join2] (batchId=28)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_outer_join3] (batchId=42)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_outer_join4] (batchId=59)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_repeated_alias] (batchId=9)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_udf_case] (batchId=39)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[reduce_deduplicate_extended2] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[regex_col] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[runtime_skewjoin_mapjoin_spark] (batchId=49)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[skewjoin] (batchId=21)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[skewjoin_noskew] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[subquery_exists] (batchId=37)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[subquery_in_having] (batchId=52)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[subquery_notexists] (batchId=80)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[subquery_notexists_having] (batchId=75)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[subquery_unqualcolumnrefs] (batchId=16)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[temp_table_join1] (batchId=51)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[tez_join_hash] (batchId=47)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_case_column_pruning] (batchId=70)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[unicode_comments] (batchId=35)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_remove_12] (batchId=39)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_remove_13] (batchId=77)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_remove_14] (batchId=11)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[union_remove_23] (batchId=66)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_binary_join_groupby] (batchId=73)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_char_mapjoin1] (batchId=29)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_left_outer_join] (batchId=20)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_outer_join1] (batchId=40)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_outer_join2] (batchId=28)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_outer_join3] (batchId=30)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_outer_join4] (batchId=76)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vector_varchar_mapjoin1] (batchId=23)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vectorized_mapjoin] (batchId=65)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vectorized_shufflejoin] (batchId=65)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[dynamic_partition_pruning_2] (batchId=133)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[explainuser_2] (batchId=134)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llapdecider] (batchId=132)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[auto_join1] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[auto_join_filters] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[auto_join_nulls] (batchId=148)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[auto_sortmerge_join_12] (batchId=142)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucket_map_join_tez1] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucket_map_join_tez2] (batchId=137)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_rp_lineage2] (batchId=137)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_rp_unionDistinct_2] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_subq_exists] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[correlationoptimizer6] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cross_join] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cross_product_check_1] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cross_product_check_2] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cte_mat_3] (batchId=139)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cte_mat_4] (batchId=136)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[deleteAnalyze] (batchId=141)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[explainuser_1] (batchId=142)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[explainuser_4] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[filter_join_breaktask] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[hybridgrace_hashjoin_1] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[join1] (batchId=136)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[join32_lessSize] (batchId=136)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[join_filters] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[join_nulls] (batchId=136)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[limit_join_transpose] (batchId=139)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[lineage2] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[lineage3] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[mapjoin_mapjoin] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[mergejoin] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[mrr] (batchId=138)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[multiMapJoin2] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[orc_llap] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[selectDistinctStar] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[skewjoin] (batchId=139)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[smb_mapjoin_17] (batchId=136)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[subquery_exists] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[subquery_in] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[subquery_notin] (batchId=148)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[subquery_views] (batchId=138)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_dynpart_hashjoin_1] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_dynpart_hashjoin_2] (batchId=142)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_join_hash] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_self_join] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_smb_1] (batchId=148)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_smb_empty] (batchId=142)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_smb_main] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_union] (batchId=142)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_vector_dynpart_hashjoin_1] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_vector_dynpart_hashjoin_2] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[unionDistinct_2] (batchId=137)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[union_top_level] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_binary_join_groupby] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_char_mapjoin1] (batchId=141)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_join_filters] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_join_nulls] (batchId=139)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_left_outer_join] (batchId=139)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_outer_join1] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_outer_join2] (batchId=141)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_outer_join3] (batchId=142)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_outer_join4] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_outer_join5] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_varchar_mapjoin1] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vectorized_mapjoin] (batchId=148)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vectorized_nested_mapjoin] (batchId=138)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vectorized_shufflejoin] (batchId=148)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[constprog_partitioner] (batchId=220)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join1] (batchId=220)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join2] (batchId=220)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join3] (batchId=220)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join4] (batchId=220)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[vector_outer_join5] (batchId=220)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_1] (batchId=90)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=91)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_4] (batchId=91)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[hybridgrace_hashjoin_1] (batchId=90)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[unionDistinct_2] (batchId=90)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[alter_view_as_select_with_partition] (batchId=83)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query17] (batchId=213)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query18] (batchId=213)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query25] (batchId=213)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query29] (batchId=213)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query50] (batchId=213)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query58] (batchId=213)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query64] (batchId=213)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query72] (batchId=213)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query85] (batchId=213)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query94] (batchId=213)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query95] (batchId=213)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[annotate_stats_join] (batchId=114)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join17] (batchId=127)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join1] (batchId=125)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join22] (batchId=115)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join24] (batchId=124)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join2] (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join3] (batchId=127)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join_filters] (batchId=117)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join_nulls] (batchId=120)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join_reordering_values] (batchId=95)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join_stats2] (batchId=129)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join_stats] (batchId=112)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_join_without_localtask] (batchId=92)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_sortmerge_join_12] (batchId=106)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[bucket_map_join_tez1] (batchId=128)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[bucket_map_join_tez2] (batchId=97)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[cross_join] (batchId=104)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[cross_product_check_1] (batchId=112)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[cross_product_check_2] (batchId=130)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[filter_join_breaktask] (batchId=124)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[index_auto_self_join] (batchId=117)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[innerjoin] (batchId=106)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join17] (batchId=122)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join1] (batchId=95)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join22] (batchId=99)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join2] (batchId=129)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join32_lessSize] (batchId=95)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join3] (batchId=126)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join41] (batchId=113)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join_alt_syntax] (batchId=126)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join_cond_pushdown_1] (batchId=107)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join_cond_pushdown_2] (batchId=117)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join_cond_pushdown_3] (batchId=103)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join_cond_pushdown_4] (batchId=127)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join_cond_pushdown_unqual2] (batchId=99)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join_cond_pushdown_unqual4] (batchId=93)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join_filters_overlap] (batchId=107)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join_merge_multi_expressions] (batchId=101)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join_merging] (batchId=126)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[join_vc] (batchId=93)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[mapjoin_mapjoin] (batchId=113)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[mapjoin_memcheck] (batchId=109)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[mergejoins] (batchId=92)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[mergejoins_mixed] (batchId=122)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[parallel_join1] (batchId=94)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[pcr] (batchId=117)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ppd_join5] (batchId=107)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ppd_outer_join1] (batchId=110)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ppd_outer_join2] (batchId=106)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ppd_outer_join3] (batchId=111)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ppd_outer_join4] (batchId=120)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[runtime_skewjoin_mapjoin_spark] (batchId=115)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[skewjoin] (batchId=102)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[skewjoin_noskew] (batchId=99)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[smb_mapjoin_17] (batchId=94)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_exists] (batchId=109)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_in] (batchId=118)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[temp_table_join1] (batchId=116)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[union_remove_12] (batchId=110)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[union_remove_13] (batchId=129)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[union_remove_14] (batchId=97)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[union_remove_23] (batchId=123)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[union_top_level] (batchId=118)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vector_left_outer_join] (batchId=102)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorized_mapjoin] (batchId=123)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorized_nested_mapjoin] (batchId=99)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorized_shufflejoin] (batchId=123)
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMethodCounts (batchId=181)
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1 (batchId=195)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJarWithoutAddDriverClazz[0] (batchId=157)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[0] (batchId=157)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[1] (batchId=157)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1668/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1668/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1668/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 279 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12834195 - PreCommit-HIVE-Build
              </div></li><li><div>
                

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12834374/HIVE-14496.01.patch

{color:green}SUCCESS:{color} +1 due to 18 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 10565 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestHBaseCliDriver.testCliDriver[hbase_bulk] (batchId=89)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_rp_unionDistinct_2] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[selectDistinctStar] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[subquery_views] (batchId=138)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[unionDistinct_2] (batchId=137)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[union_top_level] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[unionDistinct_2] (batchId=90)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[alter_view_as_select_with_partition] (batchId=83)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[union_top_level] (batchId=118)
org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.testMethodCounts (batchId=188)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJarWithoutAddDriverClazz[0] (batchId=164)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[0] (batchId=164)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[1] (batchId=164)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1693/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1693/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1693/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12834374 - PreCommit-HIVE-Build
              </div></li><li><div>
                [~jcamachorodriguez] Can you create a RB entry for this?
              </div></li><li><div>
                [~ashutoshc], I created the RB post. Trying to trigger ptest again too.
              </div></li><li><div>
                

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12834795/HIVE-14496.02.patch

{color:green}SUCCESS:{color} +1 due to 18 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 10565 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=132)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[current_date_timestamp] (batchId=144)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJarWithoutAddDriverClazz[0] (batchId=164)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[0] (batchId=164)
org.apache.hive.beeline.TestBeelineArgParsing.testAddLocalJar[1] (batchId=164)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1754/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1754/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1754/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12834795 - PreCommit-HIVE-Build
              </div></li><li><div>
                [~ashutoshc], fails are unrelated. This is ready to be reviewed. Thanks
              </div></li><li><div>
                Few high level questions based on browsing of patch:

* I am not sure seprating view info in a seperate table is a good idea. This will result in a join between these two tables for every getTable() call, which will be perf hit. We know that denormalized schemas are good for perf :) Also this makes upgrade process little fragile since data needs to be rewritten for these tables.
* Not sure if there is a need for creation time in view key. Can you explain how it is used. Using time in distributed systems is fraught with peril.
* In view matching rule currently it has Proj-FIL-TS. If all columns are projected out, Proj may be absent. Will this rule have trouble matching that? What if there is no filter, but only TS-Proj. Will this match that?
* It seems like we are only using mv present in current db, but Hive allows user to use tables in other dbs in a single session. 
* Also I don't see any index on materializations to do early discard based on table or columns referenced. Is that part of rule itself?
              </div></li><li><div><div><b>body:</b> - About the separated table in the metastore, I understand what you mean. Will we retrieve 'always' the information about the views when we query the metastore in getTable()? I thought we did not, but I think I was confused and we do. I will revert it then; maybe in the future, if we continue adding view specific columns, we can take it back.
- I added the creation time when I thought more than one HS2 instance would be able to edit the table information in the metastore. As we talked offline, this should not be the case, thus it should be fine and I will remove it. Btw,  if I remember correctly, the time was just taken from metastore information, it is not something that I was creating myself, thus in this case it should be robust.
- I will extend the rule to match Proj-FIL. About TS-Proj, it would not make much sense in this context, since there is not much gain scanning the view instead of the table, right? (unless you take into account properties of the view such as sorting or partitioning, which we do not right now).
- If I remember correctly, I load all the views for the current user. The problem is that I cannot add views for other users, since I do not have permissions to do so, e.g. if we are using any kind of authentication. All metastore methods seem to check current user permissions. Or is there any way to bypass the authentication to the metastore to retrieve all tables (I thought that was not allowed, since otherwise anybody could bypass that security measures)?
- That's done on the Calcite side by the rewriting engine, although we could add further filtering in the rule itself (in fact, currently early filtering is only table based, we have not implemented anything more sophisticated yet).
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12842587/HIVE-14496.03.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2523/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2523/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2523/

Messages:
{noformat}
**** This message was trimmed, see log for full details ****
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/util/Tool.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar(org/apache/hadoop/conf/Configurable.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/ClassNotFoundException.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/curator/curator-framework/2.7.1/curator-framework-2.7.1.jar(org/apache/curator/framework/CuratorFrameworkFactory.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar(org/apache/curator/retry/ExponentialBackoffRetry.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Mapper.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Iterator.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/LinkedList.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/ExecutorService.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/Executors.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/concurrent/TimeUnit.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-mapreduce-client-core/2.7.2/hadoop-mapreduce-client-core-2.7.2.jar(org/apache/hadoop/mapreduce/Mapper$Context.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/net/URLDecoder.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Enumeration.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/util/Properties.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/core/UriBuilder.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/apache-github-source-source/common/target/hive-common-2.2.0-SNAPSHOT.jar(org/apache/hadoop/hive/common/LogUtils.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Class.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/Annotation.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-annotations/2.7.2/hadoop-annotations-2.7.2.jar(org/apache/hadoop/classification/InterfaceAudience.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/org/apache/hadoop/hadoop-annotations/2.7.2/hadoop-annotations-2.7.2.jar(org/apache/hadoop/classification/InterfaceAudience$LimitedPrivate.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/Retention.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/RetentionPolicy.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/Target.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/annotation/ElementType.class)]]
[loading ZipFileIndexFileObject[/data/hiveptest/working/maven/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar(javax/ws/rs/HttpMethod.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/SuppressWarnings.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(java/lang/Override.class)]]
[loading ZipFileIndexFileObject[/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar(sun/misc/Contended.class)]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/HcatException$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/HcatDelegator$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/Server$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$3.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/LauncherDelegator$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/SecureProxySupport$2.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/HDFSStorage$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonUtils$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/ZooKeeperStorage$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/TempletonControllerJob$1$1.class]]
[loading RegularFileObject[/data/hiveptest/working/apache-github-source-source/hcatalog/webhcat/svr/target/classes/org/apache/hive/hcatalog/templeton/tool/LogRetriever$1.class]]
[done in 3719 ms]
+ [[ -d itests ]]
+ cd itests
+ mvn -B clean install -DskipTests -T 4 -q -Dmaven.repo.local=/data/hiveptest/working/maven
[ERROR] COMPILATION ERROR : 
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[213,9] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
      (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[297,13] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
      (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[554,19] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
      (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[559,24] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
      (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[578,25] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
      (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[604,19] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
      (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[609,24] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
      (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[628,25] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
      (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[656,19] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
      (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[661,27] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
      (actual and formal argument lists differ in length)
    constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
      (actual and formal argument lists differ in length)
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hive-hcatalog-it-unit: Compilation failure: Compilation failure:
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[213,9] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[297,13] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[554,19] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[559,24] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[578,25] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[604,19] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[609,24] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[628,25] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[656,19] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] /data/hiveptest/working/apache-github-source-source/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/TestDbNotificationListener.java:[661,27] no suitable constructor found for Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,&lt;nulltype&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;,&lt;nulltype&gt;)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table() is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(java.lang.String,java.lang.String,java.lang.String,int,int,int,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.List&lt;org.apache.hadoop.hive.metastore.api.FieldSchema&gt;,java.util.Map&lt;java.lang.String,java.lang.String&gt;,java.lang.String,java.lang.String,boolean,java.lang.String) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] constructor org.apache.hadoop.hive.metastore.api.Table.Table(org.apache.hadoop.hive.metastore.api.Table) is not applicable
[ERROR] (actual and formal argument lists differ in length)
[ERROR] -&gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn &lt;goals&gt; -rf :hive-hcatalog-it-unit
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12842587 - PreCommit-HIVE-Build
              </div></li><li><div><div><b>body:</b> [~ashutoshc], let's try to check this one in. I have addressed the most important comments in the last patch. In particular:
- Loading all materialized views definitions for all users when HS2 starts (instead of per session).
- Adding just an additional field for rewrite enabled (instead of creating a 'view descriptor'). This simplified a lot the changes in the scripts to upgrade metastore.

I left for a follow-up:
- Extension of rules to match new patterns.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12842593/HIVE-14496.04.patch

{color:green}SUCCESS:{color} +1 due to 18 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 121 failed/errored test(s), 10784 tests executed
*Failed tests:*
{noformat}
TestHBaseImport - did not produce a TEST-*.xml file (likely timed out) (batchId=193)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_sortmerge_join_2] (batchId=44)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[create_view] (batchId=36)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[create_view_partitioned] (batchId=34)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cteViews] (batchId=70)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[materialized_view_create_rewrite] (batchId=2)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=38)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=134)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_rp_unionDistinct_2] (batchId=142)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_orc_acid_part_update] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[subquery_views] (batchId=139)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[unionDistinct_2] (batchId=139)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[unionDistinct_2] (batchId=91)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggrStatsCacheIntegration.alterInvalidation (batchId=199)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggrStatsCacheIntegration.altersInvalidation (batchId=199)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggrStatsCacheIntegration.hit (batchId=199)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggrStatsCacheIntegration.invalidation (batchId=199)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggrStatsCacheIntegration.someWithStats (batchId=199)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsCache.allWithStats (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsCache.noneWithStats (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsCache.nonexistentPartitions (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsCache.someNonexistentPartitions (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsCacheWithBitVector.allPartitions (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsExtrapolation.MiddleOfPartitionsHaveBitVectorStatus (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsExtrapolation.TwoEndsAndMiddleOfPartitionsHaveBitVectorStatusDouble (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsExtrapolation.TwoEndsAndMiddleOfPartitionsHaveBitVectorStatusLong (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsExtrapolation.TwoEndsOfPartitionsHaveBitVectorStatus (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsExtrapolation.allPartitionsHaveBitVectorStatusDecimal (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsExtrapolation.allPartitionsHaveBitVectorStatusDouble (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsExtrapolation.allPartitionsHaveBitVectorStatusLong (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsExtrapolation.allPartitionsHaveBitVectorStatusString (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsExtrapolation.noPartitionsHaveBitVectorStatus (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsNDVUniformDist.MiddleOfPartitionsHaveBitVectorStatus (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsNDVUniformDist.TwoEndsAndMiddleOfPartitionsHaveBitVectorStatusDecimal (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsNDVUniformDist.TwoEndsAndMiddleOfPartitionsHaveBitVectorStatusDouble (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsNDVUniformDist.TwoEndsAndMiddleOfPartitionsHaveBitVectorStatusLong (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsNDVUniformDist.TwoEndsOfPartitionsHaveBitVectorStatus (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsNDVUniformDist.allPartitionsHaveBitVectorStatus (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsNDVUniformDist.noPartitionsHaveBitVectorStatus (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreMetrics.testMetaDataCounts (batchId=199)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.alterRename (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.alterRenamePartitioned (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.describeNonpartitionedTable (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.insertIntoPartitionTable (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.insertIntoTable (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.partitionedTable (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.table (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseSchemaTool.oneMondoTest (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.addMultiColForeignKey (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.addMultiColPrimaryKey (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.addMultiForeignKeys (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.addSecondForeignKeys (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.alterIndex (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.alterPartition (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.alterTable (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.binaryPartitionStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.binaryTableStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.booleanPartitionStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.booleanTableStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.createIndex (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.createPartition (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.createTable (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.createTableWithForeignKey (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.createTableWithPrimaryKey (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.decimalPartitionStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.decimalTableStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.doublePartitionStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.doublePrimaryKey (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.doubleTableStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.dropIndex (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.dropPartition (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.dropTable (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.getPartitions (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.listGetDropPartitionNames (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.longPartitionStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.longTableStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.skewInfo (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.stringPartitionStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStore.stringTableStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreBitVector.decimalPartitionStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreBitVector.decimalTableStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreBitVector.doublePartitionStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreBitVector.doubleTableStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreBitVector.longPartitionStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreBitVector.longTableStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreBitVector.stringPartitionStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreBitVector.stringTableStatistics (batchId=186)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreCached.alterTable (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreCached.booleanTableStatistics (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreCached.createPartition (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreCached.createTable (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreCached.dropPartition (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreCached.dropTable (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreCached.getPartitions (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreCached.listGetDropPartitionNames (batchId=187)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.addPartitions (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.alterPartitions (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.alterTable (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.createPartition (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.createTable (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.dropPartition (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.dropTable (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.getAllDbs (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.getAllTables (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.getDbsRegex (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.getPartitions (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.getPartitionsByFilter (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.grantRevokeTablePrivileges (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.listPartitions (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.listPartitionsWithPs (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.listTableGrants (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.partitionStatistics (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestHBaseStoreIntegration.tableStatistics (batchId=193)
org.apache.hadoop.hive.metastore.hbase.TestStorageDescriptorSharing.createManyPartitions (batchId=193)
org.apache.hive.beeline.TestSchemaTool.testValidateLocations (batchId=209)
org.apache.hive.beeline.TestSchemaTool.testValidateNullValues (batchId=209)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2526/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2526/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2526/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 121 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12842593 - PreCommit-HIVE-Build
              </div></li><li><div>
                

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12842771/HIVE-14496.05.patch

{color:green}SUCCESS:{color} +1 due to 18 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 14 failed/errored test(s), 10781 tests executed
*Failed tests:*
{noformat}
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=143)
	[vectorized_rcfile_columnar.q,vector_elt.q,explainuser_1.q,multi_insert.q,tez_dml.q,vector_bround.q,schema_evol_orc_acid_table.q,vector_when_case_null.q,orc_ppd_schema_evol_1b.q,vector_join30.q,vectorization_11.q,cte_3.q,update_tmp_table.q,vector_decimal_cast.q,groupby_grouping_id2.q,vector_decimal_round.q,tez_smb_empty.q,orc_merge6.q,vector_decimal_trailing.q,cte_5.q,tez_union.q,cbo_rp_subq_not_in.q,vector_decimal_2.q,columnStatsUpdateForStatsOptimizer_1.q,vector_outer_join3.q,schema_evol_text_vec_part_all_complex.q,tez_dynpart_hashjoin_2.q,auto_sortmerge_join_12.q,offset_limit.q,tez_union_multiinsert.q]
TestVectorizedColumnReaderBase - did not produce a TEST-*.xml file (likely timed out) (batchId=250)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=38)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=134)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=92)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[unionDistinct_2] (batchId=91)
org.apache.hadoop.hive.metastore.hbase.TestHBaseSchemaTool.oneMondoTest (batchId=193)
org.apache.hive.beeline.TestSchemaTool.testValidateLocations (batchId=209)
org.apache.hive.beeline.TestSchemaTool.testValidateNullValues (batchId=209)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2540/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2540/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2540/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 14 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12842771 - PreCommit-HIVE-Build
              </div></li><li><div>
                Regenerating last test files.
              </div></li><li><div>
                Rebasing patch.
              </div></li><li><div>
                

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12842960/HIVE-14496.07.patch

{color:green}SUCCESS:{color} +1 due to 19 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 11 failed/errored test(s), 10784 tests executed
*Failed tests:*
{noformat}
TestMiniLlapLocalCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=144)
	[vectorized_rcfile_columnar.q,vector_elt.q,explainuser_1.q,multi_insert.q,tez_dml.q,vector_bround.q,schema_evol_orc_acid_table.q,vector_when_case_null.q,orc_ppd_schema_evol_1b.q,vector_join30.q,vectorization_11.q,cte_3.q,update_tmp_table.q,vector_decimal_cast.q,groupby_grouping_id2.q,vector_decimal_round.q,tez_smb_empty.q,orc_merge6.q,vector_decimal_trailing.q,cte_5.q,tez_union.q,cbo_rp_subq_not_in.q,vector_decimal_2.q,columnStatsUpdateForStatsOptimizer_1.q,vector_outer_join3.q,schema_evol_text_vec_part_all_complex.q,tez_dynpart_hashjoin_2.q,auto_sortmerge_join_12.q,offset_limit.q,tez_union_multiinsert.q]
TestVectorizedColumnReaderBase - did not produce a TEST-*.xml file (likely timed out) (batchId=251)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=38)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=133)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_5] (batchId=92)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2557/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2557/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2557/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 11 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12842960 - PreCommit-HIVE-Build
              </div></li><li><div>
                [~ashutoshc], fails are unrelated. Could you take a look? Thanks
              </div></li><li><div>
                [~ashutoshc], new patch addresses your comments. I also created HIVE-15436 for a the follow-up about enhancing metastore API.
              </div></li><li><div>
                In metastore/if/hive_metastore.thrift can you declare rewrite_enabled as optional and have that declaration at last in the struct.
+1 with that change, pending QA run.
              </div></li><li><div>
                

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12843459/HIVE-14496.09.patch

{color:green}SUCCESS:{color} +1 due to 14 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 25 failed/errored test(s), 10820 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=234)
TestVectorizedColumnReaderBase - did not produce a TEST-*.xml file (likely timed out) (batchId=251)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cbo_rp_auto_join1] (batchId=3)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[create_view] (batchId=37)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[create_view_partitioned] (batchId=34)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[cteViews] (batchId=70)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[groupby_join_pushdown] (batchId=72)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=39)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_sort_array] (batchId=59)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[cbo_rp_unionDistinct_2] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[metadataonly1] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[subquery_views] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[unionDistinct_2] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=93)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[unionDistinct_2] (batchId=92)
org.apache.hadoop.hive.ql.metadata.TestHive.testTable (batchId=259)
org.apache.hadoop.hive.ql.metadata.TestHive.testThriftTable (batchId=259)
org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testTable (batchId=260)
org.apache.hadoop.hive.ql.metadata.TestHiveRemote.testThriftTable (batchId=260)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2598/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2598/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2598/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 25 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12843459 - PreCommit-HIVE-Build
              </div></li><li><div>
                

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12843582/HIVE-14496.10.patch

{color:green}SUCCESS:{color} +1 due to 15 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 10820 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=234)
TestVectorizedColumnReaderBase - did not produce a TEST-*.xml file (likely timed out) (batchId=251)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample2] (batchId=5)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample4] (batchId=15)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample6] (batchId=61)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample7] (batchId=60)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[sample9] (batchId=39)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_sort_array] (batchId=59)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[metadataonly1] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[stats_based_fetch_decision] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=93)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/2610/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/2610/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-2610/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12843582 - PreCommit-HIVE-Build
              </div></li><li><div>
                Fails are unrelated. Pushed to master, thanks for reviewing [~ashutoshc]!
              </div></li><li><div><div><b>body:</b> Doc note:  This adds *hive.materializedview.rewriting* to HiveConf.java, so it needs to be documented in the wiki in Configuration Properties and perhaps also in the new DDL section for materialized views that will be created for HIVE-14497.

* [Configuration Properties -- Query and DDL Execution | https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-QueryandDDLExecution]

Added a TODOC2.2 label.
                </div><div><b>label:</b> documentation
                </div></div></li></ol></div></div></html>