<!DOCTYPE html><html><div class="item-title">
        Item 74
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                * Whether to make transition labels Unicode code points instead of UTF8 bytes, 
   *  &lt;code&gt;false&lt;/code&gt; by default 
              </div></li><li><div>
                * Used by subclass to change the lookup automaton, if
   *  necessary. 
              </div></li><li><div>
                * Measure maxEdits, minFuzzyLength, transpositions and nonFuzzyPrefix 
   *  parameters in Unicode code points (actual letters)
   *  instead of bytes. 
              </div></li><li><div>
                 expected
              </div></li><li><div>
                 bytes 0x1e and 0x1f are reserved
              </div></li><li><div>
                *
   * If &lt;code&gt;true&lt;/code&gt;, maxEdits, minFuzzyLength, transpositions and nonFuzzyPrefix 
   * will be measured in Unicode code points (actual letters) instead of bytes.
   
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> LUCENE-5030: FuzzySuggester can optionally measure edits in Unicode code points instead of UTF8 bytes
                </div><div><b>message:</b> LUCENE-5030: FuzzySuggester can optionally measure edits in Unicode code points instead of UTF8 bytes

git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1504490 13f79535-47bb-0310-9956-ffa450edef68

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol></ol></div><div><b>github_pulls_comments:</b> <ol></ol></div><div><b>github_pulls_reviews:</b> <ol></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> FuzzySuggester has to operate FSTs of Unicode-letters, not UTF-8, to work correctly for 1-byte (like English) and multi-byte (non-Latin) letters
                </div><div><b>description:</b> There is a limitation in the current FuzzySuggester implementation: it computes edits in UTF-8 space instead of Unicode character (code point) space. 

This should be fixable: we'd need to fix TokenStreamToAutomaton to work in Unicode character space, then fix FuzzySuggester to do the same steps that FuzzyQuery does: do the LevN expansion in Unicode character space, then convert that automaton to UTF-8, then intersect with the suggest FST.

See the discussion here: http://lucene.472066.n3.nabble.com/minFuzzyLength-in-FuzzySuggester-behaves-differently-for-English-and-Russian-td4067018.html#none
                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol><li><div>
                I've added a test, which demonstrates the bug. I have fixed TokenStreamToAutomaton, but I have no idea, how to update AnalyzingSuggester, which wants bytes instead of chars (ints, which cannot fit a byte).
              </div></li><li><div><div><b>body:</b> Thanks Artem!

I think we need to change POS_SEP and HOLE?  256/267 are no longer safe to use?

Also, I think TokenStreamToAutomaton should operate with Unicode code points, not code units (java's char)?  Probably we should fork TS2A to a new class (TokenStreamToUnicodeAutomaton or something?).

Then, in FuzzySuggester, after calling toLevenshteinAutomata and before calling FSTUtil.intersectPrefixPaths, we need to insert a call to UTF32ToUTF8().convert(automaton), just like in CompiledAutomaton.java.  That call will translate the fuzzed-up unicode Automaton into the UTF8 equivalent, and then we should be able to pass that on to intersect.

Also, I think you'll need to fix that 255 in FuzzySuggester.toLevenshteinAutomata ... in fact, just remove it, so we use the ctor that passes max unicode code point.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Now all the tests pass except testRandom when preserveSep is true.

Michael, can you explain me, how this preserve separator feature works?
              </div></li><li><div>
                Hmm POS_SEP and HOLE are still eating into Unicode's space (the last 2
code points)?  Maybe, we should just use Integer.MAX_VALUE and
MAX_VALUE-1?

In fact I think you'll need to make POS_SEP and HOLE consistent across
both of the TS2A classes.  I think you should just define them in
TokenStreamToAutomaton.java, and then reference those constants from
TokenStreamToUnicodeAutomaton.java?

There's a lot of whitespace noise here ... can you remove those
changes so we can more easily see the "real" changes?  Thanks.

The preserveSep option means when finding a match we must "respect"
the boundaries between tokens.  It could be you're seeing tests fail
because POS_SEP was different between the two classes?

              </div></li><li><div>
                Sorry for autoformatting, I will upload the patch without it.
Since we use Integer.MAX_VALUE we do not need EscapingTokenStreamToUnicodeAutomaton any more, because the text will not have such a code point?
              </div></li><li><div>
                BTW, if I replace it with Integer.MAX_VALUE, UTF32ToUTF8().convert(unicodeAutomaton) will not work any more. I mean it will not convert Integer.MAX_VALUE into a sequence of bytes (it converts it into 1fff bf bf bf). So I guess, we still need EscapingTokenStreamToUnicodeAutomaton.
              </div></li><li><div>
                the patch without autoformatting
              </div></li><li><div>
                I see, the patch still has autoformatting of some spaces. Sorry, I guess I cannot stop IntelliJ IDEA doing it :(
              </div></li><li><div>
                with untouched trailing spaces
              </div></li><li><div>
                Oh, right, we can't just use MAX_VALUE: it must be a valid unicode char since we will send it through UTF32toUTF8.

Also, it's easiest if that char survives to UTF8 as a single byte to keep replaceSep [relatively] simple.

Maybe we "steal" two unicode chars?  Maybe INFO_SEP (U+001F) and INFO_SEP2 (U+001E), and we document that these chars are not allowed on the input?  (We could also try, maybe later as a separate issue, to escape them when they occur, like EscapingTokenStreamToUnicodeAutomaton now does if it sees 0xFF on the input).

              </div></li><li><div>
                you already have
private static final int PAYLOAD_SEP = '\u001f';
in AnalyzingSuggester
              </div></li><li><div>
                I have fixed testRandom, which repeats the logic of FuzzySuggester.
Now all the tests pass.
Please, review.
              </div></li><li><div><div><b>body:</b> I see, that some tests in AnalyzingSuggesterTest fail, so I have to look what's wrong...
                </div><div><b>label:</b> test
                </div></div></li><li><div>
                now tests in FuzzySuggesterTest and AnalyzingSuggesterTest pass, except for AnalyzingSuggesterTest.testRandom (when preserveSep = true).

If I enable VERBOSE, I see, that suggestions are correct. I guess, there is a bug in the test, but I cannot find it.

Can you please review?
              </div></li><li><div><div><b>body:</b> I dont think changing SEP_LABEL from a single byte to 4 bytes is necessarily a good idea.

I think benchmarks (size and speed) should be run on this change before we jump into it, I'm also concerned about the determinization and shit being in the middle of an autosuggest request... this seems like it would be way way too slow.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> Possibly we should change it to INFO_SEP2 (U+001E) as Michael suggested for TokenStreamToAutomaton?
Do you like 0x10ffff and 0x10fffe separators in TokenStreamToAutomaton? Won't they slow down the process?
I guess, Michael is the man, who runs benchmarks regularly? I don't know, how to do it...
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                The easy performance tester to run is
lucene/suggest/src/test/org/apache/lucene/search/suggest/LookupBenchmarkTest.java
... we should test that first I think?  I can also run one based on
FreeDB ... the sources are in luceneutil
(https://code.google.com/a/apache-extras.org/p/luceneutil/ ).

If the perf hit is too much then one option would be to make it
optional (whether we count edits in Unicode space UTF-8 space), or
maybe just another suggester class (FuzzyUnicodeSuggester?).

I think we can use INFO_SEP: yes, this is used for PAYLOAD_SEP, but
that only means the incoming surfaceForm cannot contain this char, I
think?  So ... I think we are free to use it in the analyzed form?  Or
did something go wrong when you tried?

Whichever chars we use (steal), we should add checks that these chars do not
occur in the input...

              </div></li><li><div>
                I ran this command:
{code}ant -Dtestcase=LookupBenchmarkTest clean test{code}

and got the same results for the patched and the original version:
{code}[junit4:junit4] Tests summary: 1 suite, 0 tests
     [echo] 5 slowest tests:
[junit4:tophints]  22.95s | org.apache.lucene.search.spell.TestSpellChecker
[junit4:tophints]  22.70s | org.apache.lucene.search.suggest.analyzing.AnalyzingSuggesterTest
[junit4:tophints]  15.08s | org.apache.lucene.search.suggest.fst.TestSort
[junit4:tophints]  11.84s | org.apache.lucene.search.suggest.fst.FSTCompletionTest
[junit4:tophints]  11.24s | org.apache.lucene.search.suggest.analyzing.FuzzySuggesterTest
{code}
              </div></li><li><div>
                I used INFO_SEP and INFO_SEP2 for separators and holes. All the tests pass (I have fixed AnalyzingSuggesterTest.testStolenBytes). The benchmark is improved:

{code}[junit4:junit4] Suite: org.apache.lucene.search.suggest.LookupBenchmarkTest
[junit4:junit4] Completed in 0.04s, 0 tests
[junit4:junit4] 
[junit4:junit4] JVM J0:     1.64 ..     2.34 =     0.71s
[junit4:junit4] Execution time total: 2.36 sec.
[junit4:junit4] Tests summary: 1 suite, 0 tests
     [echo] 5 slowest tests:
[junit4:tophints]  22.95s | org.apache.lucene.search.spell.TestSpellChecker
[junit4:tophints]  15.08s | org.apache.lucene.search.suggest.fst.TestSort
[junit4:tophints]  13.41s | org.apache.lucene.search.suggest.analyzing.AnalyzingSuggesterTest
[junit4:tophints]  11.84s | org.apache.lucene.search.suggest.fst.FSTCompletionTest
[junit4:tophints]  10.78s | org.apache.lucene.search.suggest.analyzing.FuzzySuggesterTest
{code}
              </div></li><li><div><div><b>body:</b> Hi Artem,

Sorry, running the LookupBenchmarkTest is tricky ... you need to make temporary changes in 3 places.  I'm attaching a patch that should let you run it by just doing "ant test -Dtestcase=LookupBenchmarkTest".
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> OK, in general the performance is worse twice.
before my patch: Total time: 9 minutes 20 seconds
after my patch with INFO_SEP and INFO_SEP2: Total time: 18 minutes 31 seconds

I guess, the reason is UTF32ToUTF8().convert(unicodeAutomaton), so it is better to add correct transitions on the fly... but possibly, you can postpone this for another issue?
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Hmm can you post the full output of the benchmark?  It measures different things in each test case.

Given this, I guess we should make a separate fuzzy suggester class (that measures edit distance in Unicode code point space)?  Or make it a boolean option on the current class ...

Can you also post your last patch (cutting over to INFO_SEP/2 for the stolen chars)?
              </div></li><li><div>
                The last patch with INFO_SEP/2 was posted today 20/Jun/13 at 10:52
              </div></li><li><div>
                Oh, woops, I missed it.  Thanks.
              </div></li><li><div><div><b>body:</b> I'm uploading 3 results of benchmarking:
1) benchmark-old.txt - before the patch
2) benchmark-INFO_SEP.txt - after the patch
3) benchmark-wo_convertion.txt - after the patch, but with commented lines of convertion in several places, like
{code}      //Automaton utf8lookupAutomaton = new UTF32ToUTF8().convert(lookupAutomaton);
      //BasicOperations.determinize(utf8lookupAutomaton);{code}
because they are useless for words consisting only latin letters.

As you can see the conversion takes too much time.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                OK, I will add a new option UNICODE_AWARE = 4, which will switch conversion ON.
              </div></li><li><div><div><b>body:</b> I'm a little confused by the results.  E.g., how come AnalyzingSuggester changes so much between old (57 kQPS) and INFO_SEP (27 kQPS) and wo_conversion (41 kQPS), at the 2-4 prefixes len?  And why are the unrelated impls (TSS, FST, WFST, Jaspell) changing so much?  Maybe this is just horrible Hotspot noise?
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Oh, duh, the conversion from Unicode -&gt; UTF8, and the determinization, are in AnalyzingSuggester ... so it makes sense that it got slower.

I agree we should add a UNICODE_AWARE option.
              </div></li><li><div>
                I have added UNICODE_AWARE option in Lucene and Solr. Should I create a separate Solr issue for Solr updated files or can I upload a patch for all updated files?
              </div></li><li><div>
                I have uploaded a lucene/solr combo patch with new UNICODE_AWARE option
              </div></li><li><div><div><b>body:</b> Thanks Artem!

I don't understand why we needed to change
AnalyzingSuggesterTest.testStolenBytes?  That implies something is
wrong w/ the escaping I think?  (Ie, results in that test should not
change whether SEP is preserved or not).  So I'm confused what
changed...

Also, I think we don't need the check for SEP_LABEL in
AnalyzingSuggester.lookup (that throws IllegalArgumentException)?  (We
"escape" this char).  But we should check for HOLE and throw
IllegalArgumentException, since we don't escape it.  And could you add
a test confirming you get that exc if you try to add HOLE?  Thanks.

                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Sorry, I don't understand, why testStolenBytes worked before. I have restored it and now it fails. Can you please suggest, what wrong I did?

As I understood, if we do not preserve the separator, 1 token with a separator and 2 tokens (which is actually 1 string with a separator) equals after removing the separator in  replaceSep, so we should get 2 results instead of 1 when we do a lookup. No?

I've added a test for IllegalArgumentException.
              </div></li><li><div>
                I have restored testStolenBytes completely and now all the tests pass (see [nonlatin_fuzzySuggester_combo2.patch|https://issues.apache.org/jira/secure/attachment/12589721/nonlatin_fuzzySuggester_combo2.patch]).

But I'm not sure, what did you mean by 0xff byte in {code}token(new BytesRef(new byte[] {0x61, (byte) 0xff, 0x61})){code}? Letter ÿ or SEP_LABEL?

Now it is treated as letter ÿ, but in the previous modification of the test I treated it as SEP_LABEL.
              </div></li><li><div><div><b>body:</b> Hmm, testStolenBytes should be using the 0x1f byte ... the intention
of the test is to ensure than an incoming token that contains
SEP_LABEL still works correctly (i.e., that the escaping we do is
working).

When I change the 0xff in the patch back to 0x1f I indeed see the
(unexpected) failure without the PRESERVE_SEP option, which is curious
because we do no escaping without PRESERVE_SEP.

OK I see the issue: before, when POS_SEP was 256 and the input space
was a byte, replaceSep always worked correctly because there was no
way for any byte input to be confused with POS_SEP.  But now that we
are increasing the input space to all unicode chars, there is not
"safe" value for POS_SEP.

OK given all this I think we should stop trying to not-steal the byte:
I think we should simply declare we steal both 0x1e and 0x1f.  This
means we can remove the escaping code, put back your previous code
that I had asked you to remove (sorry) that threw IAE on 0x1f (and now
also 0x1e), remove testStolenBytes, and then improve your new
testIllegalLookupArgument to also verify 0x1f gets the
IllegalArgumentException?

Also, we could maybe eliminate some code dup here, e.g. the two
toFiniteStrings ... maybe by having TS2A and TS2UA share a base class
/ interface.  Hmm, maybe we should just merge TS2UA back into TS2A,
and add a unicodeAware option to it?

                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Done. Please, review LUCENE-5030.patch
              </div></li><li><div>
                BTW, for your {code}// TODO: is there a Reader from a CharSequence?{code} in AnalyzingSuggester.toLookupAutomaton
There is org.apache.commons.io.input.CharSequenceReader
              </div></li><li><div><div><b>body:</b> Thanks Artem, new patch looks good.

Thanks for the tip about org.apache.commons.io.input.CharSequenceReader!  I'll update the TODO with this information, but I don't think we should pull in a dep on commons for this.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                I plan to commit the last patch soon ... thanks Artem!

              </div></li><li><div>
                Cool!
              </div></li><li><div>
                {noformat}
+  /** Include this flag in the options parameter to {@link
+   *  #AnalyzingSuggester(Analyzer,Analyzer,int,int,int)} if
+   *  you want your suggester to operate non-ASCII letters. */
+  public static final int UNICODE_AWARE = 4;
{noformat}

Errr... this implies that there is something wrong with AnalyzingSuggester, or that this option actually *even does anything at all* for AnalyzingSuggester, when in fact it only changes the behavior of FuzzySuggester.

Can we fix this?
              </div></li><li><div><div><b>body:</b> The javadocs are fixed.
                </div><div><b>label:</b> documentation
                </div></div></li><li><div>
                Maybe we should rename UNICODE_AWARE to FUZZY_UNICODE_AWARE?  (Because AnalyzingSuggester itself is already unicode aware... so this flag only impacts FuzzySuggester.)
              </div></li><li><div>
                Hmm also "ant precommit" is failing ...
              </div></li><li><div>
                in ant precommit I get this error:
extra-target.xml:68: javax.script.ScriptException: javax.script.ScriptException: org.tmatesoft.svn.core.SVNException: svn: E155007: 'lucene-solr' is not a working copy

I use git, not SVN, so I'm a bit confused...

What error do you get?
              </div></li><li><div><div><b>body:</b> OK no problem, I can fix it.  The javadocs linter is angry that isUnicodeAware has no text (only an @return) ...

                </div><div><b>label:</b> documentation
                </div></div></li><li><div>
                New patch, fixing the linter error, renaming UNICODE_AWARE -&gt; FUZZY_UNICODE_AWARE, and fixing one compilation warning ... I think it's ready.
              </div></li><li><div>
                I have renamed the variables in comments and tests for consistency
              </div></li><li><div>
                Sorry for the long delay here ...

Just to verify: there is no point to passing FUZZY_UNICODE_AWARE to AnalyzingSuggester, right?

In which case, I think we the AnalyzingLookupFactory should not be changed?

But, furthermore, I think we can isolate the changes to FuzzySuggester?  E.g., move the FUZZY_UNICODE_AWARE flag down to FuzzySuggester, fix its ctor to strip that option when calling super() and move the isFuzzyUnicodeAware down as well, and then override toLookupAutomaton to do the utf8 conversion + det?

This way it's not even possible to send the fuzzy flag to AnalyzingSuggester.
              </div></li><li><div>
                Then I have to override (and copy a lot of code) getTokenStreamToAutomaton, lookup, toFiniteStrings and make a lot of private variables protected.
I think, this is not a good idea.
              </div></li><li><div>
                Moved the parameter from AnalyzingLookupFactory to FuzzyLookupFactory
              </div></li><li><div>
                Michael, I got your idea. I will refactor the code not to use FUZZY_UNICODE_AWARE in AnalyzingSuggester.
              </div></li><li><div>
                The code is refactored not to touch AnalyzingSuggester. Please, review.
              </div></li><li><div>
                Patch looks great!  Thanks Artem.  No more mixing in of fuzzy-ness
into AnalyzingSuggester.

It looks like we are doing the utf8 conversion + det twice per lookup,
once in convertAutomaton and once in getFullPrefixPaths.  But, I think
this is inevitable: the first conversion is on the "straight"
automaton, for exactFirst match, and the second one is on the lev
automaton, for non-exactFirst.

Really we should only do the first convertAutomaton if exactFirst is
true ... but this is an optimization so we don't need to fix it now.

              </div></li><li><div>
                Commit 1504490 from [~mikemccand] in branch 'dev/trunk'
[ https://svn.apache.org/r1504490 ]

LUCENE-5030: FuzzySuggester can optionally measure edits in Unicode code points instead of UTF8 bytes
              </div></li><li><div>
                Commit 1504492 from [~mikemccand] in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1504492 ]

LUCENE-5030: FuzzySuggester can optionally measure edits in Unicode code points instead of UTF8 bytes
              </div></li><li><div>
                OK I committed the last patch with a few small fixes:

  * Added @lucene.experimental to FuzzySuggester

  * Removed the added ctor (so we have just two ctors: the easy one,
    which uses all defaults, and the expert one, where you specify
    everything)

  * Removed System.out.printlns from the test

Thanks Artem!

              </div></li><li><div>
                JUHUUUU! :-) Thanks for heavy committing - it took a long time, but now it is good! Many thanks, Uwe
              </div></li><li><div>
                Great! Thanks for reviewing.
              </div></li><li><div>
                4.5 release -&gt; bulk close
              </div></li></ol></div></div></html>