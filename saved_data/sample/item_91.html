<!DOCTYPE html><html><div class="item-title">
        Item 91
      </div> <div class="item-details"><div><b>git_comments:</b> <ol></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> [SPARK-10772] [STREAMING] [SCALA] NullPointerException when transform function in DStream returns NULL
                </div><div><b>message:</b> [SPARK-10772] [STREAMING] [SCALA] NullPointerException when transform function in DStream returns NULL

Currently, the ```TransformedDStream``` will using ```Some(transformFunc(parentRDDs, validTime))``` as compute return value, when the ```transformFunc``` somehow returns null as return value, the followed operator will have NullPointerExeception.

This fix uses the ```Option()``` instead of ```Some()``` to deal with the possible null value. When   ```transformFunc``` returns ```null```, the option will transform null to ```None```, the downstream can handle ```None``` correctly.

NOTE (2015-09-25): The latest fix will check the return value of transform function, if it is ```NULL```, a spark exception will be thrown out

Author: Jacker Hu &lt;gt.hu.chang@gmail.com&gt;
Author: jhu-chang &lt;gt.hu.chang@gmail.com&gt;

Closes #8881 from jhu-chang/Fix_Transform.

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> [SPARK-10772][Streaming][Scala]: NullPointerException when transform function in DStream returns NULL
                </div><div><b>body:</b> Currently, the `TransformedDStream` will using `Some(transformFunc(parentRDDs, validTime))` as compute return value, when the `transformFunc` somehow returns null as return value, the followed operator will have NullPointerExeception.

This fix uses the `Option()` instead of `Some()` to deal with the possible null value. When   `transformFunc` returns `null`, the option will transform null to `None`, the downstream can handle `None` correctly.  

NOTE (2015-09-25): The latest fix will check the return value of transform function, if it is `NULL`, a spark exception will be thrown out

                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div>
                LGTM. 

              </div></li><li><div>
                I'm not sure this is valid. Looking at how the result of `transform()` is used, `None` means "try computing this again". Why would you want or need to return a `null` RDD? instead of an empty one?

              </div></li><li><div>
                Looking at it again, @srowen is right. Returning `None` makes `getOrCompute` think that no RDDs have been generated for a given time (artifact of the fact that `None` is returned when a map has no value for a key). So this really is case of returning an empty RDD from your function.

              </div></li><li><div>
                You should not be returning in null in the first place from a transform function. May be we need to document this better. At least, in the code, we should check for whether the returned value is null, and throw a SparkException saying "Transform function return null instead of an RDD; this should be avoided . You can return an empty RDD using RDD.empty, if you dont want to return anything. " I think that should be the right solution here. If you think it makes sense, please update the PR accordingly.

              </div></li><li><div>
                I think we'd better document the return value of `compute` function, I saw lots of users in the community they're trying to return `None` instead of `RDD.empty` if there's no data, the definition of this function accepts to return `Option`, but actually `None` will introduce some undefined behaviors. I know it is hard to change the interface, at least we could document the exact meaning of this function to avoid misuse.

              </div></li><li><div>
                Absolutely agreed. This is something I wish I had done it right 3 years ago :(

              </div></li><li><div>
                There exists a case that user needs to skip the operation after the `transform`, especially, the output operation like saveAsXXXFiles (e.g. snapshot the state of updateStateByKey in every 10 batch interval), of course, user can use the `foreachRDD` to achieve this, but needs a little more coding. If `transform` can return `None`, it can make such usage easier.  

About the undefined behaviors, I think `RDD.empty` will also introduce some undefined behavior here because the new RDD has no relational with the upstream RDD (use `rdd.filter(_ =&gt; false)` can avoid this), so the upstream RDD may be not evaluated, if this RDD in state dstream (`updateStateByKey`, `reduceByKeyAndWIndow`) and after some checkpoint, the state may be not right. And creating empty RDD needs to use spark context, the user needs to pay attention for this if the application recover from some checkpoint: needs a redirect way to get the spark context instead of using the spark context directly.

I agree that we can avoid NullPointerException by checking NULL and throwing a spark exception, if we forbid the `None`  usage, I can modify the PR to check NULL. 

              </div></li><li><div>
                @jhu-chang you can write application logic that selectively performs additional operations or not on an RDD depending on whether it's empty. It's just an if statement.

Why would the lineage of the empty RDD matter? indeed, it's better if it's known to be empty without recomputing anything. Why would something not be computed -- the point is that you're deciding how to continue based on the result of some RDD that _was_ computed.

              </div></li><li><div>
                @srowen Yes, user can write his own logical to deal with the case mentioned last post. But user may still try to use the exist API like `saveAsXXXFiles` first rather than start a new one, even though it is very simple for us.   

About the lineage, I totally agree with you: user must know that the side affect of empty rdd,  but as a junior in spark, he/she may not aware of this (Well document may help him/her), In this aspect, return `None` has no difference with empty RDD if he/she knows what he/she does. So if `None` will introduce undefined behavior, the empty RDD also will, just depends how the user uses @jerryshao 

              </div></li><li><div>
                @jhu-chang I still don't see why you say a user can write logic to return a null RDD, but can't write an if statement to handle it? a user who can use `DStream` is surely capable of this. I think it's more surprising if you're allowed to make a transformation that returns nothing, yet write a program that tries to use it.

What is the problem with an empty RDD? Returning an empty RDD is not the same as what you're suggesting. It's an RDD that has nothing in it. It's meaningful to operate on it further. If the application doesn't wish to, it can write logic to do something different for an RDD.

              </div></li><li><div>
                Hi @jhu-chang , can you elaborate at what scenario emptyRDD will introduce undefined behavior, I'm afraid I could not clearly catch what you mentioned. If emptyRDD will introduce unexpected behavior, I think it is a bug should be fixed. But I can assure you `None` will bring in problems, so this patch should be changed.

              </div></li><li><div>
                @jerryshao This is an extreme sample, it is not a real case, just to demonstrate the issue of RDD.empty, you can see the dependency increasing all the time and final stack over flow will happen (You can say it is usage error). Do you know which kind of operations/dstreams will introduce undefined behavior with `None` and what's the appearance? 

```
ssc.socketTextStream("localhost", 9999).map(x =&gt; (x,1)).updateStateByKey(
    (inputs : Seq[Int], oldr : Option[Int]) =&gt; {
           inputs.headOption
      }
      ).transform(rdd =&gt; {
           println(rdd.toDebugString.split("\n").length)
           sparkContext.makeRDD(Seq[Int]())
      }).print
```

Anyway, I will change the PR to throw exception. 

P.S. The [queueStream](https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala) will return `None` on some cases, do we need to JIRA to track it? 

              </div></li><li><div>
                I cannot remember very clearly, `count()` may introduce incorrect result when you use `None` instead of `RDD.empty` as I remembered.

In the early version some operators actually return `None` instead of `RDD.empty`, that's why this interface is designed like this. But `None` will introduce some problem, I remembered TD fixed several this kind of bugs by changing to empty RDD instead of None. TD might better know this issue. 

              </div></li><li><div>
                `count` with `None` will report NullPointerException since `None` will transform to `null` in [`transform` function](https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala). Actually, all the functions which uses transform function will face the same issue. 

@srowen  The message has been changed, could you check it again?

              </div></li><li><div>
                Lgtm

              </div></li><li><div>
                  [Test build #1824 has finished](https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/1824/console) for   PR 8881 at commit [`d068000`](https://github.com/apache/spark/commit/d068000a234bf47f7d5a38c8a7474ad66c87e086).
- This patch **fails Scala style tests**.
- This patch merges cleanly.
- This patch adds no public classes.

              </div></li><li><div>
                  [Test build #1833 has finished](https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/1833/console) for   PR 8881 at commit [`cba60ed`](https://github.com/apache/spark/commit/cba60ed77e1c4812617667f5d1d3e73e588e9f96).
- This patch **fails Scala style tests**.
- This patch merges cleanly.
- This patch adds no public classes.

              </div></li><li><div>
                @jhu-chang can you fix up the style problem that fails the build? looks like whitespace at the end of the lines

              </div></li><li><div>
                @jhu-chang Could you fix the style issue and one minor issue that I pointed out. 

Style issues:

```
[error] /home/jenkins/workspace/NewSparkPullRequestBuilder/streaming/src/test/scala/org/apache/spark/streaming/BasicOperationsSuite.scala:213:0: Whitespace at end of line
[error] /home/jenkins/workspace/NewSparkPullRequestBuilder/streaming/src/test/scala/org/apache/spark/streaming/BasicOperationsSuite.scala:221:10: Whitespace at end of line

```

              </div></li><li><div>
                Hi, @tdas, I have checked in the fix for those issues, could you check again? 

              </div></li><li><div>
                LGTM except some small comments :).

              </div></li><li><div>
                  [Test build #1865 has finished](https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/1865/console) for   PR 8881 at commit [`0d660ce`](https://github.com/apache/spark/commit/0d660ce1c8ad953b20ffc78f5056e701f3e45e21).
- This patch **fails MiMa tests**.
- This patch merges cleanly.
- This patch adds no public classes.

              </div></li><li><div>
                  [Test build #1866 has finished](https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/1866/console) for   PR 8881 at commit [`2cc4fab`](https://github.com/apache/spark/commit/2cc4faba0da2f8a137ab3c00ce9da32cbf37126e).
- This patch **fails MiMa tests**.
- This patch merges cleanly.
- This patch adds no public classes.

              </div></li><li><div>
                Hi, @srowen , @tdas @jerryshao 
Do you know what's the reason of this failure? 

              </div></li><li><div>
                It might be the problem of Jenkins, you'd better run the unit test again.

              </div></li><li><div>
                On my local environment, the unit test is ok. 

              </div></li><li><div>
                I mean you need to trigger the Jenkins test again by typing "Jenkins, retest this please." in Github comment box.

              </div></li><li><div>
                Or let the committer trigger the test for you if you don't have such permission.

              </div></li><li><div>
                Jenkins, retest this please.

              </div></li><li><div>
                  [Test build #1871 has finished](https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/1871/console) for   PR 8881 at commit [`2cc4fab`](https://github.com/apache/spark/commit/2cc4faba0da2f8a137ab3c00ce9da32cbf37126e).
- This patch **passes all tests**.
- This patch merges cleanly.
- This patch adds no public classes.

              </div></li><li><div>
                Merged to master

              </div></li></ol></div><div><b>github_pulls_reviews:</b> <ol><li><div>
                nit: The formatting here is pretty weird. Maybe take the transformation method out and separate it out as a `def`, so this method call can be in a single line:

```
def transformMethod(r: DStream[Int]): Option[RDD] = {
  r.transform { rdd =&gt; 
    if (rdd != null &amp;&amp; !rdd.isEmpty()) rdd.map(_.toString) else null
  }
}

testOperation(input, transformMethod, input.filter(!_.isEmpty).map(_.map(_.toString)), 4, false)
```

              </div></li><li><div>
                I don't have a strong opinion on this, but this could be done with `require`. Generates a different exception -- `IllegalArgumentException`, but a lower-level generic exception seems pretty reasonable.

Can I suggest tightening the text to something like "Transform function may not return null. Return RDD.empty to return no elements as the result of the transformation."

              </div></li><li><div>
                I would say "must not return null". Saying "may not" is a little ambiguous.

              </div></li><li><div>
                May not relevant to this PR, it would be better to rearrange the import ordering.

              </div></li><li><div>
                "Return RDD.empty instead to represent no element as the result of transformation" ?

              </div></li><li><div>
                Do you mean to import the scala lib first and others in alpha order?

              </div></li><li><div>
                Yes, you could refer to other source codes to change the import ordering like that.

              </div></li><li><div>
                The `SparkException` was in the right place; it's really alpha by package, and then alpha within the package. At least, that's the convention. But the `scala` import can stand alone in a section above these.

              </div></li><li><div>
                I am sooo sorry for the correction, but its not RDD.empty, but `SparkContext.emptyRDD()`. Can you fix it? Everything else is LGTM.

              </div></li><li><div>
                I reverted the import and move the scala import before the spark import. @srowen @jerryshao could you please check if it is ok? 

              </div></li><li><div>
                @tdas I corrected the message, could you please check again?

              </div></li></ol></div><div><b>jira_issues:</b> <ol></ol></div><div><b>jira_issues_comments:</b> <ol></ol></div></div></html>