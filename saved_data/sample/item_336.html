<!DOCTYPE html><html><div class="item-title">
        Item 336
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                *
   * Number of cores to be used for block sort
   
              </div></li><li><div>
                *
   * Min value of number of cores to be used for block sort
   
              </div></li><li><div>
                *
   * Max value of number of cores to be used for block sort
   
              </div></li><li><div>
                *
   * Default value of number of cores to be used for block sort
   
              </div></li><li><div>
                *
   * This method validates the number cores specified for mdk block sort
   
              </div></li><li><div>
                *
   * thread pool size to be used for block sort
   
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> add configure "carbon.number.of.cores.block.sort" (#497)
                </div><div><b>message:</b> add configure "carbon.number.of.cores.block.sort" (#497)

add configure "carbon.number.of.cores.block.sort"
                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> [CARBONDATA-591] Remove unused code for dataype conversion for spark 2.x
                </div><div><b>body:</b> - Remove unused code for dataype conversion for spark 2.x
- Run all unit test cases 
- Manually debug the code for unused code
                </div><div><b>label:</b> code-design
                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div>
                Can one of the admins verify this patch?
              </div></li></ol></div><div><b>github_pulls_reviews:</b> <ol></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> Remove unused code for spark 2.0 datatype utils
                </div><div><b>description:</b> Remove unused code for data type utils for spark 2.0. I look the below code snippet and debug that there is code for spark 2.x datatype conversion in  DataTypeConverterUtil.scala.

{code:title=DataTypeConverterUtil.scala|theme=FadeToGrey|linenumbers=true|language=html/xml|firstline=0001|collapse=true}
  def convertToCarbonTypeForSpark2(dataType: String): DataType = {
    dataType.toLowerCase match {
      case "stringtype" =&gt; DataType.STRING
      case "inttype" =&gt; DataType.INT
      case "integertype" =&gt; DataType.INT
      case "tinyinttype" =&gt; DataType.SHORT
      case "shorttype" =&gt; DataType.SHORT
      case "longtype" =&gt; DataType.LONG
      case "biginttype" =&gt; DataType.LONG
      case "numerictype" =&gt; DataType.DOUBLE
      case "doubletype" =&gt; DataType.DOUBLE
      case "decimaltype" =&gt; DataType.DECIMAL
      case "timestamptype" =&gt; DataType.TIMESTAMP
      case "datetype" =&gt; DataType.DATE
      case "arraytype" =&gt; DataType.ARRAY
      case "structtype" =&gt; DataType.STRUCT
      case _ =&gt; sys.error(s"Unsupported data type: $dataType")
    }
}
{code}

In spark 2.x there is types stringtype and inttype etc as a API not in the query itself.
                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol></ol></div></div></html>