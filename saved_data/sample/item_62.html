<!DOCTYPE html><html><div class="item-title">
        Item 62
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License. You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> KAFKA-6782: solved the bug of restoration of aborted messages for GlobalStateStore and KGlobalTable (#4900)
                </div><div><b>message:</b> KAFKA-6782: solved the bug of restoration of aborted messages for GlobalStateStore and KGlobalTable (#4900)

Reviewer: Matthias J. Sax &lt;matthias@confluent.io&gt;, Bill Bejeck &lt;bill@confluent.io&gt;, Guozhang Wang &lt;guozhang@confluent.io&gt;

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> KAFKA-6782: solved the bug of restoration of aborted messages for GlobalStateStore and KGlobalTable
                </div><div><b>body:</b> 
                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div>
                \cc @bbejeck @vvcephei 
              </div></li><li><div>
                @Gitomain what is the status of this PR -- seems there are still couple of comments you did not address yet.
              </div></li><li><div>
                Hello, I have deleted all the .gitignore files and added a new test GlobalKTableEOSIntegrationTest to test the EOS case independently.
              </div></li><li><div>
                There is a recent commit that moves which package `Consumed` belongs, causing the jenkins to fail:

```
17:02:22 /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.11/streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableEOSIntegrationTest.java:26: error: cannot find symbol
17:02:22 import org.apache.kafka.streams.Consumed;
17:02:22                                ^
17:02:22   symbol:   class Consumed
17:02:22   location: package org.apache.kafka.streams
17:02:25 /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.11/streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableEOSIntegrationTest.java:114: error: cannot find symbol
17:02:25         globalTable = builder.globalTable(globalTableTopic, Consumed.with(Serdes.Long(), Serdes.String()),
17:02:25                                                             ^
17:02:25   symbol:   variable Consumed
17:02:25   location: class GlobalKTableEOSIntegrationTest
17:02:25 /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.11/streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableEOSIntegrationTest.java:118: error: cannot find symbol
17:02:25         final Consumed&lt;String, Long&gt; stringLongConsumed = Consumed.with(Serdes.String(), Serdes.Long());
17:02:25               ^
17:02:25   symbol:   class Consumed
17:02:25   location: class GlobalKTableEOSIntegrationTest
17:02:25 /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.11/streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableEOSIntegrationTest.java:118: error: cannot find symbol
17:02:25         final Consumed&lt;String, Long&gt; stringLongConsumed = Consumed.with(Serdes.String(), Serdes.Long());
```

Could you rebase your branch on latest trunk head and merge any conflicts?
              </div></li><li><div>
                For the motivation of extracting the EOS test into a separate class, I think both the EOS enabled and the EOS disabled cases should be tested. And I found it was a little messy to test the 2 cases in the same class. So I extracted the EOS test into a separate class.
              </div></li><li><div>
                @Gitomain why include `log4j` files for connect and tools?
              </div></li><li><div>
                @Gitomain Code freeze for 2.0 release is on Tuesday (6/12). Can you finish this PR so we can get it into the release? (If not, I might just take it and apply the last changes myself to merged it.) Please let us know.
              </div></li><li><div>
                Hello @mjsax , I have done some modification. But I have no time to extract shared code into an abstract class before 6/12. I think we can finish this PR now.
              </div></li><li><div>
                The deadline was pushed out by one day. I think we can merge as-is. The code rewrites to Java8 are not too important.
              </div></li><li><div>
                Thanks for the fix @Gitomain 

Merged to `trunk` and cherry-picked to `2.0`, `1.1`, `1.0`, and `0.11.0`.
              </div></li></ol></div><div><b>github_pulls_reviews:</b> <ol><li><div>
                Please avoid unnecessary reformatting -- there is more in this PR -- guess, it's a matter of IDE settings? 
              </div></li><li><div>
                I think it would be better, to have two test -- one with EOS enabled and one without.
              </div></li><li><div>
                Why this change? We don't need the variable `topology`?
              </div></li><li><div>
                nit: in tests, it better to just declare `throws Exception` -- it does not provide value to spell out the different types in test and introduces "noise"

I realized, that the existing code also does declare different types of exception. Would you mind to clean this up, too?
              </div></li><li><div>
                nit: add `final` (we try to use `final` whenever possible)
              </div></li><li><div>
                nit: formatting -- parameters should be aligned
              </div></li><li><div>
                nit: add `final`
              </div></li><li><div>
                nit: exceptions
              </div></li><li><div>
                nit: the name is a little miss leading -- aborted message should not be restored...

It also seems, this test only checks if the restore terminates -- I am wondering, if we should also write committed data into the topic and check that the messages got restored? Not sure if necessary.
              </div></li><li><div>
                Yes, it's a matter of IDE settings, we don't need these chages.
              </div></li><li><div>
                nit: formatting - align parameters
              </div></li><li><div>
                Are those changes intended? Or are they IDE enforced?
              </div></li><li><div>
                I think it's a problem of IDE
              </div></li><li><div>
                nit: formatting
              </div></li><li><div>
                with Java 8 support, I think we can remove generics here
              </div></li><li><div>
                please address this comment, too
              </div></li><li><div>
                do we need a `flush()` here? `abortTransaction()` is a sync call 
              </div></li><li><div>
                this line can be removed
              </div></li><li><div>
                do we need the timeout? `waitForCondition` already applies a timeout
              </div></li><li><div>
                this method should enable EOS by default for this class, shouldn't it?
              </div></li><li><div>
                we can merge this method into the one above as we always want EOS enabled?
              </div></li><li><div>
                Just one meta-comment, now that Streams supports Java 8 do we want to covert the use of anonymous class usage here and other places to lambda expressions?  This comment is very opinionated, and this PR has been going on for a while now, so I wouldn't hold up merging for this.  
              </div></li><li><div>
                Hello, I can't understand how can we remove generics here, so I didn't modify this part.
              </div></li><li><div>
                Thanks for remind, it's done 
              </div></li><li><div>
                Hello, I know what you mean. But I have seen that all the test codes are still in this style now, I prefer to leave it as it is, what do you think ?
              </div></li><li><div>
                Java8 should be able to infer the type and Java7 was dropped recently. So we want to update the code to Java8 incrementally. You should be able to change this to:
```
Materialized.as(globalStore)
```
              </div></li><li><div>
                As mentioned in the other comment. Java7 was just dropped recently and thus most code is still Java7. We want to incrementally rewrite to Java8. Thus, feel free to update the code accordingly.
              </div></li><li><div>
                @Gitomain  We can add `*.class` but we need to keep `classes` as some folders are named like this. Can you update the PR? Otherwise I can fix during merging.
              </div></li></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> GlobalKTable GlobalStateStore never finishes restoring when consuming aborted messages
                </div><div><b>description:</b> Same problem with https://issues.apache.org/jira/browse/KAFKA-6190, but his solution which is below, works for the succeed transactional messages. But when there are aborted messages, it will be in infinite loop.&nbsp;Here is his proposition :
{code:java}
while (offset &lt; highWatermark) {
 final ConsumerRecords&lt;byte[], byte[]&gt; records = consumer.poll(100);
 for (ConsumerRecord&lt;byte[], byte[]&gt; record : records) {
 if (record.key() != null) {
   stateRestoreCallback.restore(record.key(), record.value());
 }
 offset = consumer.position(topicPartition);
 }
 }{code}
Concretely, when the consumer consume a set of aborted messages,&nbsp;it polls&nbsp;0 records, and the code&nbsp;'offset = consumer.position(topicPartition)' doesn't have any opportunity to execute.

&nbsp;So I propose to move the code 'offset = consumer.position(topicPartition)' outside of the cycle to guarantee that event if no records are polled, the offset can always be updated.
{code:java}
while (offset &lt; highWatermark) {
 final ConsumerRecords&lt;byte[], byte[]&gt; records = consumer.poll(100);
 for (ConsumerRecord&lt;byte[], byte[]&gt; record : records) {
 if (record.key() != null) {
   stateRestoreCallback.restore(record.key(), record.value());
 }
 }
 offset = consumer.position(topicPartition);
 }{code}
&nbsp;
                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol><li><div>
                Thanks for reporting this issue. Make sense to me -- feel free to open an PR. I would assume that it affects GlobalKTables, too? Thus, we should add tests for both cases.
              </div></li><li><div>
                Yes, I think GlobalKTables will have the same problem. I'm trying to add PR and test for it.
              </div></li><li><div>
                Gitomain opened a new pull request #4900: KAFKA-6782: solved the bug of restoration of aborted messages for GlobalStateStore and KGlobalTable
URL: https://github.com/apache/kafka/pull/4900
 
 
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li><li><div>
                mjsax closed pull request #4900: KAFKA-6782: solved the bug of restoration of aborted messages for GlobalStateStore and KGlobalTable
URL: https://github.com/apache/kafka/pull/4900
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/.gitignore b/.gitignore
index 04f8feed0ad..fe191eed44b 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,5 +1,6 @@
 dist
 *classes
+*.class
 target/
 build/
 build_eclipse/
diff --git a/kafka b/kafka
new file mode 160000
index 00000000000..cc43e77bbbf
--- /dev/null
+++ b/kafka
@@ -0,0 +1 @@
+Subproject commit cc43e77bbbfad71883011186de55603c936cbcd1
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java
index e8ec5e9fe5f..96064b6d4ad 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java
@@ -268,8 +268,8 @@ private void restoreState(final StateRestoreCallback stateRestoreCallback,
                         if (record.key() != null) {
                             restoreRecords.add(KeyValue.pair(record.key(), record.value()));
                         }
-                        offset = globalConsumer.position(topicPartition);
                     }
+                    offset = globalConsumer.position(topicPartition);
                     stateRestoreAdapter.restoreAll(restoreRecords);
                     stateRestoreListener.onBatchRestored(topicPartition, storeName, offset, restoreRecords.size());
                     restoreCount += restoreRecords.size();
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableEOSIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableEOSIntegrationTest.java
new file mode 100644
index 00000000000..f7c0e55c05e
--- /dev/null
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableEOSIntegrationTest.java
@@ -0,0 +1,390 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.integration;
+
+import kafka.utils.MockTime;
+import org.apache.kafka.clients.consumer.ConsumerConfig;
+import org.apache.kafka.clients.producer.ProducerConfig;
+import org.apache.kafka.common.serialization.LongSerializer;
+import org.apache.kafka.common.serialization.Serdes;
+import org.apache.kafka.common.serialization.StringSerializer;
+import org.apache.kafka.common.utils.Bytes;
+import org.apache.kafka.streams.kstream.Consumed;
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.KeyValue;
+import org.apache.kafka.streams.StreamsBuilder;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.errors.InvalidStateStoreException;
+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;
+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;
+import org.apache.kafka.streams.kstream.ForeachAction;
+import org.apache.kafka.streams.kstream.GlobalKTable;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KeyValueMapper;
+import org.apache.kafka.streams.kstream.Materialized;
+import org.apache.kafka.streams.kstream.ValueJoiner;
+import org.apache.kafka.streams.state.KeyValueStore;
+import org.apache.kafka.streams.state.QueryableStoreTypes;
+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;
+import org.apache.kafka.test.IntegrationTest;
+import org.apache.kafka.test.TestCondition;
+import org.apache.kafka.test.TestUtils;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Properties;
+
+@Category({IntegrationTest.class})
+public class GlobalKTableEOSIntegrationTest {
+    private static final int NUM_BROKERS = 1;
+    private static final Properties BROKER_CONFIG;
+    static {
+        BROKER_CONFIG = new Properties();
+        BROKER_CONFIG.put("transaction.state.log.replication.factor", (short) 1);
+        BROKER_CONFIG.put("transaction.state.log.min.isr", 1);
+    }
+
+    @ClassRule
+    public static final EmbeddedKafkaCluster CLUSTER =
+            new EmbeddedKafkaCluster(NUM_BROKERS, BROKER_CONFIG);
+
+    private static volatile int testNo = 0;
+    private final MockTime mockTime = CLUSTER.time;
+    private final KeyValueMapper&lt;String, Long, Long&gt; keyMapper = new KeyValueMapper&lt;String, Long, Long&gt;() {
+        @Override
+        public Long apply(final String key, final Long value) {
+            return value;
+        }
+    };
+    private final ValueJoiner&lt;Long, String, String&gt; joiner = new ValueJoiner&lt;Long, String, String&gt;() {
+        @Override
+        public String apply(final Long value1, final String value2) {
+            return value1 + "+" + value2;
+        }
+    };
+    private final String globalStore = "globalStore";
+    private final Map&lt;String, String&gt; results = new HashMap&lt;&gt;();
+    private StreamsBuilder builder;
+    private Properties streamsConfiguration;
+    private KafkaStreams kafkaStreams;
+    private String globalTableTopic;
+    private String streamTopic;
+    private GlobalKTable&lt;Long, String&gt; globalTable;
+    private KStream&lt;String, Long&gt; stream;
+    private ForeachAction&lt;String, String&gt; foreachAction;
+
+    @Before
+    public void before() throws InterruptedException {
+        testNo++;
+        builder = new StreamsBuilder();
+        createTopics();
+        streamsConfiguration = new Properties();
+        final String applicationId = "globalTableTopic-table-eos-test-" + testNo;
+        streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);
+        streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
+        streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
+        streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);
+        streamsConfiguration.put(IntegrationTestUtils.INTERNAL_LEAVE_GROUP_ON_CLOSE, true);
+        streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100);
+        streamsConfiguration.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, "exactly_once");
+        globalTable = builder.globalTable(globalTableTopic, Consumed.with(Serdes.Long(), Serdes.String()),
+                                          Materialized.&lt;Long, String, KeyValueStore&lt;Bytes, byte[]&gt;&gt;as(globalStore)
+                                                  .withKeySerde(Serdes.Long())
+                                                  .withValueSerde(Serdes.String()));
+        final Consumed&lt;String, Long&gt; stringLongConsumed = Consumed.with(Serdes.String(), Serdes.Long());
+        stream = builder.stream(streamTopic, stringLongConsumed);
+        foreachAction = new ForeachAction&lt;String, String&gt;() {
+            @Override
+            public void apply(final String key, final String value) {
+                results.put(key, value);
+            }
+        };
+    }
+
+    @After
+    public void whenShuttingDown() throws IOException {
+        if (kafkaStreams != null) {
+            kafkaStreams.close();
+        }
+        IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);
+    }
+
+    @Test
+    public void shouldKStreamGlobalKTableLeftJoin() throws Exception {
+        final KStream&lt;String, String&gt; streamTableJoin = stream.leftJoin(globalTable, keyMapper, joiner);
+        streamTableJoin.foreach(foreachAction);
+        produceInitialGlobalTableValues();
+        startStreams();
+        produceTopicValues(streamTopic);
+
+        final Map&lt;String, String&gt; expected = new HashMap&lt;&gt;();
+        expected.put("a", "1+A");
+        expected.put("b", "2+B");
+        expected.put("c", "3+C");
+        expected.put("d", "4+D");
+        expected.put("e", "5+null");
+
+        TestUtils.waitForCondition(new TestCondition() {
+            @Override
+            public boolean conditionMet() {
+                return results.equals(expected);
+            }
+        }, 30000L, "waiting for initial values");
+
+
+        produceGlobalTableValues();
+
+        final ReadOnlyKeyValueStore&lt;Long, String&gt; replicatedStore = kafkaStreams.store(globalStore, QueryableStoreTypes.&lt;Long, String&gt;keyValueStore());
+
+        TestUtils.waitForCondition(new TestCondition() {
+            @Override
+            public boolean conditionMet() {
+                return "J".equals(replicatedStore.get(5L));
+            }
+        }, 30000, "waiting for data in replicated store");
+        produceTopicValues(streamTopic);
+
+        expected.put("a", "1+F");
+        expected.put("b", "2+G");
+        expected.put("c", "3+H");
+        expected.put("d", "4+I");
+        expected.put("e", "5+J");
+
+        TestUtils.waitForCondition(new TestCondition() {
+            @Override
+            public boolean conditionMet() {
+                return results.equals(expected);
+            }
+        }, 30000L, "waiting for final values");
+    }
+
+    @Test
+    public void shouldKStreamGlobalKTableJoin() throws Exception {
+        final KStream&lt;String, String&gt; streamTableJoin = stream.join(globalTable, keyMapper, joiner);
+        streamTableJoin.foreach(foreachAction);
+        produceInitialGlobalTableValues();
+        startStreams();
+        produceTopicValues(streamTopic);
+
+        final Map&lt;String, String&gt; expected = new HashMap&lt;&gt;();
+        expected.put("a", "1+A");
+        expected.put("b", "2+B");
+        expected.put("c", "3+C");
+        expected.put("d", "4+D");
+
+        TestUtils.waitForCondition(new TestCondition() {
+            @Override
+            public boolean conditionMet() {
+                return results.equals(expected);
+            }
+        }, 30000L, "waiting for initial values");
+
+
+        produceGlobalTableValues();
+
+        final ReadOnlyKeyValueStore&lt;Long, String&gt; replicatedStore = kafkaStreams.store(globalStore, QueryableStoreTypes.&lt;Long, String&gt;keyValueStore());
+
+        TestUtils.waitForCondition(new TestCondition() {
+            @Override
+            public boolean conditionMet() {
+                return "J".equals(replicatedStore.get(5L));
+            }
+        }, 30000, "waiting for data in replicated store");
+
+        produceTopicValues(streamTopic);
+
+        expected.put("a", "1+F");
+        expected.put("b", "2+G");
+        expected.put("c", "3+H");
+        expected.put("d", "4+I");
+        expected.put("e", "5+J");
+
+        TestUtils.waitForCondition(new TestCondition() {
+            @Override
+            public boolean conditionMet() {
+                return results.equals(expected);
+            }
+        }, 30000L, "waiting for final values");
+    }
+
+    @Test
+    public void shouldRestoreTransactionalMessages() throws Exception {
+        produceInitialGlobalTableValues();
+
+        startStreams();
+
+        final Map&lt;Long, String&gt; expected = new HashMap&lt;&gt;();
+        expected.put(1L, "A");
+        expected.put(2L, "B");
+        expected.put(3L, "C");
+        expected.put(4L, "D");
+
+        TestUtils.waitForCondition(new TestCondition() {
+            @Override
+            public boolean conditionMet() {
+                ReadOnlyKeyValueStore&lt;Long, String&gt; store = null;
+                try {
+                    store = kafkaStreams.store(globalStore, QueryableStoreTypes.&lt;Long, String&gt;keyValueStore());
+                } catch (InvalidStateStoreException ex) {
+                    return false;
+                }
+                Map&lt;Long, String&gt; result = new HashMap&lt;&gt;();
+                Iterator&lt;KeyValue&lt;Long, String&gt;&gt; it = store.all();
+                while (it.hasNext()) {
+                    KeyValue&lt;Long, String&gt; kv = it.next();
+                    result.put(kv.key, kv.value);
+                }
+                return result.equals(expected);
+            }
+        }, 30000L, "waiting for initial values");
+    }
+    
+    @Test
+    public void shouldNotRestoreAbortedMessages() throws Exception {
+        produceAbortedMessages();
+        produceInitialGlobalTableValues();
+        produceAbortedMessages();
+
+        startStreams();
+        
+        final Map&lt;Long, String&gt; expected = new HashMap&lt;&gt;();
+        expected.put(1L, "A");
+        expected.put(2L, "B");
+        expected.put(3L, "C");
+        expected.put(4L, "D");
+
+        TestUtils.waitForCondition(new TestCondition() {
+            @Override
+            public boolean conditionMet() {
+                ReadOnlyKeyValueStore&lt;Long, String&gt; store = null;
+                try {
+                    store = kafkaStreams.store(globalStore, QueryableStoreTypes.&lt;Long, String&gt;keyValueStore());
+                } catch (InvalidStateStoreException ex) {
+                    return false;
+                }
+                Map&lt;Long, String&gt; result = new HashMap&lt;&gt;();
+                Iterator&lt;KeyValue&lt;Long, String&gt;&gt; it = store.all();
+                while (it.hasNext()) {
+                    KeyValue&lt;Long, String&gt; kv = it.next();
+                    result.put(kv.key, kv.value);
+                }
+                return result.equals(expected);
+            }
+        }, 30000L, "waiting for initial values");
+    }
+
+    private void createTopics() throws InterruptedException {
+        streamTopic = "stream-" + testNo;
+        globalTableTopic = "globalTable-" + testNo;
+        CLUSTER.createTopics(streamTopic);
+        CLUSTER.createTopic(globalTableTopic, 2, 1);
+    }
+    
+    private void startStreams() {
+        kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);
+        kafkaStreams.start();
+    }
+
+    private void produceTopicValues(final String topic) throws Exception {
+        IntegrationTestUtils.produceKeyValuesSynchronously(
+                topic,
+                Arrays.asList(
+                        new KeyValue&lt;&gt;("a", 1L),
+                        new KeyValue&lt;&gt;("b", 2L),
+                        new KeyValue&lt;&gt;("c", 3L),
+                        new KeyValue&lt;&gt;("d", 4L),
+                        new KeyValue&lt;&gt;("e", 5L)),
+                TestUtils.producerConfig(
+                        CLUSTER.bootstrapServers(),
+                        StringSerializer.class,
+                        LongSerializer.class,
+                        new Properties()),
+                mockTime);
+    }
+
+    private void produceAbortedMessages() throws Exception {
+        final Properties properties = new Properties();
+        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "someid");
+        properties.put(ProducerConfig.RETRIES_CONFIG, 1);
+        IntegrationTestUtils.produceAbortedKeyValuesSynchronouslyWithTimestamp(
+                globalTableTopic, Arrays.asList(
+                        new KeyValue&lt;&gt;(1L, "A"),
+                        new KeyValue&lt;&gt;(2L, "B"),
+                        new KeyValue&lt;&gt;(3L, "C"),
+                        new KeyValue&lt;&gt;(4L, "D")
+                        ), 
+                TestUtils.producerConfig(
+                                CLUSTER.bootstrapServers(),
+                                LongSerializer.class,
+                                StringSerializer.class,
+                                properties),
+                mockTime.milliseconds());
+    }
+
+    private void produceInitialGlobalTableValues() throws Exception {
+        produceInitialGlobalTableValues(true);
+    }
+
+    private void produceInitialGlobalTableValues(final boolean enableTransactions) throws Exception {
+        final Properties properties = new Properties();
+        if (enableTransactions) {
+            properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "someid");
+            properties.put(ProducerConfig.RETRIES_CONFIG, 1);
+        }
+        IntegrationTestUtils.produceKeyValuesSynchronously(
+                globalTableTopic,
+                Arrays.asList(
+                        new KeyValue&lt;&gt;(1L, "A"),
+                        new KeyValue&lt;&gt;(2L, "B"),
+                        new KeyValue&lt;&gt;(3L, "C"),
+                        new KeyValue&lt;&gt;(4L, "D")
+                        ),
+                TestUtils.producerConfig(
+                        CLUSTER.bootstrapServers(),
+                        LongSerializer.class,
+                        StringSerializer.class,
+                        properties),
+                mockTime,
+                enableTransactions);
+    }
+
+    private void produceGlobalTableValues() throws Exception {
+        IntegrationTestUtils.produceKeyValuesSynchronously(
+                globalTableTopic,
+                Arrays.asList(
+                        new KeyValue&lt;&gt;(1L, "F"),
+                        new KeyValue&lt;&gt;(2L, "G"),
+                        new KeyValue&lt;&gt;(3L, "H"),
+                        new KeyValue&lt;&gt;(4L, "I"),
+                        new KeyValue&lt;&gt;(5L, "J")),
+                TestUtils.producerConfig(
+                        CLUSTER.bootstrapServers(),
+                        LongSerializer.class,
+                        StringSerializer.class,
+                        new Properties()),
+                mockTime);
+    }
+}
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableIntegrationTest.java
index 8c6a30a5972..900e65276ee 100644
--- a/streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableIntegrationTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableIntegrationTest.java
@@ -18,7 +18,6 @@
 
 import kafka.utils.MockTime;
 import org.apache.kafka.clients.consumer.ConsumerConfig;
-import org.apache.kafka.clients.producer.ProducerConfig;
 import org.apache.kafka.common.serialization.LongSerializer;
 import org.apache.kafka.common.serialization.Serdes;
 import org.apache.kafka.common.serialization.StringSerializer;
@@ -28,7 +27,6 @@
 import org.apache.kafka.streams.KeyValue;
 import org.apache.kafka.streams.StreamsBuilder;
 import org.apache.kafka.streams.StreamsConfig;
-import org.apache.kafka.streams.errors.InvalidStateStoreException;
 import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;
 import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;
 import org.apache.kafka.streams.kstream.ForeachAction;
@@ -52,23 +50,16 @@
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.HashMap;
-import java.util.Iterator;
 import java.util.Map;
 import java.util.Properties;
 
 @Category({IntegrationTest.class})
 public class GlobalKTableIntegrationTest {
     private static final int NUM_BROKERS = 1;
-    private static final Properties BROKER_CONFIG;
-    static {
-        BROKER_CONFIG = new Properties();
-        BROKER_CONFIG.put("transaction.state.log.replication.factor", (short) 1);
-        BROKER_CONFIG.put("transaction.state.log.min.isr", 1);
-    }
 
     @ClassRule
     public static final EmbeddedKafkaCluster CLUSTER =
-            new EmbeddedKafkaCluster(NUM_BROKERS, BROKER_CONFIG);
+            new EmbeddedKafkaCluster(NUM_BROKERS);
 
     private static volatile int testNo = 0;
     private final MockTime mockTime = CLUSTER.time;
@@ -229,46 +220,14 @@ public boolean conditionMet() {
             }
         }, 30000L, "waiting for final values");
     }
-
-    @Test
-    public void shouldRestoreTransactionalMessages() throws Exception {
-        produceInitialGlobalTableValues(true);
-        startStreams();
-
-        final Map&lt;Long, String&gt; expected = new HashMap&lt;&gt;();
-        expected.put(1L, "A");
-        expected.put(2L, "B");
-        expected.put(3L, "C");
-        expected.put(4L, "D");
-
-        TestUtils.waitForCondition(new TestCondition() {
-            @Override
-            public boolean conditionMet() {
-                ReadOnlyKeyValueStore&lt;Long, String&gt; store = null;
-                try {
-                    store = kafkaStreams.store(globalStore, QueryableStoreTypes.&lt;Long, String&gt;keyValueStore());
-                } catch (InvalidStateStoreException ex) {
-                    return false;
-                }
-                Map&lt;Long, String&gt; result = new HashMap&lt;&gt;();
-                Iterator&lt;KeyValue&lt;Long, String&gt;&gt; it = store.all();
-                while (it.hasNext()) {
-                    KeyValue&lt;Long, String&gt; kv = it.next();
-                    result.put(kv.key, kv.value);
-                }
-                return result.equals(expected);
-            }
-        }, 30000L, "waiting for initial values");
-        System.out.println("no failed test");
-    }
-
+    
     private void createTopics() throws InterruptedException {
         streamTopic = "stream-" + testNo;
         globalTableTopic = "globalTable-" + testNo;
         CLUSTER.createTopics(streamTopic);
         CLUSTER.createTopic(globalTableTopic, 2, 1);
     }
-
+    
     private void startStreams() {
         kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);
         kafkaStreams.start();
@@ -292,29 +251,20 @@ private void produceTopicValues(final String topic) throws Exception {
     }
 
     private void produceInitialGlobalTableValues() throws Exception {
-        produceInitialGlobalTableValues(false);
-    }
-
-    private void produceInitialGlobalTableValues(final boolean enableTransactions) throws Exception {
-        Properties properties = new Properties();
-        if (enableTransactions) {
-            properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "someid");
-            properties.put(ProducerConfig.RETRIES_CONFIG, 1);
-        }
         IntegrationTestUtils.produceKeyValuesSynchronously(
                 globalTableTopic,
                 Arrays.asList(
                         new KeyValue&lt;&gt;(1L, "A"),
                         new KeyValue&lt;&gt;(2L, "B"),
                         new KeyValue&lt;&gt;(3L, "C"),
-                        new KeyValue&lt;&gt;(4L, "D")),
+                        new KeyValue&lt;&gt;(4L, "D")
+                        ),
                 TestUtils.producerConfig(
                         CLUSTER.bootstrapServers(),
                         LongSerializer.class,
-                        StringSerializer.class,
-                        properties),
-                mockTime,
-                enableTransactions);
+                        StringSerializer.class
+                        ),
+                mockTime);
     }
 
     private void produceGlobalTableValues() throws Exception {
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java b/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java
index fe897c7ac30..441549dca77 100644
--- a/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java
@@ -179,16 +179,38 @@ public static void purgeLocalStreamsState(final Properties streamsConfiguration)
             producer.flush();
         }
     }
+    
+    public static &lt;K, V&gt; void produceAbortedKeyValuesSynchronouslyWithTimestamp(final String topic,
+                                                                                final Collection&lt;KeyValue&lt;K, V&gt;&gt; records,
+                                                                                final Properties producerConfig,
+                                                                                final Long timestamp)
+        throws ExecutionException, InterruptedException {
+        try (final Producer&lt;K, V&gt; producer = new KafkaProducer&lt;&gt;(producerConfig)) {
+            producer.initTransactions();
+            for (final KeyValue&lt;K, V&gt; record : records) {
+                producer.beginTransaction();
+                final Future&lt;RecordMetadata&gt; f = producer
+                        .send(new ProducerRecord&lt;&gt;(topic, null, timestamp, record.key, record.value));
+                f.get();
+                producer.abortTransaction();
+            }
+        }    
+    }
 
-    public static &lt;V&gt; void produceValuesSynchronously(
-        final String topic, final Collection&lt;V&gt; records, final Properties producerConfig, final Time time)
+    public static &lt;V&gt; void produceValuesSynchronously(final String topic,
+                                                      final Collection&lt;V&gt; records,
+                                                      final Properties producerConfig,
+                                                      final Time time)
         throws ExecutionException, InterruptedException {
         IntegrationTestUtils.produceValuesSynchronously(topic, records, producerConfig, time, false);
     }
 
-    public static &lt;V&gt; void produceValuesSynchronously(
-        final String topic, final Collection&lt;V&gt; records, final Properties producerConfig, final Time time, final boolean enableTransactions)
-        throws ExecutionException, InterruptedException {
+    public static &lt;V&gt; void produceValuesSynchronously(final String topic,
+                                                      final Collection&lt;V&gt; records,
+                                                      final Properties producerConfig,
+                                                      final Time time,
+                                                      final boolean enableTransactions)
+            throws ExecutionException, InterruptedException {
         final Collection&lt;KeyValue&lt;Object, V&gt;&gt; keyedRecords = new ArrayList&lt;&gt;();
         for (final V value : records) {
             final KeyValue&lt;Object, V&gt; kv = new KeyValue&lt;&gt;(null, value);
@@ -240,10 +262,9 @@ public static void waitForCompletion(final KafkaStreams streams,
     public static &lt;K, V&gt; List&lt;KeyValue&lt;K, V&gt;&gt; waitUntilMinKeyValueRecordsReceived(final Properties consumerConfig,
                                                                                   final String topic,
                                                                                   final int expectedNumRecords) throws InterruptedException {
-
         return waitUntilMinKeyValueRecordsReceived(consumerConfig, topic, expectedNumRecords, DEFAULT_TIMEOUT);
     }
-
+    
     /**
      * Wait until enough data (key-value records) has been consumed.
      *


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

              </div></li></ol></div></div></html>