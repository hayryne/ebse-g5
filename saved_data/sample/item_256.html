<!DOCTYPE html><html><div class="item-title">
        Item 256
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                 test 
              </div></li><li><div>
                 dependencies are always listed in sorted order by groupId, artifectId 
 intra-project 
              </div></li><li><div>
                
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.

              </div></li><li><div>
                 plugins are always listed in sorted order by groupId, artifectId 
              </div></li><li><div>
                
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
   * Creates a new record updater for the new batch
   * @param minTxnId smallest Txnid in the batch
   * @param maxTxnID largest Txnid in the batch
   * @throws StreamingIOFailure if failed to create record updater
   
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                * Constructor. Uses default separator of the LazySimpleSerde
  * @param colNamesForFields Column name assignment for input fields. nulls or empty
  *                          strings in the array indicates the fields to be skipped
  * @param delimiter input field delimiter
  * @param endPoint Hive endpoint
  * @param conf a Hive conf object. Can be null if not using advanced hive settings.
  * @throws ConnectionError Problem talking to Hive
  * @throws ClassNotFoundException Serde class not found
  * @throws SerializationError Serde initialization/interaction failed
  * @throws StreamingException Problem acquiring file system path for partition
  * @throws InvalidColumn any element in colNamesForFields refers to a non existing column
  
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
   * Constructor. Allows overriding separator of the LazySimpleSerde
   * @param colNamesForFields Column name assignment for input fields
   * @param delimiter input field delimiter
   * @param endPoint Hive endpoint
   * @param conf a Hive conf object. Set to null if not using advanced hive settings.
   * @param serdeSeparator separator used when encoding data that is fed into the
   *                             LazySimpleSerde. Ensure this separator does not occur
   *                             in the field data
   * @throws ConnectionError Problem talking to Hive
   * @throws ClassNotFoundException Serde class not found
   * @throws SerializationError Serde initialization/interaction failed
   * @throws StreamingException Problem acquiring file system path for partition
   * @throws InvalidColumn any element in colNamesForFields refers to a non existing column
   
              </div></li><li><div>
                *
 * Streaming Writer handles delimited input (eg. CSV).
 * Delimited input is parsed &amp; reordered to match column order in table
 * Uses Lazy Simple Serde to process delimited input
 
              </div></li><li><div>
                * Constructor. Uses default separator of the LazySimpleSerde
   * @param colNamesForFields Column name assignment for input fields. nulls or empty
   *                          strings in the array indicates the fields to be skipped
   * @param delimiter input field delimiter
   * @param endPoint Hive endpoint
   * @throws ConnectionError Problem talking to Hive
   * @throws ClassNotFoundException Serde class not found
   * @throws SerializationError Serde initialization/interaction failed
   * @throws StreamingException Problem acquiring file system path for partition
   * @throws InvalidColumn any element in colNamesForFields refers to a non existing column
   
              </div></li><li><div>
                 Reorder fields in record based on the order of columns in the table
              </div></li><li><div><div><b>comment:</b>  handles nulls in items[]
 TODO: perhaps can be made more efficient by creating a byte[] directly
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                *
   * Creates LazySimpleSerde
   * @return
   * @throws SerializationError if serde could not be initialized
   * @param tbl
   
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
   * Acquire a new connection to MetaStore for streaming
   * @param createPartIfNotExists If true, the partition specified in the endpoint
   *                              will be auto created if it does not exist
   * @param conf HiveConf object, set it to null if not using advanced hive settings.
   * @return
   * @throws ConnectionError if problem connecting
   * @throws InvalidPartition  if specified partition is not valid (createPartIfNotExists = false)
   * @throws ImpersonationFailed  if not able to impersonate 'proxyUser'
   * @throws IOException  if there was an I/O error when acquiring connection
   * @throws PartitionCreationFailed if failed to create partition
   * @throws InterruptedException
   
              </div></li><li><div>
                *
     * Abort the currently open transaction
     * @throws TransactionError
     
              </div></li><li><div>
                *
     *  Write records using RecordWriter
     * @param records collection of rows to be written
     * @throws StreamingException  serialization error
     * @throws ImpersonationFailed error writing on behalf of proxyUser
     * @throws InterruptedException
     
              </div></li><li><div>
                *
 * Information about the hive end point (i.e. table or partition) to write to.
 * A light weight object that does NOT internally hold on to resources such as
 * network connections. It can be stored in Hashed containers such as sets and hash tables.
 
              </div></li><li><div>
                *
     * get state of current tramsaction
     * @return
     
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
     * Acquires a new batch of transactions from Hive.
     *
     * @param numTransactions is a hint from client indicating how many transactions client needs.
     * @param recordWriter  Used to write record. The same writer instance can
     *                      be shared with another TransactionBatch (to the same endpoint)
     *                      only after the first TransactionBatch has been closed.
     *                      Writer will be closed when the TransactionBatch is closed.
     * @return
     * @throws StreamingIOFailure if failed to create new RecordUpdater for batch
     * @throws TransactionBatchUnAvailable if failed to acquire a new Transaction batch
     * @throws ImpersonationFailed failed to run command as proxyUser
     * @throws InterruptedException
     
              </div></li><li><div>
                *
     *  Write record using RecordWriter
     * @param record  the data to be written
     * @throws StreamingIOFailure I/O failure
     * @throws SerializationError  serialization error
     * @throws ImpersonationFailed error writing on behalf of proxyUser
     * @throws InterruptedException
     
              </div></li><li><div>
                *
     *
     * @param endPoint end point to connect to
     * @param proxyUser  can be null
     * @param ugi of prody user. If ugi is null, impersonation of proxy user will be disabled
     * @param conf HiveConf object
     * @param createPart create the partition if it does not exist
     * @throws ConnectionError if there is trouble connecting
     * @throws InvalidPartition if specified partition does not exist (and createPart=false)
     * @throws InvalidTable if specified table does not exist
     * @throws PartitionCreationFailed if createPart=true and not able to create partition
     
              </div></li><li><div>
                *
     * Commit the currently open transaction
     * @throws TransactionError
     * @throws StreamingIOFailure  if flushing records failed
     * @throws ImpersonationFailed if
     * @throws InterruptedException
     
              </div></li><li><div>
                 # of times to retry if first attempt fails
              </div></li><li><div>
                *
   *
   * @param metaStoreUri   URI of the metastore to connect to eg: thrift://localhost:9083
   * @param database       Name of the Hive database
   * @param table          Name of table to stream to
   * @param partitionVals  Indicates the specific partition to stream to. Can be null or empty List
   *                       if streaming to a table without partitions. The order of values in this
   *                       list must correspond exactly to the order of partition columns specified
   *                       during the table creation. E.g. For a table partitioned by
   *                       (continent string, country string), partitionVals could be the list
   *                       ("Asia", "India").
   
              </div></li><li><div>
                *
     * Represents a batch of transactions acquired from MetaStore
     *
     * @param proxyUser
     * @param ugi
     * @param endPt
     * @param numTxns
     * @param msClient
     * @param recordWriter
     * @throws StreamingException if failed to create new RecordUpdater for batch
     * @throws TransactionBatchUnAvailable if failed to acquire a new Transaction batch
     
              </div></li><li><div>
                 class HiveEndPoint
              </div></li><li><div>
                 for
              </div></li><li><div>
                *
     * Close the TransactionBatch
     * @throws StreamingIOFailure I/O failure when closing transaction batch
     
              </div></li><li><div>
                *
     * Close connection
     
              </div></li><li><div>
                 class TransactionBatchImpl
              </div></li><li><div>
                *
     * Activate the next available transaction in the current transaction batch
     * @throws TransactionError failed to switch to next transaction
     
              </div></li><li><div>
                 class ConnectionImpl
              </div></li><li><div>
                *
     * Get Id of currently open transaction
     * @return
     
              </div></li><li><div>
                *
   * Acquire a new connection to MetaStore for streaming
   * @param proxyUser User on whose behalf all hdfs and hive operations will be
   *                  performed on this connection. Set it to null or empty string
   *                  to connect as user of current process without impersonation.
   *                  Currently this argument is not supported and must be null
   * @param createPartIfNotExists If true, the partition specified in the endpoint
   *                              will be auto created if it does not exist
   * @return
   * @throws ConnectionError if problem connecting
   * @throws InvalidPartition  if specified partition is not valid (createPartIfNotExists = false)
   * @throws ImpersonationFailed  if not able to impersonate 'proxyUser'
   * @throws IOException  if there was an I/O error when acquiring connection
   * @throws PartitionCreationFailed if failed to create partition
   * @throws InterruptedException
   
              </div></li><li><div>
                *
     * Remaining transactions are the ones that are not committed or aborted or active.
     * Active transaction is not considered part of remaining txns.
     * @return number of transactions remaining this batch.
     
              </div></li><li><div>
                *
   * Acquire a new connection to MetaStore for streaming
   * @param createPartIfNotExists If true, the partition specified in the endpoint
   *                              will be auto created if it does not exist
   * @return
   * @throws ConnectionError if problem connecting
   * @throws InvalidPartition  if specified partition is not valid (createPartIfNotExists = false)
   * @throws ImpersonationFailed  if not able to impersonate 'proxyUser'
   * @throws IOException  if there was an I/O error when acquiring connection
   * @throws PartitionCreationFailed if failed to create partition
   * @throws InterruptedException
   
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                * Flush records from buffer. Invoked by TransactionBatch.commit() 
              </div></li><li><div>
                * Writes using a hive RecordUpdater
   *
   * @param transactionId the ID of the Txn in which the write occurs
   * @param record the record to be written
   
              </div></li><li><div>
                * Clear bufferred writes. Invoked by TransactionBatch.abort() 
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                * Close the RecordUpdater. Invoked by TransactionBatch.close() 
              </div></li><li><div>
                * Acquire a new RecordUpdater. Invoked when
   * StreamingConnection.fetchTransactionBatch() is called 
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
   * Close connection
   
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
 * Represents a connection to a HiveEndPoint. Used to acquire transaction batches.
 
              </div></li><li><div>
                *
   * Acquires a new batch of transactions from Hive.

   * @param numTransactionsHint is a hint from client indicating how many transactions client needs.
   * @param writer  Used to write record. The same writer instance can
   *                      be shared with another TransactionBatch (to the same endpoint)
   *                      only after the first TransactionBatch has been closed.
   *                      Writer will be closed when the TransactionBatch is closed.
   * @return
   * @throws ConnectionError
   * @throws InvalidPartition
   * @throws StreamingException
   * @return a batch of transactions
   
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
   * Creates JsonSerDe
   * @param tbl   used to create serde
   * @param conf  used to create serde
   * @return
   * @throws SerializationError if serde could not be initialized
   
              </div></li><li><div>
                *
 * Streaming Writer handles utf8 encoded Json (Strict syntax).
 * Uses org.apache.hive.hcatalog.data.JsonSerDe to process Json input
 
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
   * Encode Utf8 encoded string bytes using JsonSerde
   * @param utf8StrRecord
   * @return  The encoded object
   * @throws SerializationError
   
              </div></li><li><div>
                *
   *
   * @param endPoint the end point to write to
   * @throws ConnectionError
   * @throws SerializationError
   * @throws StreamingException
   
              </div></li><li><div>
                *
   *
   * @param endPoint the end point to write to
   * @param conf a Hive conf object. Should be null if not using advanced Hive settings.
   * @throws ConnectionError
   * @throws SerializationError
   * @throws StreamingException
   
              </div></li><li><div>
                *
   * Commit the currently open transaction
   * @throws StreamingException if there are errors committing
   * @throws InterruptedException if call in interrupted
   
              </div></li><li><div>
                *
   *  Write records using RecordWriter
   * @throws StreamingException if there are errors when writing
   * @throws InterruptedException if call in interrupted
   
              </div></li><li><div>
                *
   * get state of current transaction
   
              </div></li><li><div>
                *
   * Issues a heartbeat to hive metastore on the current and remaining txn ids
   * to keep them from expiring
   * @throws StreamingException if there are errors
   
              </div></li><li><div>
                *
   * Activate the next available transaction in the current transaction batch
   * @throws StreamingException if not able to switch to next Txn
   * @throws InterruptedException if call in interrupted
   
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
 * Represents a set of Transactions returned by Hive. Supports opening, writing to
 * and commiting/aborting each transaction. The interface is designed to ensure
 * transactions in a batch are used up sequentially. Multiple transaction batches can be
 * used (initialized with separate RecordWriters) for concurrent streaming
 *
 
              </div></li><li><div>
                *
   *  Write record using RecordWriter
   * @param record  the data to be written
   * @throws StreamingException if there are errors when writing
   * @throws InterruptedException if call in interrupted
   
              </div></li><li><div>
                *
   * Remaining transactions are the ones that are not committed or aborted or open.
   * Current open transaction is not considered part of remaining txns.
   * @return number of transactions remaining this batch.
   
              </div></li><li><div>
                *
   * Get Id of currently open transaction
   * @return transaction id
   
              </div></li><li><div>
                *
   * Close the TransactionBatch
   * @throws StreamingException if there are errors closing batch
   * @throws InterruptedException if call in interrupted
   
              </div></li><li><div>
                *
   * Abort the currently open transaction
   * @throws StreamingException if there are errors
   * @throws InterruptedException if call in interrupted
   
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                *
 * A stand alone utility to write data into the streaming ingest interface.
 
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div><div><b>comment:</b>  TODO make it so I can randomize the column order
                </div><div><b>label:</b> requirement
                </div></div></li><li><div>
                2)  test reordering
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                 should throw
              </div></li><li><div>
                1)  test dropping fields - first middle  &amp; last
              </div></li><li><div>
                4)  test few field names
              </div></li><li><div><div><b>comment:</b> 3)  test bad field names
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                show throw
              </div></li><li><div><div><b>comment:</b> 5)  test extra field names
                </div><div><b>label:</b> test
                </div></div></li><li><div>
                 while
              </div></li><li><div>
                 1) to partitioned table
              </div></li><li><div>
                 Create partition
              </div></li><li><div>
                 1) Basic
              </div></li><li><div>
                 Defining partition names in unsorted order
              </div></li><li><div>
                 To Unpartitioned table
              </div></li><li><div>
                 Ensure partition is present
              </div></li><li><div>
                 Acquire 2nd Txn Batch
              </div></li><li><div>
                 data should not be visible
              </div></li><li><div>
                 drop and recreate the necessary databases and tables
              </div></li><li><div>
                 2nd Txn
              </div></li><li><div>
                *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 
              </div></li><li><div>
                 2) Leave partition unspecified
              </div></li><li><div>
                 get close enough
              </div></li><li><div>
                 should not throw
              </div></li><li><div>
                 Acquire 1st Txn Batch
              </div></li><li><div>
                2) obtain metastore clients
              </div></li><li><div>
                 Interleaved writes to both batches
              </div></li><li><div>
                 2) to unpartitioned table
              </div></li><li><div>
                 1st Txn
              </div></li><li><div>
                 partitioned table
              </div></li><li><div>
                 Ensure partition is absent
              </div></li><li><div>
                 2) test with txn.Abort()
              </div></li><li><div>
                 delete db and all tables in it
              </div></li><li><div>
                1) Start from a clean slate (metastore)
              </div></li><li><div>
                 unpartitioned table
              </div></li><li><div>
                 expect this exception
              </div></li><li><div>
                 1)  to partitioned table
              </div></li><li><div>
                 2nd Txn Batch
              </div></li><li><div>
                shouldn't throw
              </div></li><li><div>
                 2) To unpartitioned table
              </div></li><li><div>
                 1) test with txn.Commit()
              </div></li><li><div>
                 find the absolute mininum transaction
              </div></li><li><div>
                !/bin/sh
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> HIVE-5687 Streaming support in Hive (Roshan Naik via gates)
                </div><div><b>message:</b> HIVE-5687 Streaming support in Hive (Roshan Naik via gates)


git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1586189 13f79535-47bb-0310-9956-ffa450edef68

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol></ol></div><div><b>github_pulls_comments:</b> <ol></ol></div><div><b>github_pulls_reviews:</b> <ol></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> Streaming support in Hive
                </div><div><b>description:</b> Implement support for Streaming data into HIVE.
 - Provide a client streaming API
 - Transaction support: Clients should be able to periodically commit a batch of records atomically
 - Immediate visibility: Records should be immediately visible to queries on commit
 - Should not overload HDFS with too many small files

Use Cases:
 - Streaming logs `into HIVE via Flume
 - Streaming results of computations from Storm
                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol><li><div>
                Attaching draft api spec for comments
              </div></li><li><div>
                revising draft spec
              </div></li><li><div>
                Initial patch  (depends upon HIVE-5843  &amp; HIVE-6060)
              </div></li><li><div>
                spec updated to match first draft patch
              </div></li><li><div>
                Revising API in patch and Spec to handle mapping of incoming data format to corresponding cols in table (RecordWriter interface). Adding out of the box support for Delimited text formats. More formats are pluggable.

Added support for auto creation of new partitions for streaming clients
              </div></li><li><div><div><b>body:</b> fixing typos in spec v4
                </div><div><b>label:</b> documentation
                </div></div></li><li><div>
                updating patch v2 with minor tweaks
              </div></li><li><div>
                A few comments:
* Right now you're building one lock request and re-using it.  That won't work.  A new lock needs to be constructed with each transaction and then associated with that transaction so that the transaction manager knows to release the lock when the transaction is committed or aborted.  This should be done in beginNextTxn().
* The partition name is currently being built incorrectly.  It is just using the values.  It should be constructed using Warehouse.makePartName.
* The lock components are being constructed incorrectly.  You are building a component for every key/value pair in the partition.  You should only build one component for each partition you want to lock.  So in your case, each lock request will have exactly one lock component.
* The file is being written to the table location instead of the partition location.  When I run this with table foo and partition bar I get files in /hive/warehouse/foo instead of /hive/warehouse/foo/bar
* The metaStoreClient is being prematurely closed.  It certainly shouldn't be closed in createPartition.  I'm not sure it should be closed at all.

              </div></li><li><div><div><b>body:</b> - Addressed review comments from Alan. 
- Tweaked the APIs
- Wrote Java docs
- Added heartbeat support
- Improved log messages
- More tests
- Fixes for multiple bugs found during unit testing and manual testing
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> Adding design &amp; spec documentation for v3 patch
                </div><div><b>label:</b> documentation
                </div></div></li><li><div>
                Link to review board  https://reviews.apache.org/r/19754/
              </div></li><li><div>
                v4 patch .. Adding JSON writer suport, tweaks to JavaDocs.
Updated pdf Document
              </div></li><li><div>
                You also need this patch to fix the unit tests so that they don't use the pfile or LocalFileSystem, which don't support flush.
              </div></li><li><div>
                v5 patch addresses  Owen's comments - fixes for unit test issues
              </div></li><li><div>
                refreshing patch v5 with minor fix to compile with hadoop1 profile
              </div></li><li><div>
                After discussing this with [~ashutoshc] I think this should move from hive/streaming to hive/hcatalog/streaming.  This is a non-SQL interface for Hive, and that is the area HCatalog covers.  This should be a simple refactor and shouldn't require a re-review.
              </div></li><li><div><div><b>body:</b> Initial comments
* Please create a package-info.java (or package.html) for the entire package that has the text from the design document, but without the example.
* I believe the API will go through some iterations and use before it becomes stable. We should warn users that it will likely evolve in future versions of Hive and won't necessarily be backwards compatible. The package-info.java is probably the best place to place the warning.
* The current API requires users to implement a RecordWriter wrapper for each SerDe they want to use. In Hive 0.14, I think we need to revisit this and switch to just requiring a serde class name and a string to string map of serde properties. This way, any of a user's current SerDes can be used to parse the byte[] and there can be a generic method for constructing the object using reflection.
* The code shouldn't depend on OrcOutputFormat, but instead find the OutputFormat of the table/partition and use that. The streaming code should only require that it implement AcidOutputFormat.
* The RecordWriter should be passed the HiveConf rather than create it. It will make it easier to do unit tests.
* The StreamingIntegrationTester needs to print the exception's getMessage to stderr if the options don't parse correctly. Otherwise, the user doesn't get any clue as to which parameter they forgot.
* I don't see how the column reordering can be invoken. The SerDe is using the table properties from the table in the MetaStore to define the columns it returns, so the two should always be the same. My suggestion is to remove all of the column reordering code.
* If you don't remove the column ordering code, you should deserialize and then reorder the columns rather than the current strategy of deserialize, reorder, serialize, and deserialize.
* Revert the change that adds startMetaStore. It isn't called and thus shouldn't be added.
* The method writeImpl(byte[]) doesn't add any value and should just be inlined.
* Why do you use DDL to create partitions rather than the MetaStoreClient API that you everywhere else?

Some style guidelines:
* Please split the lines that are longer than 100 characters and it is even better if they are less than 80 characters.
* Ensure your if statements have a space before the parenthesis.
* Remove the commented out code.
* Please remove the private uncalled functions (eg. HiveEndPoint.newConnection(String proxyUser, ...) )
* You've defined a lot of exceptions in this API. Is the user expected to handle each exception separately? Your throw declarations don't list the exact exceptions that are thrown. You'd be better off with different exceptions only when the user is expected to be able to handle a specific error. Otherwise, you might as well use StreamingException with a descriptive error message for everything.

                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> To add to Owen's style guidelines thing: Just throwing this patch in my IDE gives me a lot of warnings and errors.

Things like:
* missorted modifiers (static final private -&gt; private static final)
* Unnecessary package-level visibility
* Redundant exceptions in throws clauses
* Some very weird formatting
* Call to simple getters from within class
* for loop without initializer that can be a while loop
* Unused variables
* Conditions that are always true or false
* Empty Javadoc tags
* Unnecessary "this"
* Missing @Override annotations
* StringBuffer usage
* Modifiers in interfaces (public)

etc.

I'm happy to do a full review on ReviewBoard but these are all things that Eclipse and IntelliJ can show you out of the box. So I'd appreciate it if you could set your IDE up to show these things and fix them in addition to using proper code formatting. Contact me if I can help in any way.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Addressing review comments from Alan, Owen and some of Lars.

Owen: DDL was used there mostly for convenience and correctness. The other places where API is used, cannot be accomplished via DDL.
              </div></li><li><div>
                Thanks, could you put a new version up on RB?
              </div></li><li><div><div><b>body:</b> Remove the hcatalog/streaming/src/docs/package.html and put this file into hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/package.html.

I've removed all of the complicated formatting and non-standard characters that Microsoft Word added. It is important for open source projects to have documentation that can be edited. It is also better to include this documentation as part of the javadoc and have links to the API's javadoc rather than reproduce it.

Other than replacing the documentation, it is ok. +1
                </div><div><b>label:</b> documentation
                </div></div></li><li><div>
                Owen: Thanks a lot for revising package.html
              </div></li><li><div>
                I had posted the revised patch on RB
              </div></li><li><div>
                patch v7 using package.html from Owen and fixing a bug in packaging
              </div></li><li><div>
                Committed to both trunk and branch 0.13.  Thanks Roshan for all the hard work on this.
              </div></li><li><div>
                Would you support live hive query on streaming data like esper , CEP in this or future patch  or we would  just have an API to persist data in hadoop ?
              </div></li><li><div><div><b>body:</b> [~roshan_naik], were my comments on the review board too late?  Most of them were trivial edits of the javadocs, but several seemed worth fixing.  (For example, some methods' javadocs got an exception name wrong.)  Maybe I should have raised issues, but I figured you're the best judge of which changes should be made.
                </div><div><b>label:</b> documentation
                </div></div></li><li><div>
                [~leftylev] Yes looks like it went unnoticed due to the short time frame. For some reason i never got a notification of your review. We can get it in via another patch... but it appears to be too late to get it into this release.

[~orahive] You can query the data while it is being streamed into Hive. Queries will always see a consistent view of the data as this feature relies on the new ACID support in Hive. So queries will not see new data that was committed after they began executing. 

FLUME-1734 consumes this API to implement a Flume sink that streams data continuously into Hive.
              </div></li><li><div>
                I don't understand why this was rushed. There were only a couple of hours to review the final patch.
              </div></li><li><div>
                [~leftylev] sorry, this is my fault.  I meant to file a JIRA to address yours and Lars style comments and forgot.  I'll do that shortly.
[~lars_francke] the latest patch was only different from the one Owen +1'd in a few small packaging details.  Sorry if that wasn't clear.  I pushed it in because I know Harish is anxious to get a release candidate for 0.13 and this was one of the last blockers.  
              </div></li><li><div>
                File HIVE-6885 to address the style and docs feedback.
              </div></li><li><div>
                Thanks Alan for the follow-up!
              </div></li><li><div>
                All's well.  Thanks Alan.
              </div></li><li><div>
                updating 'Hive Streaming Ingest API for v4 patch.pdf'
  document with requirements
              </div></li></ol></div></div></html>