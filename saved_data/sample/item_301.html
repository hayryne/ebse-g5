<!DOCTYPE html><html><div class="item-title">
        Item 301
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                 buffered reader
              </div></li><li><div>
                 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
 distributed with this work for additional information
 regarding copyright ownership.  The ASF licenses this file
 to you under the Apache License, Version 2.0 (the
 "License"); you may not use this file except in compliance
 with the License.  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing,
 software distributed under the License is distributed on an
 "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 KIND, either express or implied.  See the License for the
 specific language governing permissions and limitations
 under the License.
              </div></li><li><div>
                not support
              </div></li><li><div>
                EOF
              </div></li><li><div>
                 if requested length is larger than the capacity of buffer, do not
 need to copy the character into local buffer.
              </div></li><li><div>
                 namespace doris
              </div></li><li><div>
                 fill the buffer
              </div></li><li><div>
                 retry for new content
              </div></li><li><div>
                 requested bytes missed the local buffer
              </div></li><li><div>
                 EOF
              </div></li><li><div>
                 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
 distributed with this work for additional information
 regarding copyright ownership.  The ASF licenses this file
 to you under the Apache License, Version 2.0 (the
 "License"); you may not use this file except in compliance
 with the License.  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing,
 software distributed under the License is distributed on an
 "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 KIND, either express or implied.  See the License for the
 specific language governing permissions and limitations
 under the License.
              </div></li><li><div>
                 Read 
              </div></li><li><div><div><b>comment:</b>  Buffered Reader
 Add a cache layer between the caller and the file reader to reduce the 
 times of calls to the read function to speed up. 
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                 If the reader need the file size, set it when construct FileReader.
 There is no other way to set the file size.
              </div></li><li><div>
                 Seek to the end of the file
              </div></li><li><div>
                 Seek to a wrong position
              </div></li><li><div>
                 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
 distributed with this work for additional information
 regarding copyright ownership.  The ASF licenses this file
 to you under the Apache License, Version 2.0 (the
 "License"); you may not use this file except in compliance
 with the License.  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing,
 software distributed under the License is distributed on an
 "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 KIND, either express or implied.  See the License for the
 specific language governing permissions and limitations
 under the License.
              </div></li><li><div>
                 end namespace doris
              </div></li><li><div>
                 Seek to the beginning of the file
              </div></li><li><div>
                 buffered_reader_test_file.txt 45 bytes
              </div></li><li><div>
                 if requested length is larger than the capacity of buffer, do not
 need to copy the character into local buffer.
              </div></li><li><div>
                 buffered_reader_test_file 950 bytes
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> [optimize] Optimize spark load/broker load reading parquet format file (#3878)
                </div><div><b>message:</b> [optimize] Optimize spark load/broker load reading parquet format file (#3878)

Add BufferedReader for reading parquet file via broker
                </div></div></li></ol></div><div><b>github_issues:</b> <ol><li><div><div><b>title:</b> [optimize] Optimize spark load/broker load reading parquet format file
                </div><div><b>body:</b> Currently, broker load support reading parquet file from remote, and soon we will use parquet file as be's loading source in spark load.

But due to the seperated metadata (file meta/column meta/page header...) location of parquet file, broker reader need frequently seek to get data, which leads to a lot of RPCs.  And large amount of RPCs will lead to huge network costs in cross-data-center scene.

You can see a big gap of cost in the parquet reading below between non-cross-data-center and cross-data-center scene.

(The rpc times means how many times per broker seeks during loading)

**spark load**
|cross-center|rpc times|load time|data size|
|----|----|----|----|
|No|15014|60s|560m|
|Yes|16817|2h|560m|
|No|169766|8min|5.8G|
|Yes|150476|14h|5.8G|

---

**broker load**
|cross-center|rpc times|load time|data size|
|----|----|----|----|
|No|51780|2min|250m|
|Yes|51413|42min|250m|

As a proposal, **I suggest to add a cache buffer array in broker reader reading process.** 

**Illustration:**
When a broker about to seek for a position and get data from remote parquet file, try reading with this position in the cache buffer array. Once the expected data hits the cache buffer array,  then we don't bother to read data from remote parquet file.

Our final purpose is to reduce the number of rpc times as much as we could, so I make some testing to validate. 

**Test:**
The test data I used is ssb lineorder, the target table is unpartitioned with 8 buckets.

| Field            | Type        | Null | Key   | Default | Extra   |
|----|----|----|----|----|----|
| lo_orderkey      | BIGINT      | Yes  | true  | N/A     |         |
| lo_linenumber    | BIGINT      | Yes  | true  | N/A     |         |
| lo_custkey       | INT         | Yes  | true  | N/A     |         |
| lo_partkey       | INT         | Yes  | true  | N/A     |         |
| lo_suppkey       | INT         | Yes  | true  | N/A     |         |
| lo_orderdate     | INT         | Yes  | true  | N/A     |         |
| lo_orderpriotity | VARCHAR(16) | Yes  | false | N/A     | REPLACE |
| lo_shippriotity  | INT         | Yes  | false | N/A     | SUM     |
| lo_quantity      | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_extendedprice | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_ordtotalprice | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_discount      | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_revenue       | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_supplycost    | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_tax           | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_commitdate    | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_shipmode      | VARCHAR(11) | Yes  | false | N/A     | REPLACE |

---

**spark load**
|cross-center|rpc times|load time|data size|buffer size|
|----|----|----|----|----|
|Yes|16817|2h|560m|N/A|
|Yes|613|3min|560m|128k|
|Yes|529|3min|560m|512k|
|Yes|285|3min|560m|1m|

|cross-center|rpc times|load time|data size|buffer size|
|----|----|----|----|----|
|No|15014|60s|560m|N/A|
|No|634|33s|560m|128k|
|No|526|29s|560m|512k|
|No|423|27s|560m|1m|

|cross-center|rpc times|load time|data size|buffer size|
|----|----|----|----|----|
|Yes|150476|14h|5.8g|N/A|
|Yes|6547|45min|5.8g|128k|
|Yes|5670|37min|5.8g|512k|
|Yes|5598|41min|5.8g|1m|

|cross-center|rpc times|load time|data size|buffer size|
|----|----|----|----|----|
|No|169766|8min|5.8g|N/A|
|No|6773|5min|5.8g|128k|
|No|5667|4min|5.8g|512k|
|No|5879|5min|5.8g|1m|

---

**broker load**
|cross-center|rpc times|load time|data size|buffer size|
|----|----|----|----|----|
|Yes|51413|42min|250m|N/A|
|Yes|1873|20min|250m|512k|

|cross-center|rpc times|load time|data size|buffer size|
|----|----|----|----|----|
|No|51780|2min|250m|N/A|
|No|1798|55s|250m|512k|

It can be seen that whether it is cross-data-center or not, adding the cache buffer array can reduce the number of RPC and improve the loading performance.

May be there still improve space in the Cache Policy, such as LRU-cache or someting else, we should try that gradually later.

                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>title:</b> [optimize] Optimize spark load/broker load reading parquet format file
                </div><div><b>body:</b> Currently, broker load support reading parquet file from remote, and soon we will use parquet file as be's loading source in spark load.

But due to the seperated metadata (file meta/column meta/page header...) location of parquet file, broker reader need frequently seek to get data, which leads to a lot of RPCs.  And large amount of RPCs will lead to huge network costs in cross-data-center scene.

You can see a big gap of cost in the parquet reading below between non-cross-data-center and cross-data-center scene.

(The rpc times means how many times per broker seeks during loading)

**spark load**
|cross-center|rpc times|load time|data size|
|----|----|----|----|
|No|15014|60s|560m|
|Yes|16817|2h|560m|
|No|169766|8min|5.8G|
|Yes|150476|14h|5.8G|

---

**broker load**
|cross-center|rpc times|load time|data size|
|----|----|----|----|
|No|51780|2min|250m|
|Yes|51413|42min|250m|

As a proposal, **I suggest to add a cache buffer array in broker reader reading process.** 

**Illustration:**
When a broker about to seek for a position and get data from remote parquet file, try reading with this position in the cache buffer array. Once the expected data hits the cache buffer array,  then we don't bother to read data from remote parquet file.

Our final purpose is to reduce the number of rpc times as much as we could, so I make some testing to validate. 

**Test:**
The test data I used is ssb lineorder, the target table is unpartitioned with 8 buckets.

| Field            | Type        | Null | Key   | Default | Extra   |
|----|----|----|----|----|----|
| lo_orderkey      | BIGINT      | Yes  | true  | N/A     |         |
| lo_linenumber    | BIGINT      | Yes  | true  | N/A     |         |
| lo_custkey       | INT         | Yes  | true  | N/A     |         |
| lo_partkey       | INT         | Yes  | true  | N/A     |         |
| lo_suppkey       | INT         | Yes  | true  | N/A     |         |
| lo_orderdate     | INT         | Yes  | true  | N/A     |         |
| lo_orderpriotity | VARCHAR(16) | Yes  | false | N/A     | REPLACE |
| lo_shippriotity  | INT         | Yes  | false | N/A     | SUM     |
| lo_quantity      | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_extendedprice | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_ordtotalprice | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_discount      | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_revenue       | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_supplycost    | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_tax           | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_commitdate    | BIGINT      | Yes  | false | N/A     | SUM     |
| lo_shipmode      | VARCHAR(11) | Yes  | false | N/A     | REPLACE |

---

**spark load**
|cross-center|rpc times|load time|data size|buffer size|
|----|----|----|----|----|
|Yes|16817|2h|560m|N/A|
|Yes|613|3min|560m|128k|
|Yes|529|3min|560m|512k|
|Yes|285|3min|560m|1m|

|cross-center|rpc times|load time|data size|buffer size|
|----|----|----|----|----|
|No|15014|60s|560m|N/A|
|No|634|33s|560m|128k|
|No|526|29s|560m|512k|
|No|423|27s|560m|1m|

|cross-center|rpc times|load time|data size|buffer size|
|----|----|----|----|----|
|Yes|150476|14h|5.8g|N/A|
|Yes|6547|45min|5.8g|128k|
|Yes|5670|37min|5.8g|512k|
|Yes|5598|41min|5.8g|1m|

|cross-center|rpc times|load time|data size|buffer size|
|----|----|----|----|----|
|No|169766|8min|5.8g|N/A|
|No|6773|5min|5.8g|128k|
|No|5667|4min|5.8g|512k|
|No|5879|5min|5.8g|1m|

---

**broker load**
|cross-center|rpc times|load time|data size|buffer size|
|----|----|----|----|----|
|Yes|51413|42min|250m|N/A|
|Yes|1873|20min|250m|512k|

|cross-center|rpc times|load time|data size|buffer size|
|----|----|----|----|----|
|No|51780|2min|250m|N/A|
|No|1798|55s|250m|512k|

It can be seen that whether it is cross-data-center or not, adding the cache buffer array can reduce the number of RPC and improve the loading performance.

May be there still improve space in the Cache Policy, such as LRU-cache or someting else, we should try that gradually later.

                </div></div></li></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> [optimize] Optimize spark load/broker load reading parquet format file
                </div><div><b>body:</b> Please see the description in #3877 .
spark load #3433 
                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div>
                Great job
              </div></li><li><div><div><b>body:</b> @xy720 Hi, Please add the UT for new BufferedReader class, Thanks.
                </div><div><b>label:</b> code-design
                </div></div></li></ol></div><div><b>github_pulls_reviews:</b> <ol><li><div>
                Please add the comment to describe why do we need this class, and when should we use it.
              </div></li><li><div>
                Let the caller print the log, not here.
              </div></li><li><div>
                ```suggestion
    // If the reader need the file size, set it when construct FileReader.
```
              </div></li><li><div>
                ```suggestion
    Status _fill();
```
              </div></li><li><div>
                ```suggestion
    Status _read_once(int64_t position, int64_t nbytes, int64_t* bytes_read, void* out);
```
              </div></li><li><div>
                Is it necessary to fill() when open()?
              </div></li><li><div>
                What is this retry for? add comment for it.
              </div></li><li><div>
                when will `position &gt; _buffer_limit`?
              </div></li><li><div>
                Yes. If not call fill() in open(), the actual length of the buffer will be 0 when user call read().
              </div></li><li><div>
                ok
              </div></li><li><div>
                done
              </div></li><li><div>
                done
              </div></li><li><div>
                done
              </div></li><li><div>
                ok
              </div></li><li><div><div><b>body:</b> You need to check the result, or the unit test is meaningless
                </div><div><b>label:</b> test
                </div></div></li><li><div>
                check the `buf_len` after `read()`
              </div></li><li><div>
                should it be `nbytes &gt; _buffer_size ` ?
              </div></li><li><div>
                You are right.
              </div></li><li><div>
                what is "readat ?"
              </div></li><li><div>
                &gt; what is "readat ?"

Like `pread`
              </div></li><li><div><div><b>body:</b> Yes, it's confusing.  Better named it as pread request.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> maybe this construct funciton can be remove, for this is duplicated code.
you can add default paremerts to constructor function bellow.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                good
              </div></li></ol></div><div><b>jira_issues:</b> <ol></ol></div><div><b>jira_issues_comments:</b> <ol></ol></div></div></html>