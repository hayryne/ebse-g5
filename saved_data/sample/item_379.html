<!DOCTYPE html><html><div class="item-title">
        Item 379
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                *
   * Get the number of bytes for this column.
   * @return the number of bytes
   
              </div></li><li><div>
                *
   * Get the number of bytes for a file in a givem column.
   * @param column column from which to get file size
   * @return number of bytes for the given column
   
              </div></li><li><div>
                *
   * Get the number of bytes for this column.
   *
   * @return the number of bytes
   
              </div></li><li><div>
                *
   * Get the number of bytes for a file in a given column
   * by finding all the streams (not suppressed)
   * for a given column and returning the sum of their sizes.
   * excludes index
   *
   * @param column column from which to get file size
   * @return number of bytes for the given column
   
              </div></li><li><div>
                *
     * Get the PhysicalWriter.
     *
     * @return the file's physical writer.
     
              </div></li><li><div>
                *
   * Flush the TreeWriter stream
   * @throws IOException
   
              </div></li><li><div>
                 Update byte count 
              </div></li><li><div>
                *
     * Get the PhysicalWriter.
     *
     * @return the file's physical writer.
     
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> ORC-305 - Add column statistics for the size on disk
                </div><div><b>message:</b> ORC-305 - Add column statistics for the size on disk

Fixes #255

Signed-off-by: Owen O'Malley &lt;omalley@apache.org&gt;

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> ORC-305 - Add column statistics for the size on disk
                </div><div><b>body:</b> This PR adds column statistics for the size on disk. 
I have updated the Unit Tests to reflect this change, I have also manually gone through some test cases to verify that the size on disk value is correct. ROW_INDEXs are excluded from the calculation.
                </div></div></li><li><div><div><b>title:</b> ORC-305 - Add column statistics for the size on disk
                </div><div><b>body:</b> This PR adds column statistics for the size on disk. 
I have updated the Unit Tests to reflect this change, I have also manually gone through some test cases to verify that the size on disk value is correct. ROW_INDEXs are excluded from the calculation.
                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol></ol></div><div><b>github_pulls_reviews:</b> <ol></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> Add column statistics for the size on disk
                </div><div><b>description:</b> It would be great to have the size on disk of each column.

You can generate this by adding up the sizes of the dictionary and data streams.

It is only relevant at the stripe and file level.
                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol><li><div>
                So if a stripe has streams like:
{code:java}
{ "id": 0, "column": 0, "kind": "index", "offset": 200000, "length": 7 },
{ "id": 1, "column": 1, "kind": "index", "offset": 200007, "length": 22 },
{ "id": 2, "column": 2, "kind": "index", "offset": 200029, "length": 29 },
{ "id": 3, "column": 3, "kind": "index", "offset": 200058, "length": 39 },
{ "id": 4, "column": 4, "kind": "index", "offset": 200097, "length": 27 },
{ "id": 5, "column": 5, "kind": "index", "offset": 200124, "length": 13 },
{ "id": 6, "column": 1, "kind": "data", "offset": 200137, "length": 54 },
{ "id": 7, "column": 2, "kind": "data", "offset": 200191, "length": 44 },
{ "id": 8, "column": 2, "kind": "length", "offset": 200235, "length": 4 },
{ "id": 9, "column": 2, "kind": "dictionary", "offset": 200239, "length": 8 },
{ "id": 10, "column": 3, "kind": "data", "offset": 200247, "length": 40000 },
{ "id": 11, "column": 4, "kind": "data", "offset": 240247, "length": 5000 },
{ "id": 12, "column": 4, "kind": "secondary", "offset": 245247, "length": 44 },
{ "id": 13, "column": 5, "kind": "data", "offset": 245291, "length": 88 },
{ "id": 14, "column": 5, "kind": "secondary", "offset": 245379, "length": 40 }
{code}

So when updating the column statistics are updated at the stripe, all of the data streams for the column would be added together.
              </div></li><li><div>
                Would you consider adding column raw data size (the size before encoding and compression) as well? This would be useful in a couple of scenarios. For example, query optimizer can use it to estimate input data size, and thus decides degrees of parallelism, join algorithms, etc. Column raw size can be deduced from number of value (or total length from string column). We just need to expose a new interface from ColumnStatistics.
              </div></li><li><div>
                [~xndai] as you say, we can compute this from the statistics we already have. Please create a separate jira for adding the method to the ColumnStatistics interface.
              </div></li><li><div><div><b>body:</b> [~smore] this is harder than I realized.

There are two big problems.

The first is that you need to update the byteCount in the stripeColStatistics before the stripe statistics are saved in the middle of TreeWriterBase.writeStripe around line 250. That unfortunately runs before the TreeWriters have flushed their streams. That is really unfortunate. To fix that issue, I’d suggest that we split the TreeWriter.writeStripe into two parts:

{code}
void flushStreams() throws IOException;
void writeStripe(int requiredIndexEntries) throws IOException;
{code}

So then WriterImpl.flushStripe() will call:

{code}
treeWriter.flushStreams();
treeWriter.writeStripe(requiredIndexEntries);
{code}

For TreeWriterBase, the flushStreams() will have the front part of writeStripe:

{code}
if (isPresent != null) {
  isPresent.flush();

  // if no nulls are found in a stream, then suppress the stream
  if(!foundNulls) {
    isPresentOutStream.suppress();
    // since isPresent bitstream is suppressed, update the index to
    // remove the positions of the isPresent stream
    if (rowIndex != null) {
      removeIsPresentPositions();
    }
  }
}
{code}

For IntegerTreeWriter, the flushStreams will have:

{code}
super.flushStreams();
writer.flush();
{code}

The compound types will also flush their children.

All of that should mean that now the streams are all flushed before we hit the problematic part of saving the stripe statistics.

Now the second problem is how do you actually get the number of bytes in the streams for a column. Unfortunately, the TreeWriters don’t have the stream lengths. You’ll need to add a method in PhysicalWriter that returns the number of bytes in streams for a given column. 

{code}
long getFileBytes(int column);
{code}

The PhysicalFsWriter will need to add an implementation of getFileBytes that finds all of the streams for a given column number, ignores the streams that are suppressed, and returns the sum of the sizes.

Now before TreeWriterBase.writeStripe saves the stripe statistics, use context.getPhysicalWriter().getFileBytes(id) to get the number of bytes for this column for this stripe.

                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Ah, thanks [~owen.omalley] for the details, makes it much clearer.
              </div></li><li><div>
                Hello [~owen.omalley] 
Thanks for the detailed explanation, I am getting closer and have a question about the following line

bq. Now before TreeWriterBase.writeStripe saves the stripe statistics, use context.getPhysicalWriter().getFileBytes(id) to get the number of bytes for this column for this stripe.

I am assuming you mean, use 'context.getPhysicalWriter().getFileBytes(id)'
in WriterImpl, where we can get hold of the PhysicalWriter, the problem here is that I do not have a way to get Column ids (in WriterImpl.flushStripes() ). 

And in the TreeWriterBase, where I do have access to column ids, I don't have access to PhysicalWriter instance, is there any other way I can get access to PhysicalWriter instance here ?

Best,
Sandeep



              </div></li><li><div>
                You have access to the id as TreeWriterBase.id and the PhysicalWriter via TreeWriterBase.streamFactory.getPhysicalWriter(). The name streamFactory is a historical artifact, but is effectively the WriterContext that provides access to the important parts of the WriterImpl.
              </div></li><li><div>
                Looks like the streamFactory does not expose the getPhysicalWriter(), the [WriterContext|https://github.com/apache/orc/blob/ded204a4a10bfad1ed739fc98f612a41005640c5/java/core/src/java/org/apache/orc/impl/writer/WriterContext.java] interface for streamFactory, used by TreeWriterBase does not have the getPhysicalWriter() method. That method is exposed by a different WriterContext, [OrcFile.WriterContext |https://github.com/apache/orc/blob/master/java/core/src/java/org/apache/orc/OrcFile.java#L330] but it can't be accessed by TreeWriterBase class.

One way would be to add getWriter() method to [org.apache.orc.impl.writer.WriterContext|https://github.com/apache/orc/blob/ded204a4a10bfad1ed739fc98f612a41005640c5/java/core/src/java/org/apache/orc/impl/writer/WriterContext.java] class, so we could do 
 TreeWriterBase.streamFactory.getWriter() to get the PhysicalWriter. let me know your thoughts !
              </div></li><li><div>
                Sorry, I was looking at my changes for ORC-14 and forgot that I had added that!

Yeah, go ahead and add the method to WriterContext:
{code}
 /**
  * Get the PhysicalWriter.
  * @return the file's physical writer.
  */
 PhysicalWriter getPhysicalWriter();
{code}

              </div></li><li><div>
                A quick update on my status, I am done with the code code and currently working on fixing the tests.&nbsp;

BTW I am excluding the ROW_INDEX and BLOOM_FILTER_UTF8 from bytes on disk calculation.
              </div></li><li><div>
                [~owen.omalley] do you know best way to update *.orc files under /example folder ? for e.g. how can I update footer for file [https://github.com/apache/orc/blob/master/examples/TestOrcFile.test1.orc], I tried exporting it to json and re-importing it but no luck.
              </div></li><li><div>
                GitHub user moresandeep opened a pull request:

    https://github.com/apache/orc/pull/255

    ORC-305 - Add column statistics for the size on disk

    This PR adds column statistics for the size on disk. 
    I have updated the Unit Tests to reflect this change, I have also manually gone through some test cases to verify that the size on disk value is correct. ROW_INDEXs are excluded from the calculation.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/moresandeep/orc ORC-305-Column_Statistics

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/orc/pull/255.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #255
    
----
commit 688517b0072de56bdabe118a1dcf2346d10d3f48
Author: Sandeep More &lt;more@...&gt;
Date:   2018-03-21T13:46:19Z

    ORC-305 - Add column statistics for the size on disk

----

              </div></li><li><div>
                Github user asfgit closed the pull request at:

    https://github.com/apache/orc/pull/255

              </div></li><li><div>
                I just committed this. Thanks, Sandeep!
              </div></li><li><div>
                Great, thanks !
              </div></li><li><div>
                Released as ORC 1.5.0.
              </div></li></ol></div></div></html>