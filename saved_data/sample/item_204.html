<!DOCTYPE html><html><div class="item-title">
        Item 204
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
 distributed with this work for additional information
 regarding copyright ownership.  The ASF licenses this file
 to you under the Apache License, Version 2.0 (the
 "License"); you may not use this file except in compliance
 with the License.  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing,
 software distributed under the License is distributed on an
 "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 KIND, either express or implied.  See the License for the
 specific language governing permissions and limitations
 under the License.
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>message:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool

https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.

Author: Robert Gruener &lt;robbieg@uber.com&gt;

Closes #2185 from rgruener/thread-pooling and squashes the following commits:

4af3cdc3 &lt;Robert Gruener&gt; ARROW-2656:  Improve the creation time of ParquetManifest for partitioned datasets using a thread pool

                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div><div><b>label:</b> test
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div><div><b>label:</b> requirement
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li><li><div><div><b>title:</b> ARROW-2656: [Python] Improve creation time of ParquetManifest for partitioned datasets using thread pool
                </div><div><b>body:</b> https://issues.apache.org/jira/browse/ARROW-2656 I probably should still write a benchmark for this, but figured I can get some quick feedback on this while working on that.
                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div>
                Test failure was `ImportError: No module named concurrent` which is weird since there is a install_requires on futures in setup.py
              </div></li><li><div>
                @rgruener I guess `python setup.py build_ext` doesn't install [install_requires](https://github.com/apache/arrow/blob/master/python/setup.py#L473) dependencies.

For [OSX build](https://travis-ci.org/apache/arrow/jobs/397460285) the optional dependencies are handled [here](https://github.com/apache/arrow/blob/master/ci/travis_script_python.sh#L97), in this particular build conda fails to dowload, which is unrelated and probably will pass next time.

However for the [linux build](https://travis-ci.org/apache/arrow/jobs/397460286) the optional `futures` dependency for PY27 is indeed [missing](https://github.com/apache/arrow/blob/master/python/manylinux1/build_arrow.sh#L77). I suggest to install it in the same fashion as `travis_script_python.sh` [does](https://github.com/apache/arrow/blob/master/ci/travis_script_python.sh#L97). Hopefully that will resolve the build errors.



              </div></li><li><div>
                @rgruener benchmark probably fails because parquet is disabled [here](https://github.com/apache/arrow/blob/master/ci/travis_script_python.sh#L185)
              </div></li><li><div>
                Cool, updated that. Thanks for all the test help so far @kszucs 
              </div></li><li><div>
                You're welcome, hopefully conda connection won't drop again.
              </div></li><li><div>
                # [Codecov](https://codecov.io/gh/apache/arrow/pull/2185?src=pr&amp;el=h1) Report
&gt; Merging [#2185](https://codecov.io/gh/apache/arrow/pull/2185?src=pr&amp;el=desc) into [master](https://codecov.io/gh/apache/arrow/commit/566e398650b40292b1f6293eb80c8f09d2451f98?src=pr&amp;el=desc) will **increase** coverage by `&lt;.01%`.
&gt; The diff coverage is `100%`.

[![Impacted file tree graph](https://codecov.io/gh/apache/arrow/pull/2185/graphs/tree.svg?width=650&amp;src=pr&amp;token=LpTCFbqVT1&amp;height=150)](https://codecov.io/gh/apache/arrow/pull/2185?src=pr&amp;el=tree)

```diff
@@            Coverage Diff             @@
##           master    #2185      +/-   ##
==========================================
+ Coverage   84.38%   84.38%   +&lt;.01%     
==========================================
  Files         293      293              
  Lines       44784    44807      +23     
==========================================
+ Hits        37789    37812      +23     
  Misses       6964     6964              
  Partials       31       31
```


| [Impacted Files](https://codecov.io/gh/apache/arrow/pull/2185?src=pr&amp;el=tree) | Coverage Δ | |
|---|---|---|
| [python/pyarrow/tests/test\_parquet.py](https://codecov.io/gh/apache/arrow/pull/2185/diff?src=pr&amp;el=tree#diff-cHl0aG9uL3B5YXJyb3cvdGVzdHMvdGVzdF9wYXJxdWV0LnB5) | `97.02% &lt;100%&gt; (+0.03%)` | :arrow_up: |
| [python/pyarrow/parquet.py](https://codecov.io/gh/apache/arrow/pull/2185/diff?src=pr&amp;el=tree#diff-cHl0aG9uL3B5YXJyb3cvcGFycXVldC5weQ==) | `91.27% &lt;100%&gt; (+0.18%)` | :arrow_up: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/2185?src=pr&amp;el=continue).
&gt; **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
&gt; `Δ = absolute &lt;relative&gt; (impact)`, `ø = not affected`, `? = missing data`
&gt; Powered by [Codecov](https://codecov.io/gh/apache/arrow/pull/2185?src=pr&amp;el=footer). Last update [566e398...4af3cdc](https://codecov.io/gh/apache/arrow/pull/2185?src=pr&amp;el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).

              </div></li><li><div><div><b>body:</b> I'd like to understand what is being sped up by this patch. Is it doing I/O on a remote filesystem? If so, should the optimization really live at the parquet level (rather than the filesystem level)?

                </div><div><b>label:</b> code-design
                </div></div></li><li><div><div><b>body:</b> &gt; I'd like to understand what is being sped up by this patch. Is it doing I/O on a remote filesystem? If so, should the optimization really live at the parquet level (rather than the filesystem level)?

Yeah, it is basically for the use case of reading a parquet dataset on a remote filesystem. This is not possible to live at the parquet level given the current code structure and the way directories are sturctured for partitioned parquet datasets. In the case of a partitioned dataset the first call to `_visit_level` will return pretty much only directories from `fs.walk` and it will then make a blocking call for each of those directories serially to read their contents. When dealing with a remote filesystem you are adding up the round trip RPC call for each of these calls.

My own estimates from reading from HDFS in a remote datacenter to my laptop have shown this can improve times by multiple orders of magnitude. The benchmark I included really isnt the best way to measure this since with the local filesystem the overhead of threading is worse than making the calls to the local filesystem however the minimal hit there is more than made up for when reading from remote filesystems. Im not sure how to write a benchmark to a remote filesystem though, but I can share some number of me running it against a remote filesystem.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                In principle I would be interested in pushing down the directory traversal code into C++ (and making it asynchronous / concurrent). This could be some follow up work
              </div></li><li><div>
                If we're parallelizing IO (rather than local computation) then using a custom thread pool sounds ok. But it would be better in the io / filesystem layer than in the parquet layer still, IMHO.
              </div></li><li><div>
                &gt; But it would be better in the io / filesystem layer

I agree. In essence we are doing `os.walk` here but need to perform the HDFS RPC requests in a thread pool. If we wanted to build something generic, we would need to first address some preliminaries like https://issues.apache.org/jira/browse/ARROW-767
              </div></li><li><div><div><b>body:</b> I agree as well that something more generic would be better in the long run but for now opening datasets with many partitions (~10,000) can take minutes (which makes the code seem broken while you're waiting).

&gt; Can you add some unit tests? I think repurposing existing unit tests for partitioned datasets should be sufficient

I added a test `test_create_parquet_manifest_with_thread_pool` to explicitly test passing in a thread pool and reading a partitioned dataset. There seem to be many unit tests already reading partitioned datasets (examples include`test_read_partitioned_directory` and `test_read_partitioned_directory_s3fs` among many others) so I am not sure there is a need to add any others. Unless there is something specific you would like to see added?
                </div><div><b>label:</b> test
                </div></div></li><li><div>
                Any chance this can get another look to get in prior to 0.10?
              </div></li><li><div>
                Yeah, I want to see this get into 0.10. I'll have a closer look today
              </div></li></ol></div><div><b>github_pulls_reviews:</b> <ol><li><div>
                Please add this directly to the requirements file using environment markers (`futures; python_version &lt; 3`)
              </div></li><li><div>
                @pitrou Wouldn't this be a use case for the ThreadPool you have introduced recently?
              </div></li><li><div><div><b>body:</b> I may be missing something, but above `thread_pool` defaults to `None` and I see where `_thread_pool` is set on line 602. What is being `shutdown()` here if `thread_pool` is `None`?
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                If this were C++, then yes ;-)
              </div></li><li><div><div><b>body:</b> Is `_visit_level` even thread-safe?
                </div><div><b>label:</b> requirement
                </div></div></li><li><div>
                Then whit [one](https://github.com/apache/arrow/blob/master/ci/travis_script_python.sh#L97) should be removed too.
              </div></li><li><div><div><b>body:</b> If `thread_pool is None` that means we instantiated it on line 602 and therefore the calling user will not be using that same thread pool so we should properly shut it down to preserve resources. If it wasn't none that means the user passed in a thread pool executor likely to reuse an existing one that already existed.
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Is this benchmark stable from run to run? What kind of numbers does it give?
              </div></li><li><div>
                I believe it should be. It mutates `pieces` which is a python list by using `extend` which should be thread safe thanks to the GIL.
              </div></li><li><div>
                This ensures that a user-supplied thread pool is not shut down; if one was created above then it should be closed now
              </div></li><li><div><div><b>body:</b> Apparently the default number of threads is ncpu * 5 (https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor). 

I think this needs to be exposed and configurable at the level of `ParquetDataset`, which is how most people will be consuming this. How about adding a parameter `metadata_nthreads` to ParquetDataset and passing that down here? If the user does not pass anything into `ParquetDataset`, we might either want to make `max_workers=1` or a more conservative number of threads. @pitrou do you have an opinion on the default? In the case that we have a fast filesystem, it shouldn't matter much. For slow filesystems and ones with distributed metadata services (like HDFS) this can definitely make things a lot faster
                </div><div><b>label:</b> code-design
                </div></div></li><li><div>
                Add extra `ParquetDataset` using here per above. Maybe check that the number of partitions is the same in the threaded vs. unthreaded cases? 
              </div></li><li><div>
                I would say keep `max_workers=1` as a default. The optimal number in a given situation will be filesystem- and system-dependent.
              </div></li></ol></div><div><b>jira_issues:</b> <ol><li><div><div><b>summary:</b> [Python] Improve ParquetManifest creation time 
                </div><div><b>description:</b> When a parquet dataset is highly partitioned, the time to call the constructor for&nbsp;[ParquetManifest|https://github.com/apache/arrow/blob/master/python/pyarrow/parquet.py#L588]&nbsp;takes a significant amount of time since it serially visits directories to find all parquet files. In a dataset with thousands of partition values this can take several minutes from a personal laptop.

A quick win to vastly improve this performance would be to use a ThreadPool to have calls to&nbsp;{{_visit_level}}&nbsp;happen concurrently to prevent wasting a ton of time waiting on I/O.

An even faster option could be to allow for optional indexing of dataset metadata in something like the&nbsp;{{common_metadata}}. This could contain all files in the manifest and their row_group information. This would also allow for [split_row_groups|https://github.com/apache/arrow/blob/master/python/pyarrow/parquet.py#L746]&nbsp;to be implemented efficiently without needing to open every parquet file in the dataset to retrieve the metadata which is quite time consuming for large datasets. The main problem with the indexing approach are it requires immutability of the dataset, which doesn't seem too unreasonable. This specific implementation seems related to https://issues.apache.org/jira/browse/ARROW-1983&nbsp;however that only covers the write portion.
                </div></div></li></ol></div><div><b>jira_issues_comments:</b> <ol><li><div>
                I will&nbsp;attempt to get some code as a benchmark however (I believe) you cannot create partitioned parquet datasets through pyarrow right now which will make it necessary to do so through spark.
              </div></li><li><div>
                I have opened [https://github.com/apache/arrow/pull/2185]&nbsp;which is a quick win that gives a rather large performance boost for creating a parquet manifest on a partitioned dataset which lives in hdfs.

I think using the summary file will be another improvement to make on top of this, but this is extremely useful in the meantime.
              </div></li><li><div>
                Issue resolved by pull request 2185
[https://github.com/apache/arrow/pull/2185]
              </div></li></ol></div></div></html>