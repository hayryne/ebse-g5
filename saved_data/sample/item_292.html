<!DOCTYPE html><html><div class="item-title">
        Item 292
      </div> <div class="item-details"><div><b>git_comments:</b> <ol><li><div>
                ! \brief input is one tensor 
              </div></li><li><div>
                 For Sparse CSR 
              </div></li><li><div>
                ! \brief Binary launch 
              </div></li></ol></div><div><b>git_commits:</b> <ol><li><div><div><b>summary:</b> [numpy] change unary infer type (#17254)
                </div><div><b>message:</b> [numpy] change unary infer type (#17254)

* cpu ok

* gpu ok

* change name

* sanity

* re-trigger
                </div></div></li></ol></div><div><b>github_issues:</b> <ol></ol></div><div><b>github_issues_comments:</b> <ol></ol></div><div><b>github_pulls:</b> <ol><li><div><div><b>title:</b> [numpy] change unary infer type
                </div><div><b>body:</b> ## Description ##
change unary infer type

## Checklist ##
### Essentials ###
Please feel free to remove inapplicable items for your PR.
- [ ] The PR title starts with [MXNET-$JIRA_ID], where $JIRA_ID refers to the relevant [JIRA issue](https://issues.apache.org/jira/projects/MXNET/issues) created (except PRs with tiny changes)
- [ ] Changes are complete (i.e. I finished coding on this PR)
- [ ] All changes have test coverage:
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- [ ] Code is well-documented: 
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- Check the API doc at https://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
- [ ] To the best of my knowledge, examples are either not affected by this change, or have been fixed to be compatible with this change

### Changes ###
- [ ] Feature1, tests, (and when applicable, API doc)
- [ ] Feature2, tests, (and when applicable, API doc)

## Comments ##
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here

                </div></div></li></ol></div><div><b>github_pulls_comments:</b> <ol><li><div>
                @mxnet-bot run ci [unix-gpu]
              </div></li><li><div>
                Jenkins CI successfully triggered : [unix-gpu]
              </div></li><li><div>
                @mxnet-bot run ci [centos-gpu]
              </div></li><li><div>
                Jenkins CI successfully triggered : [centos-gpu]
              </div></li></ol></div><div><b>github_pulls_reviews:</b> <ol><li><div>
                please get a better name.
              </div></li><li><div>
                better use `OType` and `IType`
              </div></li><li><div>
                also, better name for this one.
              </div></li><li><div>
                same here, get a better name for this.
              </div></li><li><div>
                better name for both `Compute2` and `Compute3`
              </div></li><li><div>
                remove dead code.
              </div></li><li><div>
                1 more blank line above.
              </div></li><li><div>
                get rid of this blank line.
              </div></li><li><div>
                Change the name to `TestMixedUnary`
              </div></li><li><div>
                I think both `rad2deg` and `deg2rad` support mixed precision.
              </div></li><li><div>
                Please also add checks for backward computation for floating point input cases.
              </div></li><li><div>
                Simply use `np.array` instead of `mx.numpy.array`
              </div></li><li><div>
                `MixedUnaryOpType`
              </div></li><li><div>
                pay attention to alignments
              </div></li><li><div>
                use `kFloat32` as default for now.
              </div></li><li><div>
                `MXNET_OPERATOR_REGISTER_NUMPY_MIXED_TYPE_UNARY`
              </div></li><li><div>
                `MXNET_OPERATOR_REGISTER_NUMPY_MIXED_TYPE_UNARY_GPU `
              </div></li><li><div>
                alignment
              </div></li><li><div>
                alignment
              </div></li><li><div>
                `MixedUnaryBackwardUseInCompute`
              </div></li><li><div>
                `MixedUnaryBackwardUseInOutCompute`
              </div></li><li><div>
                use `MXNET_REAL_TYPE_SWITCH` here since you already excluded the integer case.
              </div></li><li><div>
                I think you should directly check if the `output[0]`'s type is an integer type here.
              </div></li><li><div>
                alignment
              </div></li><li><div>
                `MSHADOW_REAL_TYPE_SWITCH` here since you've masked out integer cases above.
              </div></li><li><div>
                alignment
              </div></li><li><div>
                this is the case when forward computation's input is floating point number, the output will be of the same type as the input, so you can directly call the original `Compute` function above, no need for launching the kernel here.
              </div></li><li><div>
                this case and the above case could be merged.
Plus, the error message is not meaningful enough.
`mshadow::dtype_string(outputs[0].type_flag_)` will give you the corresponding string that represents the data type.
Also the problem with showing the op's name is that it will be something in the form of `_backward_npi_xxx`. I think here simply say that "gradient computation for xxx type is not supported" is better. ("xxx" should be the string returned by call to `dtype_string`)
              </div></li><li><div>
                Same for the error message here since `outputs[0].type_flag_` is technically same as `inputs[1].type_flag_`
              </div></li><li><div>
                no need for `WITH_SPARSE` since this does not include sparse functionalities.
              </div></li><li><div>
                `MXNET_OPERATOR_REGISTER_UNARY_MIXEDTYPE_USEIN_BWD_CPU`
              </div></li><li><div>
                `MXNET_OPERATOR_REGISTER_UNARY_MIXEDTYPE_USEINOUT_BWD_CPU`
              </div></li><li><div>
                `MXNET_OPERATOR_REGISTER_UNARY_MIXEDTYPE_BWD_IN` here and `MXNET_OPERATOR_REGISTER_UNARY_MIXEDTYPE_BWD_INOUT` below
              </div></li><li><div>
                seems like `FResourceRequest` is not used, you can simply get rid of this `set_attr`
              </div></li></ol></div><div><b>jira_issues:</b> <ol></ol></div><div><b>jira_issues_comments:</b> <ol></ol></div></div></html>